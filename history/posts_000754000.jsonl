{"id": "lh4gT0oGU_t-kiixJT0GBYp-_ucSFohyaFOJH07p7Es", "title": "UX Research to Improve NFT Auction Experience, using Mempool Data", "body": "User experience (UX) describes how people feel when they interact with a system or service and encompasses several factors including usability, design, marketing, accessibility, performance, comfort, and utility. Don Norman once said,\n\n> “Everything has a personality; everything sends an emotional signal. Even where this was not the intention of the designer, the people who view the website infer personalities and experience emotions. Bad websites have horrible personalities and instill horrid emotional states in their users, usually unwittingly. We need to design things — products, websites, services — to convey whatever personality and emotions are desired.”\n\nEthereum's personality is someone who is extremely inscrutable and easily misunderstood. To make matters worse, most users don't even think about it as interacting with Ethereum when they're using your interface or a wallet. If you're ever in the live chat of an Artblock's auction, you'll notice as soon as an auction ends there are at least a dozen people complaining that it is Metamask's fault that they didn't get a mint. I think in the last year the UX of many dapps on Ethereum has improved greatly in both product interaction and explainability of transactions. For the most part, dapps don't just leave you with a loading spinner after you sign the transaction anymore.\n\nEven with the design of dapps is generally improving, I'm not sure how deep UX research has gone yet. When I see data analytics or research on various protocols, users are mostly treated as homogenous. This is somewhat shifting with some of the analysis I've seen of Uniswap V3 liquidity providers and Rabbithole Questers, but even those are still heavily focused on just confirmed transactions on-chain. From my own experience, most of the emotion evoked and behavioral quirks happen when I'm submitting, waiting, speeding up, or canceling my transaction. For some applications, the user might leave and go do something else after submitting a transaction. But for a product like Artblock's auctions, they're going to stay around until the confirmation has happened, likely checking anything they can for updates and with compounding anxiety.\n\nI think we can do a much better job of understanding user behaviors and frictions by starting to leverage the mempool more. The [mempool](https://compassmining.io/education/what-is-a-mempool/) is where unconfirmed transactions are temporarily stored by a node. This means if you submit, speed up, or cancel your transaction then those actions will show up in the mempool first. It's important to note that the data from the mempool is not stored in the node, so you can't query historical data the same way you could on confirmed transactions. From here, you can see that they submitted a few transactions, sped up the transactions quite a few times but nowhere near the gas price needed, and ultimately saw confirmations 20 blocks later. I believe this is a pretty good proxy for user experience and likely emotions they were feeling throughout the whole process. If we understand how different groups of users behave in this cycle, we can figure out how to supplement their decision-making or ease their anxiety. To my knowledge, pretty much only the Ethereum Foundation, All Core Devs, and some wallet teams leverage the mempool data for UX reasons.\n\n**UX Research Thesis:** By looking at a user's behavior through auctions over time and also their wallet history, we can start to give behavioral identities to different groups of users. From here, we can identify the main issues to try and alleviate. We'll do this by taking a month of Artblocks Auctions data using [Blocknative](https://www.blocknative.com/), and layering in the history of these addresses using [Dune queries](https://dune.xyz/).\n\nThis article will be more technical than some of my previous ones since I believe the work can and should be generalized fairly easily.   *I want to emphasize that my background is not in UX research, I'm purely experimenting with what I think crypto-native UX research could look like.*\n\n## Data Sourcing and Preprocessing All Auction Data\n\n*If you have no interest in the technical bits, skip to the next section on Feature Engineering*\n\n### Blocknative and Mempool Data Streaming\n\nUsing Blocknative's Mempool explorer, you can filter for transactions submitted to specific contracts or from specific wallets. In my case, I wanted to listen to the whitelisted minter contracts for Artblock's NFT contract. You can find the stream that I used [here](https://tinyurl.com/yftus9h2), and save it down if you want to use the exact same setup.\n\nYou can find the whitelisted minter addresses with the following query [in their subgraph](https://thegraph.com/legacy-explorer/subgraph/artblocks/art-blocks):\n\n```python\n{\n  contracts(first: 2) {\n    id\n    mintWhitelisted\n  }\n}\n```\n\nThere were three steps to get to a subscription filter for all purchases:\n\n1. Add the new address with the \"create new subscription\" button\n2. Add the ABIs by clicking the \"ABI\" button next to the address. In my case I just needed the \"purchase\" function.\n\n```python\n{\n    \"inputs\": [\n      {\n        \"internalType\": \"uint256\",\n        \"name\": \"_projectId\",\n        \"type\": \"uint256\"\n      }\n    ],\n    \"name\": \"purchase\",\n    \"outputs\": [\n      {\n        \"internalType\": \"uint256\",\n        \"name\": \"_tokenId\",\n        \"type\": \"uint256\"\n      }\n    ],\n    \"stateMutability\": \"payable\",\n    \"type\": \"function\"\n  }\n```\n\n1. Add filters for the `methodName` matches `purchase` (make sure you don't do a global filter)\n\nIn the end, your setup should look like this:\n\n![](https://images.mirror-media.xyz/publication-images/ad1f283a-7ce1-4899-9fa8-bdfda6a716e1.png?height=577&width=844)\n\nTo store this data down, I created a ngrok/express endpoint to store in an SQLite database, run locally. I've created a [GitHub template with steps to replicate this setup](https://github.com/andrewhong5297/blocknative_stream_to_sql). Probably the most important point to remember here is that you need to include the POST endpoint as part of the ngrok URL when adding it as a webhook in the Blocknative account page.\n\n### Key Preprocessing Functions\n\n**Multiple Transaction Hashes**\n\nWhen you speed up or cancel a transaction, the original transaction hash is replaced with the new transaction. This means if you want to track a users' transaction through its full lifecycle, you need to reconcile the new transaction hashes with the original one. Assuming you speed up a transaction five times, you'll have six hashes total (the original hash + five new hashes). I reconciled this by getting a dictionary mapping of `tx_hash` to the new `replaceHash`, and then replaced recursively.\n\n```python\nreplaceHashKeys = dict(zip(auctions[\"replaceHash\"],auctions[\"tx_hash\"])) #assign tx_hash based on replacements, just to keep consistency. \nreplaceHashKeys.pop(\"none\") #remove none key\n\ndef recursive_tx_search(key):\n    if key in replaceHashKeys:\n        return recursive_tx_search(replaceHashKeys[key])\n    else:\n        return key\n\nauctions[\"tx_hash\"] = auctions[\"tx_hash\"].apply(lambda x: recursive_tx_search(x))\n```\n\n**Blocknumber Issues**\n\nDropped transactions have a `blocknumber` of 0, so to deal with this I sorted my dataframe by `timestamp` in ascending order, and then did a backward fill so the 0 would be replaced by the correct `blocknumber` it was dropped in. This is important fix for feature engineering.\n\n```python\nauctions = auctions.sort_values(by=\"timestamp\",ascending=True)\nauctions[\"blocknumber\"] = auctions[\"blocknumber\"].replace(to_replace=0, method='bfill') #deal with dropped txs that show as blocknumber 0\n```\n\n**Dealing with Mints Outside of Main Auction Period**\n\nFor most projects, the artist will mint a few pieces before the auction opens up to the public. Some projects don't sell out right away, so you will get mints still occurring a few days after the auction has opened. My analysis is focused on the key auction period, mostly the first 30 minutes. To get rid of the two mint cases above, I removed outliers based on `blocknumber`.\n\n```python\nto_remove_indicies = []\nfor project in list(set(auctions[\"projectId\"])):\n    auction_spec = auctions[auctions[\"projectId\"]==project]\n    all_times = pd.Series(list(set(auction_spec.blocknumber)))\n    to_remove_blocktimes = all_times[(np.abs(stats.zscore(all_times)) > 2.5)]\n    if len(to_remove_blocktimes)==0:\n        break\n    to_remove_indicies.extend(auction_spec.index[auction_spec['blocknumber'].isin(to_remove_blocktimes)].tolist())\nauctions.drop(index=to_remove_indicies, inplace=True)\n```\n\n**Adding on Dutch Auction Prices**\n\nFor all projects in the dataset besides project 118, a Dutch auction price format was used. I took the mint price data using a [dune query](https://dune.xyz/queries/113834), and then merged it onto the dataset. I had to use a forward and backward fill for the blocks that had mempool actions but no confirmations during the auction.\n\n```python\nauction_prices = pd.read_csv(r'artblock_auctions_analytics/datasets/dune_auction_prices.csv', index_col=0)\nauctions = pd.merge(auctions,auction_prices, how=\"left\", left_on=[\"projectId\",\"blocknumber\"],right_on=[\"projectId\",\"blocknumber\"])\nauctions.sort_values(by=[\"projectId\",\"blocknumber\"], ascending=True, inplace=True)\nauctions[\"price_eth\"].fillna(method=\"ffill\", inplace=True)\nauctions[\"price_eth\"].fillna(method=\"bfill\", inplace=True)\n```\n\n## Feature Engineering For Each Auction\n\n*If you have no interest in the technical bits, just read the parts in bold and skip the rest.*\n\nIn data science, a feature is a variable that is calculated from the larger dataset to be used as an input in some sort of model or algorithm. All features are calculated in the   `preprocess_auction` function and are calculated *per auction rather than combining all the auctions into a feature set.*\n\n**The first set of features are the totals for transaction states**, and was a simple `pivot_table` function:\n\n* `number_submitted` : total number of transactions submitted\n* `cancel` : count of transactions that ended in canceled\n* `failed`: count of transactions that ended in failed\n* `dropped`: count of transactions that ended in dropped\n* `confirmed`: count of transactions that ended in confirmed\n\nI mentioned earlier some data was not captured for auctions due to various issues, these transactions were dropped from the dataset.\n\n**The next set of features included their gas behavior.**     The key concept here was capturing how far away their transaction gas was from the average confirmed gas per block (shifted by 1 block). **Then we can create features for the average, median, and standard deviation of gas price distance over the whole auction.** There are a bunch of transposes and index resets to get the `blocknumber` columns in the right order, but the important function is `fill_pending_values_gas` which forward fills the gas price between actions captured. This means that if I put in a transaction at `blocknumber` 1000 with a gas of 0.05 ETH and my next action wasn't until `blocknumber` 1005 where I sped up to 0.1 ETH gas, then this function will fill in the blocks between 1000-1005 with 0.05 ETH.\n\n```python\ndef fill_pending_values_gas(x):\n    first = x.first_valid_index()\n    last = x.last_valid_index()\n    x.loc[first:last] = x.loc[first:last].fillna(method=\"ffill\")\n    return x\n```\n\n**The third set of features were calculating the total number and frequency of actions taken in the auction.**   Here we start with a pivot of total actions (speed ups) per block, with some special calculations for getting the first instance of pending for each transaction:\n\n```python\nget_first_pending = df[df[\"status\"]==\"pending\"] #first submitted \nget_first_pending = get_first_pending.drop_duplicates(subset=[\"tx_hash\",\"status\"], keep=\"first\")\nauctions_time_data = pd.concat([get_first_pending,df[df[\"status\"]==\"speedup\"]], axis=0)\ntime_action = auctions_time_data.pivot_table(index=[\"sender\",\"tx_hash\"], columns=\"blocknumber\",values=\"status\",aggfunc=\"count\") \\\n                    .reindex(set(df[\"blocknumber\"]), axis=1, fill_value=np.nan)\n```\n\nFrom here we get to `average_action_delay` in three steps:\n\n1. we take the number of actions per block (yes some people sped up transactions multiple times in the same block)\n2. we drop the blocks with no actions, and then take the difference between the blocknumbers that remain. We add a 0 for each additional action taken per block.\n3. Taking the mean across the differences and added zeroes gives us the `average_action_delay`\n\n```python\ndef get_actions_diff(row):\n    row = row.dropna().reset_index()\n    actions_diff_nominal =list(row[\"blocknumber\"].diff(1).fillna(0))\n \n    #take the blocks with muliple actions and subtract one, then sum up. \n    zeros_to_add = sum([ actions - 1 if actions > 1 else 0 for actions in row[row.columns[1]]])\n    actions_diff_nominal.extend(list(np.zeros(int(zeros_to_add))))\n    actions_diff = np.mean(actions_diff_nominal)\n    if (actions_diff==0) and (zeros_to_add==0):\n        return 2000 #meaning they never took another action\n    else:\n        return actions_diff\n```\n\n`total_actions` is much simpler, as it is just the sum of actions across the pivot.\n\n```python\ntime_action[\"total_actions\"] = time_action.iloc[:,:-1].sum(axis=1)\n```\n\n**The last time-dependent feature is** `block_entry`, which is an important one due to the introduction of Dutch auctions. Essentially this tracks what block the transaction was submitted on since the start.\n\n```python\nget_first_pending[\"block_entry\"] =   get_first_pending[\"blocknumber\"] - get_first_pending[\"blocknumber\"].min()\n\nentry_pivot = get_first_pending.pivot_table(index=\"sender\",values=\"block_entry\",aggfunc=\"min\")\n```\n\n`price_eth` is added as a feature as well, which is tied to the `block_entry` point.\n\n**The last set of features were based on a Dune query, specifically the days since the first transaction, total gas used in transactions, and the total number of transactions.**   To get the address array in the right format I used the following line of code after reading in the SQL data:\n\n```python\nall_users = list(set(auctions[\"sender\"].apply(lambda x: x.replace('0x','\\\\x'))))\nall_users_string = \"('\" + \"'),('\".join(all_users) + \"')\"\n```\n\nThe dune query for this is fairly simple. I pasted the addresses string under `VALUES`, and made some CTEs to get the features I wanted. In the final `SELECT` I tried to add each address's ens as well. You can find the query here: <https://dune.xyz/queries/96523>\n\nLastly, we just merge in the per wallet data on days active, total gas used, and the total number of transactions.\n\n```python\nauctions_all_df = pd.merge(auctions_all_df,wh,on=\"sender\",how=\"left\")\nauctions_all_df.set_index([\"sender\",\"ens\"],inplace=True)\n```\n\n*With all this done, we're finally ready to run some fun unsupervised learning algorithms and try and validate our hypothesis on user groups.*\n\n## Clustering and Visualizing User Groups\n\nBefore I started this project, I had expected to see the following user groups pop out of the data:\n\n* **Set and Forget:** There should be two groups here, those who set a transaction with really high gas and an average/low gas and then don't touch it for the rest of the auction.\n* **Speed Up:** There should be two groups here as well, those who are speeding up often and updating the transaction directly as a factor of gas prices and those who are speeding up often but with basically no change in gas price.\n\nI was very interested in validating these groups, seeing how large each group was, and seeing if any users moved between groups over the course of many auctions. The easiest way to do this was to use unsupervised machine learning to identify clusters of user groups based on the variability across all features. Essentially this is like looking at the distribution of income in a state and then splitting it into sub-distributions for different income concentrations, geographic coordinates, and age. Note that this isn't binning, where the distribution is split into equal ranges - it is instead calculated based on the density of observations within the whole range. The approach we will take is called \"unsupervised\" because our dataset doesn't have any existing labels, rather than something like a regression where there is a value being predicted that can be verified as right or wrong.\n\nThe algorithm I decided to use is called [k-means](https://stanford.edu/\\~cpiech/cs221/handouts/kmeans.html), where k stands for the number of clusters you are expecting to identify. Each cluster has a \"centroid\", which is like the center of a circle. There are various methods for figuring out how many clusters are optimal, and the two I used were elbow points and silhouette scores. Those are both fancy ways of asking,\n\n> \"does each additional cluster help increase the density of the cluster (calculated as the average distance of points in a cluster from the centroid) and maintain adequate separation between clusters (no overlap between two clusters)?\"\n\nI found that 3 clusters were optimal in terms of most inertia improvement while keeping a high silhouette score (around 0.55).\n\n![6 auctions were used for this analysis](https://images.mirror-media.xyz/publication-images/7bef376a-b96c-42de-87b6-991db4503ef9.png?height=264&width=393)\n\nWith clusters chosen, we want to be able to visualize and verify their existence. There are over 15 variables, so we need to reduce the number of dimensions in order to plot it. Dimension reduction typically relies on either PCA or t-SNE algorithms, in our case I went with t-SNE. Don't worry too much about understanding this part, these algorithms essentially capture the variance across all features to give us X and Y components that maximize the spread of points from each other.\n\n**Let's look at project 118, LeWitt Generator Generator, from August 4th:**\n\n![](https://images.mirror-media.xyz/publication-images/4f7dad88-47b6-4dde-9a52-f0e4258cb829.png?height=605&width=615)\n\n![These are the sub-distributions for each variable by cluster, calculated using a KDE. The colors match those in the above clusters.](https://images.mirror-media.xyz/publication-images/5bafdebf-7b9b-43b3-8ed0-7e4ac923725f.png?height=856&width=1123)\n\nAfter taking a look at the sub-distributions for each variable and some data examples, I was able to categorize the clusters. The Orange cluster is the group that is speeding up the most while also submitting slightly lower gas transactions on average. Blue and Green clusters exhibit similar behavior to each other, but addresses in Blue typically have less history than addresses in the Green cluster.\n\nLooking at the overall picture, it seems the original hypothesis on \"speed-up\" and \"set high and set low\" producing two groups each was wrong. Instead, we have a single \"speed-up\" group (Orange) and a single \"set-and-forget\" group (Blue and Green are behaviorally the same). I think the new (Blue) versus old wallets (Green) in the \"set and forget\" group probably carries a lot of overlap in actual users, where users just created new wallets to bid on more mints. Based on their impatience and lower-than-average gas prices, the \"speed-up\" group reads to me as either less experienced or just more greedy than other users.   *What did surprise me is that the speed-up group makes up a smaller proportion of total bidders, as I had expected that group to make up 60-70% of bidders instead of 30%.*\n\n**Now, where this user behavior study gets really interesting is when comparing project 118 (the set price of 0.75 ETH) to project 140 (Dutch auction with the price decreasing from 1.559 to .159 ETH).**\n\n**Here's the clustering for project 140, Good Vibrations from August 21st:**\n\n![](https://images.mirror-media.xyz/publication-images/ae090b2e-141c-4dd0-a0cb-06f4ed45dd13.png?height=605&width=615)\n\n![](https://images.mirror-media.xyz/publication-images/7294dd21-59e5-48fd-b7a1-7babbda77040.png?height=856&width=1130)\n\nWe can see that now most of the clustering variability comes from `block_entry`, `price_eth`, and all of the `gas_behavior` features. This is a big departure from the main variables for project 118. In 118, the set price meant that people entered the auction in a fairly uniform distribution (quantity left didn't seem to matter), and the \"speed-up\" group makes actions up fairly endlessly - probably very anxious.\n\nIn project 140, we don't see the same difference in actions within `average_action_delay` or `total_actions` , instead, we see probably that same \"speed-up\" group entering at a very late stage block and setting far below-average gas prices as seen in `average_gas_behavior`. The Green cluster probably represents users with more experience than the Orange cluster, but who are still transitioning between Orange and Blue in terms of their behavior. If I was to try and map this to the clusters in 118, I believe the \"speed-up\" group is now the \"greedy\" group (Orange) that is entering late and bidding a low gas. The \"set-and-forget\" group maps pretty well to the \"early-grab\" group (Green and Blue), where they all exhibit pretty good patience and an adequate safety net on gas bids.\n\n**I call the Orange group \"greedy\" not just because of their behavior, but also because of their rate of failed transactions.**\n\nFor project 118, fail rates are of the \"speed-up\" versus \"set-and-forget\" groups are within 10-15%.\n\n![percent_lost takes (cancel + dropped + failed) / number_submitted](https://images.mirror-media.xyz/publication-images/306752cb-87c2-4494-8915-86102c214d29.png?height=77&width=724)\n\nFor project 140, the fail rate of the \"greedy\" cluster is around 69% versus the \"early-grab\" group at around 5-15%.\n\n![](https://images.mirror-media.xyz/publication-images/55ceaf8d-6756-4807-beb9-fcfbbe4e549e.png?height=84&width=727)\n\n**Overall, my read of this is that the group's bad habits were amplified - it feels to me like we made a tradeoff in anxiety → greed. This may have made the auction less stressful, but ultimately led to more users being upset (due to failed mints).**\n\n*I'm sure there's a more granular analysis that can be done to segment the auctions further based on factory/curated/playground or by artists themselves too. This will only get more interesting and complex as the community continues to grow, and emotions play a larger factor in both a single auction and on if they return for future auctions.*\n\n*This study of multiple auctions helped us validate our assumptions, understand the proportions of user groups, and see how users' good or bad behaviors shift over time (and other parameters). Now we need to plug it into the rest of the product cycle process.*\n\n## Where We Go From Here:\n\nThe reason that I chose just Artblocks auctions for this and not a mix of platforms is because I wanted to look for a place where the variability in terms of interface and project type is mostly controlled. This should have given us fairly consistent users and behavior types.\n\nThis is just the start of a UX research cycle, so ideally we could continue in the following steps:\n\n1. Use an unsupervised machine learning algorithm to identify the user groups (clusters) and see how many people are making \"mistakes\" when entering an auction.   *This is the step we covered today.*\n2. Create a new user interface, such as a [histogram view on the bidding screen](https://twitter.com/andrewhong5297/status/1423020670825504768), or showing historical data on when most people usually enter/crowd the auction and at what prices. Anything to give both current and historical context to the user, especially those from the speed cluster.\n3. With each auction, run the mempool/wallet data through the created algorithm to see if the user groups have shifted and if specific users have \"learned\" to participate in the auction differently (i.e. did they move between user groups). *I think that the most value can be found in this step if done well. Using ENS or other identifiers to help supplement this grouping would be exponentially helpful too*\n4. Based on the results, continue to iterate on the user interface and design. You could also run more informed A/B testing since you could even figure out what screens to show by making an educated guess based on the users' last cluster (or use label propagation for new users).\n\n**The Dutch auction-style change is also an example of step #2, and we were able to see a clear shift in user behaviors.**     While typically this sort of A/B testing is focused on increasing engagement or conversions, here we are optimizing for the user's ability to learn and improve instead. This may become even more robust if this was iterated in a multiplatform context, so that we can study how someone is learning at an ecosystem level (maybe even supplement with Rabbithole data and user profiles). Since my Artblocks user research is all based on publicly sourced data, it can be replicated and supplemented by any other auction/sale platform. **Crypto could be the first industry that would have a synchronized and transparent set of user groups and UX research, to be applied in products and academia.** Nansen wallet labeling is already a step towards this, but it's different when teams from different products build this up from various facets and approaches.\n\nWhat I would eventually envision is using data to build on the following user personas (with subgroups/levels within them):\n\n* I want to buy a Fidenza, so I can either buy one through a private sale, bid on one in an auction myself, bid on one in a prtyDAO bid auction, or buy a fraction of one with [fractional.art](https://fractional.art/)\n* I like Fidenzas in general, so I'll just buy the NFTX Fidenza index token or an NFT basket of Artblocks Curated on [fractional.art](https://fractional.art/)\n* I'm a collector already, so I want to swap or bid on a Fidenza using a curated set of NFTs and ERC20s I already hold (using genie-xyz swap).\n* I like the rush of acquiring through initial mint versus secondary market, and heavily participate in auctions like Artblocks live mints.\n\nI hope you found this project interesting and/or helpful, I had a ton of fun with it. Thanks to the folks at Blocknative for setting me up, and the community at Artblocks for answering my many auction questions. As always, feel free to reach out with any questions or ideas you may have!\n\nYou can find the GitHub repo [with all the data and script here](https://github.com/andrewhong5297/artblock_auctions_analysis). The script may be a bit hard to read since I’m still refactoring and cleaning it up. The script and some of the analysis here may get updated as I analyze the last few auctions of August for new patterns.", "timestamp": 1629646856, "digest": "l_-4fQ08cpxUZpn9V9S5R27wfKvNgdnrXlZAWZWvdlg", "contributor": "0x2Ae8c972fB2E6c00ddED8986E2dc672ED190DA06"}
{"id": "lu4bRLUG3iqxV0QvtU9ynwPm7j5yGg5aJAiyJjQ_CeY", "title": "UX Research to Improve NFT Auction Experience, using Mempool Data", "body": "User experience (UX) describes how people feel when they interact with a system or service and encompasses several factors including usability, design, marketing, accessibility, performance, comfort, and utility. Don Norman once said,\n\n> “Everything has a personality; everything sends an emotional signal. Even where this was not the intention of the designer, the people who view the website infer personalities and experience emotions. Bad websites have horrible personalities and instill horrid emotional states in their users, usually unwittingly. We need to design things — products, websites, services — to convey whatever personality and emotions are desired.”\n\nEthereum's personality is someone who is extremely inscrutable and easily misunderstood. To make matters worse, most users don't even think about it as interacting with Ethereum when they're using your interface or a wallet. If you're ever in the live chat of an Artblock's auction, you'll notice as soon as an auction ends there are at least a dozen people complaining that it is Metamask's fault that they didn't get a mint. I think in the last year the UX of many dapps on Ethereum has improved greatly in both product interaction and explainability of transactions. For the most part, dapps don't just leave you with a loading spinner after you sign the transaction anymore.\n\nEven with the design of dapps is generally improving, I'm not sure how deep UX research has gone yet. When I see data analytics or research on various protocols, users are mostly treated as homogenous. This is somewhat shifting with some of the analysis I've seen of Uniswap V3 liquidity providers and Rabbithole Questers, but even those are still heavily focused on just confirmed transactions on-chain. From my own experience, most of the emotion evoked and behavioral quirks happen when I'm submitting, waiting, speeding up, or canceling my transaction. For some applications, the user might leave and go do something else after submitting a transaction. But for a product like Artblock's auctions, they're going to stay around until the confirmation has happened, likely checking anything they can for updates and with compounding anxiety.\n\nI think we can do a much better job of understanding user behaviors and frictions by starting to leverage the mempool more. The [mempool](https://compassmining.io/education/what-is-a-mempool/) is where unconfirmed transactions are temporarily stored by a node. This means if you submit, speed up, or cancel your transaction then those actions will show up in the mempool first. It's important to note that the data from the mempool is not stored in the node, so you can't query historical data the same way you could on confirmed transactions. From here, you can see that they submitted a few transactions, sped up the transactions quite a few times but nowhere near the gas price needed, and ultimately saw confirmations 20 blocks later. I believe this is a pretty good proxy for user experience and likely emotions they were feeling throughout the whole process. If we understand how different groups of users behave in this cycle, we can figure out how to supplement their decision-making or ease their anxiety. To my knowledge, pretty much only the Ethereum Foundation, All Core Devs, and some wallet teams leverage the mempool data for UX reasons.\n\n**UX Research Thesis:** By looking at a user's behavior through auctions over time and also their wallet history, we can start to give behavioral identities to different groups of users. From here, we can identify the main issues to try and alleviate. We'll do this by taking a month of Artblocks Auctions data using [Blocknative](https://www.blocknative.com/), and layering in the history of these addresses using [Dune queries](https://dune.xyz/).\n\nThis article will be more technical than some of my previous ones since I believe the work can and should be generalized fairly easily.  *I want to emphasize that my background is not in UX research, I'm purely experimenting with what I think crypto-native UX research could look like.*\n\n## Data Sourcing and Preprocessing All Auction Data\n\n*If you have no interest in the technical bits, skip to the next section on Feature Engineering*\n\n### Blocknative and Mempool Data Streaming\n\nUsing Blocknative's Mempool explorer, you can filter for transactions submitted to specific contracts or from specific wallets. In my case, I wanted to listen to the whitelisted minter contracts for Artblock's NFT contract. You can find the stream that I used [here](https://tinyurl.com/yftus9h2), and save it down if you want to use the exact same setup.\n\nYou can find the whitelisted minter addresses with the following query [in their subgraph](https://thegraph.com/legacy-explorer/subgraph/artblocks/art-blocks):\n\n```python\n{\n  contracts(first: 2) {\n    id\n    mintWhitelisted\n  }\n}\n```\n\nThere were three steps to get to a subscription filter for all purchases:\n\n1. Add the new address with the \"create new subscription\" button\n2. Add the ABIs by clicking the \"ABI\" button next to the address. In my case I just needed the \"purchase\" function.\n\n   ```python\n   {\n       \"inputs\": [\n         {\n           \"internalType\": \"uint256\",\n           \"name\": \"_projectId\",\n           \"type\": \"uint256\"\n         }\n       ],\n       \"name\": \"purchase\",\n       \"outputs\": [\n         {\n           \"internalType\": \"uint256\",\n           \"name\": \"_tokenId\",\n           \"type\": \"uint256\"\n         }\n       ],\n       \"stateMutability\": \"payable\",\n       \"type\": \"function\"\n     }\n   ```\n3. Add filters for the `methodName` matches `purchase` (make sure you don't do a global filter)\n\nIn the end, your setup should look like this:\n\n![](https://images.mirror-media.xyz/publication-images/ad1f283a-7ce1-4899-9fa8-bdfda6a716e1.png?height=577&width=844)\n\nTo store this data down, I created a ngrok/express endpoint to store in an SQLite database, run locally. I've created a [GitHub template with steps to replicate this setup](https://github.com/andrewhong5297/blocknative_stream_to_sql). Probably the most important point to remember here is that you need to include the POST endpoint as part of the ngrok URL when adding it as a webhook in the Blocknative account page.\n\n### Key Preprocessing Functions\n\n**Multiple Transaction Hashes**\n\nWhen you speed up or cancel a transaction, the original transaction hash is replaced with the new transaction. This means if you want to track a users' transaction through its full lifecycle, you need to reconcile the new transaction hashes with the original one. Assuming you speed up a transaction five times, you'll have six hashes total (the original hash + five new hashes). I reconciled this by getting a dictionary mapping of `tx_hash` to the new `replaceHash`, and then replaced recursively.\n\n```python\nreplaceHashKeys = dict(zip(auctions[\"replaceHash\"],auctions[\"tx_hash\"])) #assign tx_hash based on replacements, just to keep consistency. \nreplaceHashKeys.pop(\"none\") #remove none key\n\ndef recursive_tx_search(key):\n    if key in replaceHashKeys:\n        return recursive_tx_search(replaceHashKeys[key])\n    else:\n        return key\n\nauctions[\"tx_hash\"] = auctions[\"tx_hash\"].apply(lambda x: recursive_tx_search(x))\n```\n\n**Blocknumber Issues**\n\nDropped transactions have a `blocknumber` of 0, so to deal with this I sorted my dataframe by `timestamp` in ascending order, and then did a backward fill so the 0 would be replaced by the correct `blocknumber` it was dropped in. This is important fix for feature engineering.\n\n```python\nauctions = auctions.sort_values(by=\"timestamp\",ascending=True)\nauctions[\"blocknumber\"] = auctions[\"blocknumber\"].replace(to_replace=0, method='bfill') #deal with dropped txs that show as blocknumber 0\n```\n\n**Dealing with Mints Outside of Main Auction Period**\n\nFor most projects, the artist will mint a few pieces before the auction opens up to the public. Some projects don't sell out right away, so you will get mints still occurring a few days after the auction has opened. My analysis is focused on the key auction period, mostly the first 30 minutes. To get rid of the two mint cases above, I removed outliers based on `blocknumber`.\n\n```python\nto_remove_indicies = []\nfor project in list(set(auctions[\"projectId\"])):\n    auction_spec = auctions[auctions[\"projectId\"]==project]\n    all_times = pd.Series(list(set(auction_spec.blocknumber)))\n    to_remove_blocktimes = all_times[(np.abs(stats.zscore(all_times)) > 2.5)]\n    if len(to_remove_blocktimes)==0:\n        break\n    to_remove_indicies.extend(auction_spec.index[auction_spec['blocknumber'].isin(to_remove_blocktimes)].tolist())\nauctions.drop(index=to_remove_indicies, inplace=True)\n```\n\n**Adding on Dutch Auction Prices**\n\nFor all projects in the dataset besides project 118, a Dutch auction price format was used. I took the mint price data using a [dune query](https://dune.xyz/queries/113834), and then merged it onto the dataset. I had to use a forward and backward fill for the blocks that had mempool actions but no confirmations during the auction.\n\n```python\nauction_prices = pd.read_csv(r'artblock_auctions_analytics/datasets/dune_auction_prices.csv', index_col=0)\nauctions = pd.merge(auctions,auction_prices, how=\"left\", left_on=[\"projectId\",\"blocknumber\"],right_on=[\"projectId\",\"blocknumber\"])\nauctions.sort_values(by=[\"projectId\",\"blocknumber\"], ascending=True, inplace=True)\nauctions[\"price_eth\"].fillna(method=\"ffill\", inplace=True)\nauctions[\"price_eth\"].fillna(method=\"bfill\", inplace=True)\n```\n\n## Feature Engineering For Each Auction\n\n*If you have no interest in the technical bits, just read the parts in bold and skip the rest.*\n\nIn data science, a feature is a variable that is calculated from the larger dataset to be used as an input in some sort of model or algorithm. All features are calculated in the  `preprocess_auction` function and are calculated *per auction rather than combining all the auctions into a feature set.*\n\n**The first set of features are the totals for transaction states**, and was a simple `pivot_table` function:\n\n* `number_submitted` : total number of transactions submitted\n* `cancel` : count of transactions that ended in canceled\n* `failed`: count of transactions that ended in failed\n* `dropped`: count of transactions that ended in dropped\n* `confirmed`: count of transactions that ended in confirmed\n\nI mentioned earlier some data was not captured for auctions due to various issues, these transactions were dropped from the dataset.\n\n**The next set of features included their gas behavior.**   The key concept here was capturing how far away their transaction gas was from the average confirmed gas per block (shifted by 1 block). **Then we can create features for the average, median, and standard deviation of gas price distance over the whole auction.** There are a bunch of transposes and index resets to get the `blocknumber` columns in the right order, but the important function is `fill_pending_values_gas` which forward fills the gas price between actions captured. This means that if I put in a transaction at `blocknumber` 1000 with a gas of 0.05 ETH and my next action wasn't until `blocknumber` 1005 where I sped up to 0.1 ETH gas, then this function will fill in the blocks between 1000-1005 with 0.05 ETH.\n\n```python\ndef fill_pending_values_gas(x):\n    first = x.first_valid_index()\n    last = x.last_valid_index()\n    x.loc[first:last] = x.loc[first:last].fillna(method=\"ffill\")\n    return x\n```\n\n**The third set of features were calculating the total number and frequency of actions taken in the auction.**  Here we start with a pivot of total actions (speed ups) per block, with some special calculations for getting the first instance of pending for each transaction:\n\n```python\nget_first_pending = df[df[\"status\"]==\"pending\"] #first submitted \nget_first_pending = get_first_pending.drop_duplicates(subset=[\"tx_hash\",\"status\"], keep=\"first\")\nauctions_time_data = pd.concat([get_first_pending,df[df[\"status\"]==\"speedup\"]], axis=0)\ntime_action = auctions_time_data.pivot_table(index=[\"sender\",\"tx_hash\"], columns=\"blocknumber\",values=\"status\",aggfunc=\"count\") \\\n                    .reindex(set(df[\"blocknumber\"]), axis=1, fill_value=np.nan)\n```\n\nFrom here we get to `average_action_delay` in three steps:\n\n1. we take the number of actions per block (yes some people sped up transactions multiple times in the same block)\n2. we drop the blocks with no actions, and then take the difference between the blocknumbers that remain. We add a 0 for each additional action taken per block.\n3. Taking the mean across the differences and added zeroes gives us the `average_action_delay`\n\n```python\ndef get_actions_diff(row):\n    row = row.dropna().reset_index()\n    actions_diff_nominal =list(row[\"blocknumber\"].diff(1).fillna(0))\n \n    #take the blocks with muliple actions and subtract one, then sum up. \n    zeros_to_add = sum([ actions - 1 if actions > 1 else 0 for actions in row[row.columns[1]]])\n    actions_diff_nominal.extend(list(np.zeros(int(zeros_to_add))))\n    actions_diff = np.mean(actions_diff_nominal)\n    if (actions_diff==0) and (zeros_to_add==0):\n        return 2000 #meaning they never took another action\n    else:\n        return actions_diff\n```\n\n`total_actions` is much simpler, as it is just the sum of actions across the pivot.\n\n```python\ntime_action[\"total_actions\"] = time_action.iloc[:,:-1].sum(axis=1)\n```\n\n**The last time-dependent feature is** `block_entry`, which is an important one due to the introduction of Dutch auctions. Essentially this tracks what block the transaction was submitted on since the start.\n\n```python\nget_first_pending[\"block_entry\"] =   get_first_pending[\"blocknumber\"] - get_first_pending[\"blocknumber\"].min()\n\nentry_pivot = get_first_pending.pivot_table(index=\"sender\",values=\"block_entry\",aggfunc=\"min\")\n```\n\n`price_eth` is added as a feature as well, which is tied to the `block_entry` point.\n\n**The last set of features were based on a Dune query, specifically the days since the first transaction, total gas used in transactions, and the total number of transactions.**  To get the address array in the right format I used the following line of code after reading in the SQL data:\n\n```python\nall_users = list(set(auctions[\"sender\"].apply(lambda x: x.replace('0x','\\\\x'))))\nall_users_string = \"('\" + \"'),('\".join(all_users) + \"')\"\n```\n\nThe dune query for this is fairly simple. I pasted the addresses string under `VALUES`, and made some CTEs to get the features I wanted. In the final `SELECT` I tried to add each address's ens as well. You can find the query here: <https://dune.xyz/queries/96523>\n\nLastly, we just merge in the per wallet data on days active, total gas used, and the total number of transactions.\n\n```python\nauctions_all_df = pd.merge(auctions_all_df,wh,on=\"sender\",how=\"left\")\nauctions_all_df.set_index([\"sender\",\"ens\"],inplace=True)\n```\n\n*With all this done, we're finally ready to run some fun unsupervised learning algorithms and try and validate our hypothesis on user groups.*\n\n## Clustering and Visualizing User Groups\n\nBefore I started this project, I had expected to see the following user groups pop out of the data:\n\n* **Set and Forget:** There should be two groups here, those who set a transaction with really high gas and an average/low gas and then don't touch it for the rest of the auction.\n* **Speed Up:** There should be two groups here as well, those who are speeding up often and updating the transaction directly as a factor of gas prices and those who are speeding up often but with basically no change in gas price.\n\nI was very interested in validating these groups, seeing how large each group was, and seeing if any users moved between groups over the course of many auctions. The easiest way to do this was to use unsupervised machine learning to identify clusters of user groups based on the variability across all features. Essentially this is like looking at the distribution of income in a state and then splitting it into sub-distributions for different income concentrations, geographic coordinates, and age. Note that this isn't binning, where the distribution is split into equal ranges - it is instead calculated based on the density of observations within the whole range. The approach we will take is called \"unsupervised\" because our dataset doesn't have any existing labels, rather than something like a regression where there is a value being predicted that can be verified as right or wrong.\n\nThe algorithm I decided to use is called [k-means](https://stanford.edu/\\~cpiech/cs221/handouts/kmeans.html), where k stands for the number of clusters you are expecting to identify. Each cluster has a \"centroid\", which is like the center of a circle. There are various methods for figuring out how many clusters are optimal, and the two I used were elbow points and silhouette scores. Those are both fancy ways of asking,\n\n> \"does each additional cluster help increase the density of the cluster (calculated as the average distance of points in a cluster from the centroid) and maintain adequate separation between clusters (no overlap between two clusters)?\"\n\nI found that 3 clusters were optimal in terms of most inertia improvement while keeping a high silhouette score (around 0.55).\n\n![6 auctions were used for this analysis](https://images.mirror-media.xyz/publication-images/7bef376a-b96c-42de-87b6-991db4503ef9.png?height=264&width=393)\n\nWith clusters chosen, we want to be able to visualize and verify their existence. There are over 15 variables, so we need to reduce the number of dimensions in order to plot it. Dimension reduction typically relies on either PCA or t-SNE algorithms, in our case I went with t-SNE. Don't worry too much about understanding this part, these algorithms essentially capture the variance across all features to give us X and Y components that maximize the spread of points from each other.\n\n**Let's look at project 118, LeWitt Generator Generator, from August 4th:**\n\n![](https://images.mirror-media.xyz/publication-images/4f7dad88-47b6-4dde-9a52-f0e4258cb829.png?height=605&width=615)\n\n![These are the sub-distributions for each variable by cluster, calculated using a KDE. The colors match those in the above clusters.](https://images.mirror-media.xyz/publication-images/5bafdebf-7b9b-43b3-8ed0-7e4ac923725f.png?height=856&width=1123)\n\nAfter taking a look at the sub-distributions for each variable and some data examples, I was able to categorize the clusters. The Orange cluster is the group that is speeding up the most while also submitting slightly lower gas transactions on average. Blue and Green clusters exhibit similar behavior to each other, but addresses in Blue typically have less history than addresses in the Green cluster.\n\nLooking at the overall picture, it seems the original hypothesis on \"speed-up\" and \"set high and set low\" producing two groups each was wrong. Instead, we have a single \"speed-up\" group (Orange) and a single \"set-and-forget\" group (Blue and Green are behaviorally the same). I think the new (Blue) versus old wallets (Green) in the \"set and forget\" group probably carries a lot of overlap in actual users, where users just created new wallets to bid on more mints. Based on their impatience and lower-than-average gas prices, the \"speed-up\" group reads to me as either less experienced or just more greedy than other users.  *What did surprise me is that the speed-up group makes up a smaller proportion of total bidders, as I had expected that group to make up 60-70% of bidders instead of 30%.*\n\n**Now, where this user behavior study gets really interesting is when comparing project 118 (the set price of 0.75 ETH) to project 140 (Dutch auction with the price decreasing from 1.559 to .159 ETH).**\n\n**Here's the clustering for project 140, Good Vibrations from August 21st:**\n\n![](https://images.mirror-media.xyz/publication-images/ae090b2e-141c-4dd0-a0cb-06f4ed45dd13.png?height=605&width=615)\n\n![](https://images.mirror-media.xyz/publication-images/7294dd21-59e5-48fd-b7a1-7babbda77040.png?height=856&width=1130)\n\nWe can see that now most of the clustering variability comes from `block_entry`, `price_eth`, and all of the `gas_behavior` features. This is a big departure from the main variables for project 118. In 118, the set price meant that people entered the auction in a fairly uniform distribution (quantity left didn't seem to matter), and the \"speed-up\" group makes actions up fairly endlessly - probably very anxious.\n\nIn project 140, we don't see the same difference in actions within `average_action_delay` or `total_actions` , instead, we see probably that same \"speed-up\" group entering at a very late stage block and setting far below-average gas prices as seen in `average_gas_behavior`. The Green cluster probably represents users with more experience than the Orange cluster, but who are still transitioning between Orange and Blue in terms of their behavior. If I was to try and map this to the clusters in 118, I believe the \"speed-up\" group is now the \"greedy\" group (Orange) that is entering late and bidding a low gas. The \"set-and-forget\" group maps pretty well to the \"early-grab\" group (Green and Blue), where they all exhibit pretty good patience and an adequate safety net on gas bids.\n\n**I call the Orange group \"greedy\" not just because of their behavior, but also because of their rate of failed transactions.**\n\nFor project 118, fail rates are of the \"speed-up\" versus \"set-and-forget\" groups are within 10-15%.\n\n![percent_lost takes (cancel + dropped + failed) / number_submitted](https://images.mirror-media.xyz/publication-images/306752cb-87c2-4494-8915-86102c214d29.png?height=77&width=724)\n\nFor project 140, the fail rate of the \"greedy\" cluster is around 69% versus the \"early-grab\" group at around 5-15%.\n\n![](https://images.mirror-media.xyz/publication-images/55ceaf8d-6756-4807-beb9-fcfbbe4e549e.png?height=84&width=727)\n\n**Overall, my read of this is that the group's bad habits were amplified - it feels to me like we made a tradeoff in anxiety → greed. This may have made the auction less stressful, but ultimately led to more users being upset (due to failed mints).**\n\n*I'm sure there's a more granular analysis that can be done to segment the auctions further based on factory/curated/playground or by artists themselves too. This will only get more interesting and complex as the community continues to grow, and emotions play a larger factor in both a single auction and on if they return for future auctions.*\n\n*This study of multiple auctions helped us validate our assumptions, understand the proportions of user groups, and see how users' good or bad behaviors shift over time (and other parameters). Now we need to plug it into the rest of the product cycle process.*\n\n## Where We Go From Here:\n\nThe reason that I chose just Artblocks auctions for this and not a mix of platforms is because I wanted to look for a place where the variability in terms of interface and project type is mostly controlled. This should have given us fairly consistent users and behavior types.\n\nThis is just the start of a UX research cycle, so ideally we could continue in the following steps:\n\n1. Use an unsupervised machine learning algorithm to identify the user groups (clusters) and see how many people are making \"mistakes\" when entering an auction.  *This is the step we covered today.*\n2. Create a new user interface, such as a [histogram view on the bidding screen](https://twitter.com/andrewhong5297/status/1423020670825504768), or showing historical data on when most people usually enter/crowd the auction and at what prices. Anything to give both current and historical context to the user, especially those from the speed cluster.\n3. With each auction, run the mempool/wallet data through the created algorithm to see if the user groups have shifted and if specific users have \"learned\" to participate in the auction differently (i.e. did they move between user groups). *I think that the most value can be found in this step if done well. Using ENS or other identifiers to help supplement this grouping would be exponentially helpful too*\n4. Based on the results, continue to iterate on the user interface and design. You could also run more informed A/B testing since you could even figure out what screens to show by making an educated guess based on the users' last cluster (or use label propagation for new users).\n\n**The Dutch auction-style change is also an example of step #2, and we were able to see a clear shift in user behaviors.**   While typically this sort of A/B testing is focused on increasing engagement or conversions, here we are optimizing for the user's ability to learn and improve instead. This may become even more robust if this was iterated in a multiplatform context, so that we can study how someone is learning at an ecosystem level (maybe even supplement with Rabbithole data and user profiles). Since my Artblocks user research is all based on publicly sourced data, it can be replicated and supplemented by any other auction/sale platform. **Crypto could be the first industry that would have a synchronized and transparent set of user groups and UX research, to be applied in products and academia.** Nansen wallet labeling is already a step towards this, but it's different when teams from different products build this up from various facets and approaches.\n\nWhat I would eventually envision is using data to build on the following user personas (with subgroups/levels within them):\n\n* I want to buy a Fidenza, so I can either buy one through a private sale, bid on one in an auction myself, bid on one in a prtyDAO bid auction, or buy a fraction of one with [fractional.art](https://fractional.art/)\n* I like Fidenzas in general, so I'll just buy the NFTX Fidenza index token or an NFT basket of Artblocks Curated on [fractional.art](https://fractional.art/)\n* I'm a collector already, so I want to swap or bid on a Fidenza using a curated set of NFTs and ERC20s I already hold (using genie-xyz swap).\n* I like the rush of acquiring through initial mint versus secondary market, and heavily participate in auctions like Artblocks live mints.\n\nI hope you found this project interesting and/or helpful, I had a ton of fun with it. Thanks to the folks at Blocknative for setting me up, and the community at Artblocks for answering my many auction questions. As always, feel free to reach out with any questions or ideas you may have!\n\nYou can find the GitHub repo [with all the data and script here](https://github.com/andrewhong5297/artblock_auctions_analysis). The script may be a bit hard to read since I’m still refactoring and cleaning it up. The script and some of the analysis here may get updated as I analyze the last few auctions of August for new patterns.", "timestamp": 1629646747, "digest": "l_-4fQ08cpxUZpn9V9S5R27wfKvNgdnrXlZAWZWvdlg", "contributor": "0x2Ae8c972fB2E6c00ddED8986E2dc672ED190DA06"}
{"id": "Xv4Dv_hzPS9CWP_WSP7y0-siHjvEd4CPb3j2OyY0AIc", "title": "UX Research to Improve NFT Auction Experience Using the Mempool", "body": "User experience (UX) describes how people feel when they interact with a system or service and encompasses several factors including usability, design, marketing, accessibility, performance, comfort, and utility. Don Norman once said,\n\n> “Everything has a personality; everything sends an emotional signal. Even where this was not the intention of the designer, the people who view the website infer personalities and experience emotions. Bad websites have horrible personalities and instill horrid emotional states in their users, usually unwittingly. We need to design things — products, websites, services — to convey whatever personality and emotions are desired.”\n\nEthereum's personality is someone who is extremely inscrutable and easily misunderstood. To make matters worse, most users don't even think about it as interacting with Ethereum when they're using your interface or a wallet. If you're ever in the live chat of an Artblock's auction, you'll notice as soon as an auction ends there are at least a dozen people complaining that it is Metamask's fault that they didn't get a mint. I think in the last year the UX of many dapps on Ethereum has improved greatly in both product interaction and explainability of transactions. For the most part, dapps don't just leave you with a loading spinner after you sign the transaction anymore.\n\nEven with the design of dapps is generally improving, I'm not sure how deep UX research has gone yet. When I see data analytics or research on various protocols, users are mostly treated as homogenous. This is somewhat shifting with some of the analysis I've seen of Uniswap V3 liquidity providers and Rabbithole Questers, but even those are still heavily focused on just confirmed transactions on-chain. From my own experience, most of the emotion evoked and behavioral quirks happen when I'm submitting, waiting, speeding up, or canceling my transaction. For some applications, the user might leave and go do something else after submitting a transaction. But for a product like Artblock's auctions, they're going to stay around until the confirmation has happened, likely checking anything they can for updates and with compounding anxiety.\n\nI think we can do a much better job of understanding user behaviors and frictions by starting to leverage the mempool more. The [mempool](https://compassmining.io/education/what-is-a-mempool/) is where unconfirmed transactions are temporarily stored by a node. This means if you submit, speed up, or cancel your transaction then those actions will show up in the mempool first. It's important to note that the data from the mempool is not stored in the node, so you can't query historical data the same way you could on confirmed transactions. From here, you can see that they submitted a few transactions, sped up the transactions quite a few times but nowhere near the gas price needed, and ultimately saw confirmations 20 blocks later. I believe this is a pretty good proxy for user experience and likely emotions they were feeling throughout the whole process. If we understand how different groups of users behave in this cycle, we can figure out how to supplement their decision-making or ease their anxiety. To my knowledge, pretty much only the Ethereum Foundation, All Core Devs, and some wallet teams leverage the mempool data for UX reasons.\n\n**UX Research Thesis:** By looking at a user's behavior through auctions over time and also their wallet history, we can start to give behavioral identities to different groups of users. From here, we can identify the main issues to try and alleviate. We'll do this by taking a month of Artblocks Auctions data using [Blocknative](https://www.blocknative.com/), and layering in the history of these addresses using [Dune queries](https://dune.xyz/).\n\nThis article will be more technical than some of my previous ones since I believe the work can and should be generalized fairly easily. *I want to emphasize that my background is not in UX research, I'm purely experimenting with what I think crypto-native UX research could look like.*\n\n## Data Sourcing and Preprocessing All Auction Data\n\n*If you have no interest in the technical bits, skip to the next section on Feature Engineering*\n\n### Blocknative and Mempool Data Streaming\n\nUsing Blocknative's Mempool explorer, you can filter for transactions submitted to specific contracts or from specific wallets. In my case, I wanted to listen to the whitelisted minter contracts for Artblock's NFT contract. You can find the stream that I used [here](https://tinyurl.com/yftus9h2), and save it down if you want to use the exact same setup.\n\nYou can find the whitelisted minter addresses with the following query [in their subgraph](https://thegraph.com/legacy-explorer/subgraph/artblocks/art-blocks):\n\n```python\n{\n  contracts(first: 2) {\n    id\n    mintWhitelisted\n  }\n}\n```\n\nThere were three steps to get to a subscription filter for all purchases:\n\n1. Add the new address with the \"create new subscription\" button\n2. Add the ABIs by clicking the \"ABI\" button next to the address. In my case I just needed the \"purchase\" function.\n\n   ```python\n   {\n       \"inputs\": [\n         {\n           \"internalType\": \"uint256\",\n           \"name\": \"_projectId\",\n           \"type\": \"uint256\"\n         }\n       ],\n       \"name\": \"purchase\",\n       \"outputs\": [\n         {\n           \"internalType\": \"uint256\",\n           \"name\": \"_tokenId\",\n           \"type\": \"uint256\"\n         }\n       ],\n       \"stateMutability\": \"payable\",\n       \"type\": \"function\"\n     }\n   ```\n3. Add filters for the `methodName` matches `purchase` (make sure you don't do a global filter)\n\n   ![](https://images.mirror-media.xyz/publication-images/c0f07bee-6562-43d3-8174-28e47af26b28.png?height=483&width=562)\n\n\nIn the end, your setup should look like this:\n\n![](https://images.mirror-media.xyz/publication-images/ad1f283a-7ce1-4899-9fa8-bdfda6a716e1.png?height=577&width=844)\n\nTo store this data down, I created a ngrok/express endpoint to store in an SQLite database, run locally. I've created a [GitHub template with steps to replicate this setup](https://github.com/andrewhong5297/blocknative_stream_to_sql). Probably the most important point to remember here is that you need to include the POST endpoint as part of the ngrok URL when adding it as a webhook in the Blocknative account page.\n\n### Key Preprocessing Functions\n\n**Multiple Transaction Hashes**\n\nWhen you speed up or cancel a transaction, the original transaction hash is replaced with the new transaction. This means if you want to track a users' transaction through its full lifecycle, you need to reconcile the new transaction hashes with the original one. Assuming you speed up a transaction five times, you'll have six hashes total (the original hash + five new hashes). I reconciled this by getting a dictionary mapping of `tx_hash` to the new `replaceHash`, and then replaced recursively.\n\n```python\nreplaceHashKeys = dict(zip(auctions[\"replaceHash\"],auctions[\"tx_hash\"])) #assign tx_hash based on replacements, just to keep consistency. \nreplaceHashKeys.pop(\"none\") #remove none key\n\ndef recursive_tx_search(key):\n    if key in replaceHashKeys:\n        return recursive_tx_search(replaceHashKeys[key])\n    else:\n        return key\n\nauctions[\"tx_hash\"] = auctions[\"tx_hash\"].apply(lambda x: recursive_tx_search(x))\n```\n\n**Blocknumber Issues**\n\nDropped transactions have a `blocknumber` of 0, so to deal with this I sorted my dataframe by `timestamp` in ascending order, and then did a backward fill so the 0 would be replaced by the correct `blocknumber` it was dropped in. This is important fix for feature engineering.\n\n```python\nauctions = auctions.sort_values(by=\"timestamp\",ascending=True)\nauctions[\"blocknumber\"] = auctions[\"blocknumber\"].replace(to_replace=0, method='bfill') #deal with dropped txs that show as blocknumber 0\n```\n\n**Dealing with Mints Outside of Main Auction Period**\n\nFor most projects, the artist will mint a few pieces before the auction opens up to the public. Some projects don't sell out right away, so you will get mints still occurring a few days after the auction has opened. My analysis is focused on the key auction period, mostly the first 30 minutes. To get rid of the two mint cases above, I removed outliers based on `blocknumber`.\n\n```python\nto_remove_indicies = []\nfor project in list(set(auctions[\"projectId\"])):\n    auction_spec = auctions[auctions[\"projectId\"]==project]\n    all_times = pd.Series(list(set(auction_spec.blocknumber)))\n    to_remove_blocktimes = all_times[(np.abs(stats.zscore(all_times)) > 2.5)]\n    if len(to_remove_blocktimes)==0:\n        break\n    to_remove_indicies.extend(auction_spec.index[auction_spec['blocknumber'].isin(to_remove_blocktimes)].tolist())\nauctions.drop(index=to_remove_indicies, inplace=True)\n```\n\n**Adding on Dutch Auction Prices**\n\nFor all projects in the dataset besides project 118, a Dutch auction price format was used. I took the mint price data using a [dune query](https://dune.xyz/queries/113834), and then merged it onto the dataset. I had to use a forward and backward fill for the blocks that had mempool actions but no confirmations during the auction. \n\n```python\nauction_prices = pd.read_csv(r'artblock_auctions_analytics/datasets/dune_auction_prices.csv', index_col=0)\nauctions = pd.merge(auctions,auction_prices, how=\"left\", left_on=[\"projectId\",\"blocknumber\"],right_on=[\"projectId\",\"blocknumber\"])\nauctions.sort_values(by=[\"projectId\",\"blocknumber\"], ascending=True, inplace=True)\nauctions[\"price_eth\"].fillna(method=\"ffill\", inplace=True)\nauctions[\"price_eth\"].fillna(method=\"bfill\", inplace=True)\n```\n\n## Feature Engineering For Each Auction\n\n*If you have no interest in the technical bits, just read the parts in bold and skip the rest.*\n\nIn data science, a feature is a variable that is calculated from the larger dataset to be used as an input in some sort of model or algorithm. All features are calculated in the `preprocess_auction` function and are calculated *per auction rather than combining all the auctions into a feature set.*\n\n**The first set of features are the totals for transaction states**, and was a simple `pivot_table` function:\n\n* `number_submitted` : total number of transactions submitted\n* `cancel` : count of transactions that ended in canceled\n* `failed`: count of transactions that ended in failed\n* `dropped`: count of transactions that ended in dropped\n* `confirmed`: count of transactions that ended in confirmed\n\nI mentioned earlier some data was not captured for auctions due to various issues, these transactions were dropped from the dataset.\n\n**The next set of features included their gas behavior.** The key concept here was capturing how far away their transaction gas was from the average confirmed gas per block (shifted by 1 block). **Then we can create features for the average, median, and standard deviation of gas price distance over the whole auction.** There are a bunch of transposes and index resets to get the `blocknumber` columns in the right order, but the important function is `fill_pending_values_gas` which forward fills the gas price between actions captured. This means that if I put in a transaction at `blocknumber` 1000 with a gas of 0.05 ETH and my next action wasn't until `blocknumber` 1005 where I sped up to 0.1 ETH gas, then this function will fill in the blocks between 1000-1005 with 0.05 ETH.\n\n```python\ndef fill_pending_values_gas(x):\n    first = x.first_valid_index()\n    last = x.last_valid_index()\n    x.loc[first:last] = x.loc[first:last].fillna(method=\"ffill\")\n    return x\n```\n\n**The third set of features were calculating the total number and frequency of actions taken in the auction.** Here we start with a pivot of total actions (speed ups) per block, with some special calculations for getting the first instance of pending for each transaction:\n\n```python\nget_first_pending = df[df[\"status\"]==\"pending\"] #first submitted \nget_first_pending = get_first_pending.drop_duplicates(subset=[\"tx_hash\",\"status\"], keep=\"first\")\nauctions_time_data = pd.concat([get_first_pending,df[df[\"status\"]==\"speedup\"]], axis=0)\ntime_action = auctions_time_data.pivot_table(index=[\"sender\",\"tx_hash\"], columns=\"blocknumber\",values=\"status\",aggfunc=\"count\") \\\n                    .reindex(set(df[\"blocknumber\"]), axis=1, fill_value=np.nan)\n```\n\nFrom here we get to `average_action_delay` in three steps:\n\n1. we take the number of actions per block (yes some people sped up transactions multiple times in the same block)\n2. we drop the blocks with no actions, and then take the difference between the blocknumbers that remain. We add a 0 for each additional action taken per block.\n3. Taking the mean across the differences and added zeroes gives us the `average_action_delay`\n\n```python\ndef get_actions_diff(row):\n    row = row.dropna().reset_index()\n    actions_diff_nominal =list(row[\"blocknumber\"].diff(1).fillna(0))\n \n    #take the blocks with muliple actions and subtract one, then sum up. \n    zeros_to_add = sum([ actions - 1 if actions > 1 else 0 for actions in row[row.columns[1]]])\n    actions_diff_nominal.extend(list(np.zeros(int(zeros_to_add))))\n    actions_diff = np.mean(actions_diff_nominal)\n    if (actions_diff==0) and (zeros_to_add==0):\n        return 2000 #meaning they never took another action\n    else:\n        return actions_diff\n```\n\n`total_actions` is much simpler, as it is just the sum of actions across the pivot.\n\n```python\ntime_action[\"total_actions\"] = time_action.iloc[:,:-1].sum(axis=1)\n```\n\n**The last time-dependent feature is** `block_entry`, which is an important one due to the introduction of Dutch auctions. Essentially this tracks what block the transaction was submitted on since the start.\n\n```python\nget_first_pending[\"block_entry\"] =   get_first_pending[\"blocknumber\"] - get_first_pending[\"blocknumber\"].min()\n\nentry_pivot = get_first_pending.pivot_table(index=\"sender\",values=\"block_entry\",aggfunc=\"min\")\n```\n\n`price_eth` is added as a feature as well, which is tied to the `block_entry` point.\n\n**The last set of features were based on a Dune query, specifically the days since the first transaction, total gas used in transactions, and the total number of transactions.** To get the address array in the right format I used the following line of code after reading in the SQL data:\n\n```python\nall_users = list(set(auctions[\"sender\"].apply(lambda x: x.replace('0x','\\\\x'))))\nall_users_string = \"('\" + \"'),('\".join(all_users) + \"')\"\n```\n\nThe dune query for this is fairly simple. I pasted the addresses string under `VALUES`, and made some CTEs to get the features I wanted. In the final `SELECT` I tried to add each address's ens as well. You can find the query here: <https://dune.xyz/queries/96523>\n\nLastly, we just merge in the per wallet data on days active, total gas used, and the total number of transactions.\n\n```python\nauctions_all_df = pd.merge(auctions_all_df,wh,on=\"sender\",how=\"left\")\nauctions_all_df.set_index([\"sender\",\"ens\"],inplace=True)\n```\n\n*With all this done, we're finally ready to run some fun unsupervised learning algorithms and try and validate our hypothesis on user groups.*\n\n## Clustering and Visualizing User Groups\n\nBefore I started this project, I had expected to see the following user groups pop out of the data:\n\n* **Set and Forget:** There should be two groups here, those who set a transaction with really high gas and an average/low gas and then don't touch it for the rest of the auction.\n* **Speed Up:** There should be two groups here as well, those who are speeding up often and updating the transaction directly as a factor of gas prices and those who are speeding up often but with basically no change in gas price.\n\nI was very interested in validating these groups, seeing how large each group was, and seeing if any users moved between groups over the course of many auctions. The easiest way to do this was to use unsupervised machine learning to identify clusters of user groups based on the variability across all features. Essentially this is like looking at the distribution of income in a state and then splitting it into sub-distributions for different income concentrations, geographic coordinates, and age. Note that this isn't binning, where the distribution is split into equal ranges - it is instead calculated based on the density of observations within the whole range. The approach we will take is called \"unsupervised\" because our dataset doesn't have any existing labels, rather than something like a regression where there is a value being predicted that can be verified as right or wrong.\n\nThe algorithm I decided to use is called [k-means](https://stanford.edu/\\~cpiech/cs221/handouts/kmeans.html), where k stands for the number of clusters you are expecting to identify. Each cluster has a \"centroid\", which is like the center of a circle. There are various methods for figuring out how many clusters are optimal, and the two I used were elbow points and silhouette scores. Those are both fancy ways of asking,\n\n> \"does each additional cluster help increase the density of the cluster (calculated as the average distance of points in a cluster from the centroid) and maintain adequate separation between clusters (no overlap between two clusters)?\"\n\nI found that 3 clusters were optimal in terms of most inertia improvement while keeping a high silhouette score (around 0.55).\n\n![6 auctions were used for this analysis](https://images.mirror-media.xyz/publication-images/7bef376a-b96c-42de-87b6-991db4503ef9.png?height=264&width=393)\n\nWith clusters chosen, we want to be able to visualize and verify their existence. There are over 15 variables, so we need to reduce the number of dimensions in order to plot it. Dimension reduction typically relies on either PCA or t-SNE algorithms, in our case I went with t-SNE. Don't worry too much about understanding this part, these algorithms essentially capture the variance across all features to give us X and Y components that maximize the spread of points from each other.\n\n**Let's look at project 118, LeWitt Generator Generator, from August 4th:**\n\n![](https://images.mirror-media.xyz/publication-images/4f7dad88-47b6-4dde-9a52-f0e4258cb829.png?height=605&width=615)\n\n![These are the sub-distributions for each variable by cluster, calculated using a KDE. The colors match those in the above clusters.](https://images.mirror-media.xyz/publication-images/5bafdebf-7b9b-43b3-8ed0-7e4ac923725f.png?height=856&width=1123)\n\nAfter taking a look at the sub-distributions for each variable and some data examples, I was able to categorize the clusters. The Orange cluster is the group that is speeding up the most while also submitting slightly lower gas transactions on average. Blue and Green clusters exhibit similar behavior to each other, but addresses in Blue typically have less history than addresses in the Green cluster.\n\nLooking at the overall picture, it seems the original hypothesis on \"speed-up\" and \"set high and set low\" producing two groups each was wrong. Instead, we have a single \"speed-up\" group (Orange) and a single \"set-and-forget\" group (Blue and Green are behaviorally the same). I think the new (Blue) versus old wallets (Green) in the \"set and forget\" group probably carries a lot of overlap in actual users, where users just created new wallets to bid on more mints. Based on their impatience and lower-than-average gas prices, the \"speed-up\" group reads to me as either less experienced or just more greedy than other users. *What did surprise me is that the speed-up group makes up a smaller proportion of total bidders, as I had expected that group to make up 60-70% of bidders instead of 30%.*\n\n**Now, where this user behavior study gets really interesting is when comparing project 118 (the set price of 0.75 ETH) to project 140 (Dutch auction with the price decreasing from 1.559 to .159 ETH).**\n\n**Here's the clustering for project 140, Good Vibrations from August 21st:**\n\n![](https://images.mirror-media.xyz/publication-images/ae090b2e-141c-4dd0-a0cb-06f4ed45dd13.png?height=605&width=615)\n\n![](https://images.mirror-media.xyz/publication-images/7294dd21-59e5-48fd-b7a1-7babbda77040.png?height=856&width=1130)\n\nWe can see that now most of the clustering variability comes from `block_entry`, `price_eth`, and all of the `gas_behavior` features. This is a big departure from the main variables for project 118. In 118, the set price meant that people entered the auction in a fairly uniform distribution (quantity left didn't seem to matter), and the \"speed-up\" group makes actions up fairly endlessly - probably very anxious.\n\nIn project 140, we don't see the same difference in actions within `average_action_delay` or `total_actions` , instead, we see probably that same \"speed-up\" group entering at a very late stage block and setting far below-average gas prices as seen in `average_gas_behavior`. The Green cluster probably represents users with more experience than the Orange cluster, but who are still transitioning between Orange and Blue in terms of their behavior. If I was to try and map this to the clusters in 118, I believe the \"speed-up\" group is now the \"greedy\" group (Orange) that is entering late and bidding a low gas. The \"set-and-forget\" group maps pretty well to the \"early-grab\" group (Green and Blue), where they all exhibit pretty good patience and an adequate safety net on gas bids.\n\n**I call the Orange group \"greedy\" not just because of their behavior, but also because of their rate of failed transactions.**\n\nFor project 118, fail rates are of the \"speed-up\" versus \"set-and-forget\" groups are within 10-15%.\n\n![percent_lost takes (cancel + dropped + failed) / number_submitted](https://images.mirror-media.xyz/publication-images/306752cb-87c2-4494-8915-86102c214d29.png?height=77&width=724)\n\nFor project 140, the fail rate of the \"greedy\" cluster is around 69% versus the \"early-grab\" group at around 5-15%.\n\n![](https://images.mirror-media.xyz/publication-images/55ceaf8d-6756-4807-beb9-fcfbbe4e549e.png?height=84&width=727)\n\n**Overall, my read of this is that the group's bad habits were amplified - it feels to me like we made a tradeoff in anxiety → greed. This may have made the auction less stressful, but ultimately led to more users being upset (due to failed mints).**\n\n*I'm sure there's a more granular analysis that can be done to segment the auctions further based on factory/curated/playground or by artists themselves too. This will only get more interesting and complex as the community continues to grow, and emotions play a larger factor in both a single auction and on if they return for future auctions.*\n\n*This study of multiple auctions helped us validate our assumptions, understand the proportions of user groups, and see how users' good or bad behaviors shift over time (and other parameters). Now we need to plug it into the rest of the product cycle process.*\n\n## Where We Go From Here:\n\nThe reason that I chose just Artblocks auctions for this and not a mix of platforms is because I wanted to look for a place where the variability in terms of interface and project type is mostly controlled. This should have given us fairly consistent users and behavior types.\n\nThis is just the start of a UX research cycle, so ideally we could continue in the following steps:\n\n1. Use an unsupervised machine learning algorithm to identify the user groups (clusters) and see how many people are making \"mistakes\" when entering an auction. *This is the step we covered today.*\n2. Create a new user interface, such as a [histogram view on the bidding screen](https://twitter.com/andrewhong5297/status/1423020670825504768), or showing historical data on when most people usually enter/crowd the auction and at what prices. Anything to give both current and historical context to the user, especially those from the speed cluster. \n3. With each auction, run the mempool/wallet data through the created algorithm to see if the user groups have shifted and if specific users have \"learned\" to participate in the auction differently (i.e. did they move between user groups). *I think that the most value can be found in this step if done well. Using ENS or other identifiers to help supplement this grouping would be exponentially helpful too*\n4. Based on the results, continue to iterate on the user interface and design. You could also run more informed A/B testing since you could even figure out what screens to show by making an educated guess based on the users' last cluster (or use label propagation for new users).\n\n**The Dutch auction-style change is also an example of step #2, and we were able to see a clear shift in user behaviors.** While typically this sort of A/B testing is focused on increasing engagement or conversions, here we are optimizing for the user's ability to learn and improve instead. This may become even more robust if this was iterated in a multiplatform context, so that we can study how someone is learning at an ecosystem level (maybe even supplement with Rabbithole data and user profiles). Since my Artblocks user research is all based on publicly sourced data, it can be replicated and supplemented by any other auction/sale platform. **Crypto could be the first industry that would have a synchronized and transparent set of user groups and UX research, to be applied in products and academia.** Nansen wallet labeling is already a step towards this, but it's different when teams from different products build this up from various facets and approaches.\n\nWhat I would eventually envision is using data to build on the following user personas (with subgroups/levels within them):\n\n* I want to buy a Fidenza, so I can either buy one through a private sale, bid on one in an auction myself, bid on one in a prtyDAO bid auction, or buy a fraction of one with [fractional.art](https://fractional.art/)\n* I like Fidenzas in general, so I'll just buy the NFTX Fidenza index token or an NFT basket of Artblocks Curated on [fractional.art](https://fractional.art/)\n* I'm a collector already, so I want to swap or bid on a Fidenza using a curated set of NFTs and ERC20s I already hold (using genie-xyz swap).\n* I like the rush of acquiring through initial mint versus secondary market, and heavily participate in auctions like Artblocks live mints.\n\nI hope you found this project interesting and/or helpful, I had a ton of fun with it. Thanks to the folks at Blocknative for setting me up, and the community at Artblocks for answering my many auction questions. As always, feel free to reach out with any questions or ideas you may have!\n\nYou can find the GitHub repo [with all the data and script here](https://github.com/andrewhong5297/artblock_auctions_analysis). The script may be a bit hard to read since I’m still refactoring and cleaning it up. The script and some of the analysis here may get updated as I analyze the last few auctions of August for new patterns. ", "timestamp": 1629646728, "digest": "l_-4fQ08cpxUZpn9V9S5R27wfKvNgdnrXlZAWZWvdlg", "contributor": "0x2Ae8c972fB2E6c00ddED8986E2dc672ED190DA06"}
{"id": "dzQjuDSeCuyKY5-48TS8hU1mtYExLCzJr74LLe76z4Y", "title": "UX Research to Improve NFT Auction Experience, using Mempool Data", "body": "User experience (UX) describes how people feel when they interact with a system or service and encompasses several factors including usability, design, marketing, accessibility, performance, comfort, and utility. Don Norman once said,\n\n> “Everything has a personality; everything sends an emotional signal. Even where this was not the intention of the designer, the people who view the website infer personalities and experience emotions. Bad websites have horrible personalities and instill horrid emotional states in their users, usually unwittingly. We need to design things — products, websites, services — to convey whatever personality and emotions are desired.”\n\nEthereum's personality is someone who is extremely inscrutable and easily misunderstood. To make matters worse, most users don't even think about it as interacting with Ethereum when they're using your interface or a wallet. If you're ever in the live chat of an Artblock's auction, you'll notice as soon as an auction ends there are at least a dozen people complaining that it is Metamask's fault that they didn't get a mint. I think in the last year the UX of many dapps on Ethereum has improved greatly in both product interaction and explainability of transactions. For the most part, dapps don't just leave you with a loading spinner after you sign the transaction anymore.\n\nEven with the design of dapps is generally improving, I'm not sure how deep UX research has gone yet. When I see data analytics or research on various protocols, users are mostly treated as homogenous. This is somewhat shifting with some of the analysis I've seen of Uniswap V3 liquidity providers and Rabbithole Questers, but even those are still heavily focused on just confirmed transactions on-chain. From my own experience, most of the emotion evoked and behavioral quirks happen when I'm submitting, waiting, speeding up, or canceling my transaction. For some applications, the user might leave and go do something else after submitting a transaction. But for a product like Artblock's auctions, they're going to stay around until the confirmation has happened, likely checking anything they can for updates and with compounding anxiety.\n\nI think we can do a much better job of understanding user behaviors and frictions by starting to leverage the mempool more. The [mempool](https://compassmining.io/education/what-is-a-mempool/) is where unconfirmed transactions are temporarily stored by a node. This means if you submit, speed up, or cancel your transaction then those actions will show up in the mempool first. It's important to note that the data from the mempool is not stored in the node, so you can't query historical data the same way you could on confirmed transactions. From here, you can see that they submitted a few transactions, sped up the transactions quite a few times but nowhere near the gas price needed, and ultimately saw confirmations 20 blocks later. I believe this is a pretty good proxy for user experience and likely emotions they were feeling throughout the whole process. If we understand how different groups of users behave in this cycle, we can figure out how to supplement their decision-making or ease their anxiety. To my knowledge, pretty much only the Ethereum Foundation, All Core Devs, and some wallet teams leverage the mempool data for UX reasons.\n\n**UX Research Thesis:** By looking at a user's behavior through auctions over time and also their wallet history, we can start to give behavioral identities to different groups of users. From here, we can identify the main issues to try and alleviate. We'll do this by taking a month of Artblocks Auctions data using [Blocknative](https://www.blocknative.com/), and layering in the history of these addresses using [Dune queries](https://dune.xyz/).\n\nThis article will be more technical than some of my previous ones since I believe the work can and should be generalized fairly easily.    *I want to emphasize that my background is not in UX research, I'm purely experimenting with what I think crypto-native UX research could look like.*\n\n## Data Sourcing and Preprocessing All Auction Data\n\n*If you have no interest in the technical bits, skip to the next section on Feature Engineering*\n\n### Blocknative and Mempool Data Streaming\n\nUsing Blocknative's Mempool explorer, you can filter for transactions submitted to specific contracts or from specific wallets. In my case, I wanted to listen to the whitelisted minter contracts for Artblock's NFT contract. You can find the stream that I used [here](https://tinyurl.com/yftus9h2), and save it down if you want to use the exact same setup.\n\nYou can find the whitelisted minter addresses with the following query [in their subgraph](https://thegraph.com/legacy-explorer/subgraph/artblocks/art-blocks):\n\n```python\n{\n  contracts(first: 2) {\n    id\n    mintWhitelisted\n  }\n}\n```\n\nThere were three steps to get to a subscription filter for all purchases:\n\n1. Add the new address with the \"create new subscription\" button\n2. Add the ABIs by clicking the \"ABI\" button next to the address. In my case I just needed the \"purchase\" function.\n\n```python\n{\n    \"inputs\": [\n      {\n        \"internalType\": \"uint256\",\n        \"name\": \"_projectId\",\n        \"type\": \"uint256\"\n      }\n    ],\n    \"name\": \"purchase\",\n    \"outputs\": [\n      {\n        \"internalType\": \"uint256\",\n        \"name\": \"_tokenId\",\n        \"type\": \"uint256\"\n      }\n    ],\n    \"stateMutability\": \"payable\",\n    \"type\": \"function\"\n  }\n```\n\n1. Add filters for the `methodName` matches `purchase` (make sure you don't do a global filter)\n\nIn the end, your setup should look like this:\n\n![](https://images.mirror-media.xyz/publication-images/ad1f283a-7ce1-4899-9fa8-bdfda6a716e1.png?height=577&width=844)\n\nTo store this data down, I created a ngrok/express endpoint to store in an SQLite database, run locally. I've created a [GitHub template with steps to replicate this setup](https://github.com/andrewhong5297/blocknative_stream_to_sql). Probably the most important point to remember here is that you need to include the POST endpoint as part of the ngrok URL when adding it as a webhook in the Blocknative account page.\n\n### Key Preprocessing Functions\n\n**Multiple Transaction Hashes**\n\nWhen you speed up or cancel a transaction, the original transaction hash is replaced with the new transaction. This means if you want to track a users' transaction through its full lifecycle, you need to reconcile the new transaction hashes with the original one. Assuming you speed up a transaction five times, you'll have six hashes total (the original hash + five new hashes). I reconciled this by getting a dictionary mapping of `tx_hash` to the new `replaceHash`, and then replaced recursively.\n\n```python\nreplaceHashKeys = dict(zip(auctions[\"replaceHash\"],auctions[\"tx_hash\"])) #assign tx_hash based on replacements, just to keep consistency. \nreplaceHashKeys.pop(\"none\") #remove none key\n\ndef recursive_tx_search(key):\n    if key in replaceHashKeys:\n        return recursive_tx_search(replaceHashKeys[key])\n    else:\n        return key\n\nauctions[\"tx_hash\"] = auctions[\"tx_hash\"].apply(lambda x: recursive_tx_search(x))\n```\n\n**Blocknumber Issues**\n\nDropped transactions have a `blocknumber` of 0, so to deal with this I sorted my dataframe by `timestamp` in ascending order, and then did a backward fill so the 0 would be replaced by the correct `blocknumber` it was dropped in. This is important fix for feature engineering.\n\n```python\nauctions = auctions.sort_values(by=\"timestamp\",ascending=True)\nauctions[\"blocknumber\"] = auctions[\"blocknumber\"].replace(to_replace=0, method='bfill') #deal with dropped txs that show as blocknumber 0\n```\n\n**Dealing with Mints Outside of Main Auction Period**\n\nFor most projects, the artist will mint a few pieces before the auction opens up to the public. Some projects don't sell out right away, so you will get mints still occurring a few days after the auction has opened. My analysis is focused on the key auction period, mostly the first 30 minutes. To get rid of the two mint cases above, I removed outliers based on `blocknumber`.\n\n```python\nto_remove_indicies = []\nfor project in list(set(auctions[\"projectId\"])):\n    auction_spec = auctions[auctions[\"projectId\"]==project]\n    all_times = pd.Series(list(set(auction_spec.blocknumber)))\n    to_remove_blocktimes = all_times[(np.abs(stats.zscore(all_times)) > 2.5)]\n    if len(to_remove_blocktimes)==0:\n        break\n    to_remove_indicies.extend(auction_spec.index[auction_spec['blocknumber'].isin(to_remove_blocktimes)].tolist())\nauctions.drop(index=to_remove_indicies, inplace=True)\n```\n\n**Adding on Dutch Auction Prices**\n\nFor all projects in the dataset besides project 118, a Dutch auction price format was used. I took the mint price data using a [dune query](https://dune.xyz/queries/113834), and then merged it onto the dataset. I had to use a forward and backward fill for the blocks that had mempool actions but no confirmations during the auction.\n\n```python\nauction_prices = pd.read_csv(r'artblock_auctions_analytics/datasets/dune_auction_prices.csv', index_col=0)\nauctions = pd.merge(auctions,auction_prices, how=\"left\", left_on=[\"projectId\",\"blocknumber\"],right_on=[\"projectId\",\"blocknumber\"])\nauctions.sort_values(by=[\"projectId\",\"blocknumber\"], ascending=True, inplace=True)\nauctions[\"price_eth\"].fillna(method=\"ffill\", inplace=True)\nauctions[\"price_eth\"].fillna(method=\"bfill\", inplace=True)\n```\n\n## Feature Engineering For Each Auction\n\n*If you have no interest in the technical bits, just read the parts in bold and skip the rest.*\n\nIn data science, a feature is a variable that is calculated from the larger dataset to be used as an input in some sort of model or algorithm. All features are calculated in the    `preprocess_auction` function and are calculated *per auction rather than combining all the auctions into a feature set.*\n\n**The first set of features are the totals for transaction states**, and was a simple `pivot_table` function:\n\n* `number_submitted` : total number of transactions submitted\n* `cancel` : count of transactions that ended in canceled\n* `failed`: count of transactions that ended in failed\n* `dropped`: count of transactions that ended in dropped\n* `confirmed`: count of transactions that ended in confirmed\n\nI mentioned earlier some data was not captured for auctions due to various issues, these transactions were dropped from the dataset.\n\n**The next set of features included their gas behavior.**       The key concept here was capturing how far away their transaction gas was from the average confirmed gas per block (shifted by 1 block). **Then we can create features for the average, median, and standard deviation of gas price distance over the whole auction.** There are a bunch of transposes and index resets to get the `blocknumber` columns in the right order, but the important function is `fill_pending_values_gas` which forward fills the gas price between actions captured. This means that if I put in a transaction at `blocknumber` 1000 with a gas of 0.05 ETH and my next action wasn't until `blocknumber` 1005 where I sped up to 0.1 ETH gas, then this function will fill in the blocks between 1000-1005 with 0.05 ETH.\n\n```python\ndef fill_pending_values_gas(x):\n    first = x.first_valid_index()\n    last = x.last_valid_index()\n    x.loc[first:last] = x.loc[first:last].fillna(method=\"ffill\")\n    return x\n```\n\n**The third set of features were calculating the total number and frequency of actions taken in the auction.**    Here we start with a pivot of total actions (speed ups) per block, with some special calculations for getting the first instance of pending for each transaction:\n\n```python\nget_first_pending = df[df[\"status\"]==\"pending\"] #first submitted \nget_first_pending = get_first_pending.drop_duplicates(subset=[\"tx_hash\",\"status\"], keep=\"first\")\nauctions_time_data = pd.concat([get_first_pending,df[df[\"status\"]==\"speedup\"]], axis=0)\ntime_action = auctions_time_data.pivot_table(index=[\"sender\",\"tx_hash\"], columns=\"blocknumber\",values=\"status\",aggfunc=\"count\") \\\n                    .reindex(set(df[\"blocknumber\"]), axis=1, fill_value=np.nan)\n```\n\nFrom here we get to `average_action_delay` in three steps:\n\n1. we take the number of actions per block (yes some people sped up transactions multiple times in the same block)\n2. we drop the blocks with no actions, and then take the difference between the blocknumbers that remain. We add a 0 for each additional action taken per block.\n3. Taking the mean across the differences and added zeroes gives us the `average_action_delay`\n\n```python\ndef get_actions_diff(row):\n    row = row.dropna().reset_index()\n    actions_diff_nominal =list(row[\"blocknumber\"].diff(1).fillna(0))\n \n    #take the blocks with muliple actions and subtract one, then sum up. \n    zeros_to_add = sum([ actions - 1 if actions > 1 else 0 for actions in row[row.columns[1]]])\n    actions_diff_nominal.extend(list(np.zeros(int(zeros_to_add))))\n    actions_diff = np.mean(actions_diff_nominal)\n    if (actions_diff==0) and (zeros_to_add==0):\n        return 2000 #meaning they never took another action\n    else:\n        return actions_diff\n```\n\n`total_actions` is much simpler, as it is just the sum of actions across the pivot.\n\n```python\ntime_action[\"total_actions\"] = time_action.iloc[:,:-1].sum(axis=1)\n```\n\n**The last time-dependent feature is** `block_entry`, which is an important one due to the introduction of Dutch auctions. Essentially this tracks what block the transaction was submitted on since the start.\n\n```python\nget_first_pending[\"block_entry\"] =   get_first_pending[\"blocknumber\"] - get_first_pending[\"blocknumber\"].min()\n\nentry_pivot = get_first_pending.pivot_table(index=\"sender\",values=\"block_entry\",aggfunc=\"min\")\n```\n\n`price_eth` is added as a feature as well, which is tied to the `block_entry` point.\n\n**The last set of features were based on a Dune query, specifically the days since the first transaction, total gas used in transactions, and the total number of transactions.**    To get the address array in the right format I used the following line of code after reading in the SQL data:\n\n```python\nall_users = list(set(auctions[\"sender\"].apply(lambda x: x.replace('0x','\\\\x'))))\nall_users_string = \"('\" + \"'),('\".join(all_users) + \"')\"\n```\n\nThe dune query for this is fairly simple. I pasted the addresses string under `VALUES`, and made some CTEs to get the features I wanted. In the final `SELECT` I tried to add each address's ens as well. You can find the query here: <https://dune.xyz/queries/96523>\n\nLastly, we just merge in the per wallet data on days active, total gas used, and the total number of transactions.\n\n```python\nauctions_all_df = pd.merge(auctions_all_df,wh,on=\"sender\",how=\"left\")\nauctions_all_df.set_index([\"sender\",\"ens\"],inplace=True)\n```\n\n*With all this done, we're finally ready to run some fun unsupervised learning algorithms and try and validate our hypothesis on user groups.*\n\n## Clustering and Visualizing User Groups\n\nBefore I started this project, I had expected to see the following user groups pop out of the data:\n\n* **Set and Forget:** There should be two groups here, those who set a transaction with really high gas and an average/low gas and then don't touch it for the rest of the auction.\n* **Speed Up:** There should be two groups here as well, those who are speeding up often and updating the transaction directly as a factor of gas prices and those who are speeding up often but with basically no change in gas price.\n\nI was very interested in validating these groups, seeing how large each group was, and seeing if any users moved between groups over the course of many auctions. The easiest way to do this was to use unsupervised machine learning to identify clusters of user groups based on the variability across all features. Essentially this is like looking at the distribution of income in a state and then splitting it into sub-distributions for different income concentrations, geographic coordinates, and age. Note that this isn't binning, where the distribution is split into equal ranges - it is instead calculated based on the density of observations within the whole range. The approach we will take is called \"unsupervised\" because our dataset doesn't have any existing labels, rather than something like a regression where there is a value being predicted that can be verified as right or wrong.\n\nThe algorithm I decided to use is called [k-means](https://stanford.edu/\\~cpiech/cs221/handouts/kmeans.html), where k stands for the number of clusters you are expecting to identify. Each cluster has a \"centroid\", which is like the center of a circle. There are various methods for figuring out how many clusters are optimal, and the two I used were elbow points and silhouette scores. Those are both fancy ways of asking,\n\n> \"does each additional cluster help increase the density of the cluster (calculated as the average distance of points in a cluster from the centroid) and maintain adequate separation between clusters (no overlap between two clusters)?\"\n\nI found that 3 clusters were optimal in terms of most inertia improvement while keeping a high silhouette score (around 0.55).\n\n![6 auctions were used for this analysis](https://images.mirror-media.xyz/publication-images/7bef376a-b96c-42de-87b6-991db4503ef9.png?height=264&width=393)\n\nWith clusters chosen, we want to be able to visualize and verify their existence. There are over 15 variables, so we need to reduce the number of dimensions in order to plot it. Dimension reduction typically relies on either PCA or t-SNE algorithms, in our case I went with t-SNE. Don't worry too much about understanding this part, these algorithms essentially capture the variance across all features to give us X and Y components that maximize the spread of points from each other.\n\n**Let's look at project 118, LeWitt Generator Generator, from August 4th:**\n\n![](https://images.mirror-media.xyz/publication-images/4f7dad88-47b6-4dde-9a52-f0e4258cb829.png?height=605&width=615)\n\n![These are the sub-distributions for each variable by cluster, calculated using a KDE. The colors match those in the above clusters.](https://images.mirror-media.xyz/publication-images/5bafdebf-7b9b-43b3-8ed0-7e4ac923725f.png?height=856&width=1123)\n\nAfter taking a look at the sub-distributions for each variable and some data examples, I was able to categorize the clusters. The Orange cluster is the group that is speeding up the most while also submitting slightly lower gas transactions on average. Blue and Green clusters exhibit similar behavior to each other, but addresses in Blue typically have less history than addresses in the Green cluster.\n\nLooking at the overall picture, it seems the original hypothesis on \"speed-up\" and \"set high and set low\" producing two groups each was wrong. Instead, we have a single \"speed-up\" group (Orange) and a single \"set-and-forget\" group (Blue and Green are behaviorally the same). I think the new (Blue) versus old wallets (Green) in the \"set and forget\" group probably carries a lot of overlap in actual users, where users just created new wallets to bid on more mints. Based on their impatience and lower-than-average gas prices, the \"speed-up\" group reads to me as either less experienced or just more greedy than other users.    *What did surprise me is that the speed-up group makes up a smaller proportion of total bidders, as I had expected that group to make up 60-70% of bidders instead of 30%.*\n\n**Now, where this user behavior study gets really interesting is when comparing project 118 (the set price of 0.75 ETH) to project 140 (Dutch auction with the price decreasing from 1.559 to .159 ETH).**\n\n**Here's the clustering for project 140, Good Vibrations from August 21st:**\n\n![](https://images.mirror-media.xyz/publication-images/ae090b2e-141c-4dd0-a0cb-06f4ed45dd13.png?height=605&width=615)\n\n![](https://images.mirror-media.xyz/publication-images/7294dd21-59e5-48fd-b7a1-7babbda77040.png?height=856&width=1130)\n\nWe can see that now most of the clustering variability comes from `block_entry`, `price_eth`, and all of the `gas_behavior` features. This is a big departure from the main variables for project 118. In 118, the set price meant that people entered the auction in a fairly uniform distribution (quantity left didn't seem to matter), and the \"speed-up\" group makes actions up fairly endlessly - probably very anxious.\n\nIn project 140, we don't see the same difference in actions within `average_action_delay` or `total_actions` , instead, we see probably that same \"speed-up\" group entering at a very late stage block and setting far below-average gas prices as seen in `average_gas_behavior`. The Green cluster probably represents users with more experience than the Orange cluster, but who are still transitioning between Orange and Blue in terms of their behavior. If I was to try and map this to the clusters in 118, I believe the \"speed-up\" group is now the \"greedy\" group (Orange) that is entering late and bidding a low gas. The \"set-and-forget\" group maps pretty well to the \"early-grab\" group (Green and Blue), where they all exhibit pretty good patience and an adequate safety net on gas bids.\n\n**I call the Orange group \"greedy\" not just because of their behavior, but also because of their rate of failed transactions.**\n\nFor project 118, fail rates are of the \"speed-up\" versus \"set-and-forget\" groups are within 10-15%.\n\n![percent_lost takes (cancel + dropped + failed) / number_submitted](https://images.mirror-media.xyz/publication-images/306752cb-87c2-4494-8915-86102c214d29.png?height=77&width=724)\n\nFor project 140, the fail rate of the \"greedy\" cluster is around 69% versus the \"early-grab\" group at around 5-15%.\n\n![](https://images.mirror-media.xyz/publication-images/55ceaf8d-6756-4807-beb9-fcfbbe4e549e.png?height=84&width=727)\n\n**Overall, my read of this is that the group's bad habits were amplified - it feels to me like we made a tradeoff in anxiety → greed. This may have made the auction less stressful, but ultimately led to more users being upset (due to failed mints).**\n\n*I'm sure there's a more granular analysis that can be done to segment the auctions further based on factory/curated/playground or by artists themselves too. This will only get more interesting and complex as the community continues to grow, and emotions play a larger factor in both a single auction and on if they return for future auctions.*\n\n*This study of multiple auctions helped us validate our assumptions, understand the proportions of user groups, and see how users' good or bad behaviors shift over time (and other parameters). Now we need to plug it into the rest of the product cycle process.*\n\n## Where We Go From Here:\n\nThe reason that I chose just Artblocks auctions for this and not a mix of platforms is because I wanted to look for a place where the variability in terms of interface and project type is mostly controlled. This should have given us fairly consistent users and behavior types.\n\nThis is just the start of a UX research cycle, so ideally we could continue in the following steps:\n\n1. Use an unsupervised machine learning algorithm to identify the user groups (clusters) and see how many people are making \"mistakes\" when entering an auction.    *This is the step we covered today.*\n2. Create a new user interface, such as a [histogram view on the bidding screen](https://twitter.com/andrewhong5297/status/1423020670825504768), or showing historical data on when most people usually enter/crowd the auction and at what prices. Anything to give both current and historical context to the user, especially those from the speed cluster.\n3. With each auction, run the mempool/wallet data through the created algorithm to see if the user groups have shifted and if specific users have \"learned\" to participate in the auction differently (i.e. did they move between user groups). *I think that the most value can be found in this step if done well. Using ENS or other identifiers to help supplement this grouping would be exponentially helpful too*\n4. Based on the results, continue to iterate on the user interface and design. You could also run more informed A/B testing since you could even figure out what screens to show by making an educated guess based on the users' last cluster (or use label propagation for new users).\n\n**The Dutch auction-style change is also an example of step #2, and we were able to see a clear shift in user behaviors.**       While typically this sort of A/B testing is focused on increasing engagement or conversions, here we are optimizing for the user's ability to learn and improve instead. This may become even more robust if this was iterated in a multiplatform context, so that we can study how someone is learning at an ecosystem level (maybe even supplement with Rabbithole data and user profiles). Since my Artblocks user research is all based on publicly sourced data, it can be replicated and supplemented by any other auction/sale platform. **Crypto could be the first industry that would have a synchronized and transparent set of user groups and UX research, to be applied in products and academia.** Nansen wallet labeling is already a step towards this, but it's different when teams from different products build this up from various facets and approaches.\n\nWhat I would eventually envision is using data to build on the following user personas (with subgroups/levels within them):\n\n* I want to buy a Fidenza, so I can either buy one through a private sale, bid on one in an auction myself, bid on one in a prtyDAO bid auction, or buy a fraction of one with [fractional.art](https://fractional.art/)\n* I like Fidenzas in general, so I'll just buy the NFTX Fidenza index token or an NFT basket of Artblocks Curated on [fractional.art](https://fractional.art/)\n* I'm a collector already, so I want to swap or bid on a Fidenza using a curated set of NFTs and ERC20s I already hold (using genie-xyz swap).\n* I like the rush of acquiring through initial mint versus secondary market, and heavily participate in auctions like Artblocks live mints.\n\nI hope you found this project interesting and/or helpful, I had a ton of fun with it. Thanks to the folks at Blocknative for setting me up, and the community at Artblocks for answering my many auction questions. As always, feel free to reach out with any questions or ideas you may have!\n\nYou can find the GitHub repo [with all the data and script here](https://github.com/andrewhong5297/artblock_auctions_analysis). The script may be a bit hard to read since I’m still refactoring and cleaning it up. The script and some of the analysis here may get updated as I analyze the last few auctions of August for new patterns.", "timestamp": 1629647346, "digest": "l_-4fQ08cpxUZpn9V9S5R27wfKvNgdnrXlZAWZWvdlg", "contributor": "0x2Ae8c972fB2E6c00ddED8986E2dc672ED190DA06"}
{"id": "DGIfOCq3tp5tfMxE_fe0E-q7FScre8-k5fgAWCmjyWQ", "title": "Tutorial: Purchasing an NFT on Opensea", "body": "Today, we’re showing you **how to purchase a non-fungible token (NFT) on [Opensea](https://opensea.io/)** - by writing a post on Mirror and listing it as an NFT itself. \n\n## What you will need for this Quest:\n\n* ETH to purchase the NFT and cover gas fees \n* MetaMask wallet (or another web3 wallet)\n\n## What is OpenSea?\n\n![OpenSea's homepage](https://images.mirror-media.xyz/publication-images/b2a1ee39-d38f-4ec1-84a4-30a5d15d0f0c.png?height=616&width=1432)\n\nOpenSea is a peer-to-peer marketplace for NFTs – showcasing everything from digital collectibles and art to virtual land and utility tokens. \n\nThink of OpenSea as the eBay of NFTs, where you can bid on an NFT auction, mint and sell your own NFT, or explore the wild west of non-fungibles. OpenSea is where most NFT collectors go to discover almost any project.\n\n## How to Purchase an NFT on Opensea \n\n### Step 1: Choose your NFT on OpenSea.\n\nGo to [OpenSea](https://opensea.io/) and choose which NFT you want to buy.\n\nFor the purposes of this tutorial, simply **choose an NFT on OpenSea that you love** and would be happy holding on to for the long term. \n\n***Note****: you may need to pay additional gas fees if it’s your first time purchasing on OpenSea.*\n\n### Step 2: Click “Buy Now” or “Place Bid”\n\nSome NFTs are on auction while others can be bought right away. \n\nFor this tutorial, we will buy an NFT that is ready for immediate purchase. \n\nClick “Buy Now” and agree to OpenSea’s Terms of Service.\n\n![](https://images.mirror-media.xyz/publication-images/ead79986-eada-4887-8430-66a6d6f41ae9.png?height=145&width=586)\n\n### Step 3: Agree to OpenSea’s Terms of Service and Checkout.\n\nOnce you check the box, and click “Checkout\", you will be prompted to pay ETH gas fees for your NFT. Set the amount of gas fees and confirm your NFT purchase.\n\n![](https://images.mirror-media.xyz/publication-images/d1c7722e-9cf6-4475-b3d5-cddfef80d34e.png?height=146&width=502)\n\n### Step 4: Wait for confirmation.\n\nNow you wait! Depending how much gas you used, it could take up to 30 minutes or longer for the payment to confirm.\n\nOnce confirmed, your NFT will be in your wallet and displayed on your OpenSea profile. \n\n### That’s all you need! \n\nCongratulations on buying an NFT on OpenSea. You can now show your NFT off to all of your friends.\n\nPlease let me know if this tutorial was helpful (I could have gone into more detail but wanted to keep this short and sweet) or if you have any questions whatsoever in the RabbitHole Discord. \n\nSee you on the other side of the RabbitHole!", "timestamp": 1629653292, "digest": "6UF5AgtcoSBgHvJR_-ZvAyBEJiiCEINP1SEXsRpe2NM", "contributor": "0xF69EFCc2dECE59Fbc75B23aE75f51DD7Be3bb1E1"}
{"id": "Y78WA1YBZt9s7jM1py82CCAmrSr6s6tF2z5g245MGek", "title": "Tutorial: Purchasing an NFT on Opensea", "body": "Today, we’re showing you **how to purchase a non-fungible token (NFT) on [Opensea](https://opensea.io/)** - by writing a post on Mirror and listing it as an NFT itself. \n\n## What you will need for this Quest:\n\n* ETH to purchase the NFT and cover gas fees \n* MetaMask wallet (or another web3 wallet)\n\n## What is OpenSea?\n\n![OpenSea's homepage](https://images.mirror-media.xyz/publication-images/b2a1ee39-d38f-4ec1-84a4-30a5d15d0f0c.png?height=616&width=1432)\n\nOpenSea is a peer-to-peer marketplace for NFTs – showcasing everything from digital collectibles and art to virtual land and utility tokens. \n\nThink of OpenSea as the eBay of NFTs, where you can bid on an NFT auction, mint and sell your own NFT, or explore the wild west of non-fungibles. OpenSea is where most NFT collectors go to discover almost any project.\n\n## How to Purchase an NFT on Opensea \n\n### Step 1: Choose your NFT on OpenSea.\n\nGo to [OpenSea](https://opensea.io/) and choose which NFT you want to buy.\n\nFor the purposes of this tutorial, simply **choose an NFT on OpenSea that you love** and would be happy holding on to for the long term. \n\n***Note****: you may need to pay additional gas fees if it’s your first time purchasing on OpenSea.*\n\n### Step 2: Click “Buy Now” or “Place Bid”\n\nSome NFTs are on auction while others can be bought right away. \n\nFor this tutorial, we will buy an NFT that is ready for immediate purchase. \n\nClick “Buy Now” and agree to OpenSea’s Terms of Service.\n\n![](https://images.mirror-media.xyz/publication-images/ead79986-eada-4887-8430-66a6d6f41ae9.png?height=145&width=586)\n\n### Step 3: Agree to OpenSea’s Terms of Service and Checkout.\n\nOnce you check the box, and click “Checkout\", you will be prompted to pay ETH gas fees for your NFT. Set the amount of gas fees and confirm your NFT purchase.\n\n![](https://images.mirror-media.xyz/publication-images/d1c7722e-9cf6-4475-b3d5-cddfef80d34e.png?height=146&width=502)\n\n### Step 4: Wait for confirmation.\n\nNow you wait! Depending how much gas you used, it could take up to 30 minutes or longer for the payment to confirm.\n\nOnce confirmed, your NFT will be in your wallet and displayed on your OpenSea profile. \n\n### That’s all you need! \n\nCongratulations on buying an NFT on OpenSea. You can now show your NFT off to all of your friends.\n\nPlease let me know if this tutorial was helpful (I could have gone into more detail but wanted to keep this short and sweet) or if you have any questions whatsoever in the RabbitHole Discord. \n\nSee you on the other side of the RabbitHole!", "timestamp": 1629653320, "digest": "6UF5AgtcoSBgHvJR_-ZvAyBEJiiCEINP1SEXsRpe2NM", "contributor": "0xF69EFCc2dECE59Fbc75B23aE75f51DD7Be3bb1E1"}
{"id": "bFSKI8PrxcQVZJJQCL6kW4gFB-MSfGkNr7vOBvsewbU", "title": "Care Package", "body": "*\"The need in Afghanistan is overwhelming and growing fast as conflict escalates and drives more than 393,000 newly displaced people from their homes. They need food, water, shelter, and protection. Please give now to rush emergency aid to displaced Afghan families and your gift will matched to go twice as far to help.\"*\n\nEach edition of this NFT covers one family’s emergency needs for a month. (scroll down to buy 10 at a time)\n\nFunds are routed directly to \\[care.org\\]\n\nFunds Sent:  \\n   \\n 8/18/21 — 33.824 ETH / $103,992 [(view tx)](https://etherscan.io/tx/0x633a96794e090e2977e9c95063d16051bcdbbbadea02e703fbb62f263971b322)\n8/19/21 — 5.04 ETH / $16,238 [(view tx)](https://etherscan.io/tx/0x292796e375a25bf44a8ed40a7876e61d4d4a1d5e1d2689377685fcaec783378d) \\n 8/21/21 — 5.684 ETH / $18,079 [(view tx)](https://etherscan.io/tx/0x43822f7d15b7e78ac7ae5fbff7442541fcf03551fc3324460517777a6045e008)\n\nFunds sent directly to [care.org](https://my.care.org/site/Donation2?df_id=30197&mfc_pref=T&30197.donation=form1&_ga=2.18597753.1765542984.1629153044-549514178.1629153044).\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=184](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=184)\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186)\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186)\n\nDonate fiat [here](https://my.care.org/site/Donation2?df_id=30197&mfc_pref=T&30197.donation=form1&_ga=2.18597753.1765542984.1629153044-549514178.1629153044).", "timestamp": 1629653618, "digest": "sb9NU7Wzpg-53wqNFPM-MCwWQ7y48RJlri3cr3Nt0Do", "contributor": "0xc8f8e2F59Dd95fF67c3d39109ecA2e2A017D4c8a"}
{"id": "UmSkLw3BQiIPwgLaYqz-Tx86a2fjDrKCOCHz8wYtwFQ", "title": "Care Package", "body": "*\"The need in Afghanistan is overwhelming and growing fast as conflict escalates and drives more than 393,000 newly displaced people from their homes. They need food, water, shelter, and protection. Please give now to rush emergency aid to displaced Afghan families and your gift will matched to go twice as far to help.\"*\n\nEach edition of this NFT covers one family’s emergency needs for a month. (scroll down to buy 10 at a time)\n\nFunds are routed directly to \\[care.org\\]\n\nFunds Sent:  \\n 8/18/21 — 33.824 ETH / $103,992 [(view tx)](https://etherscan.io/tx/0x633a96794e090e2977e9c95063d16051bcdbbbadea02e703fbb62f263971b322)\n8/19/21 — 5.04 ETH / $16,238 [(view tx)](https://etherscan.io/tx/0x292796e375a25bf44a8ed40a7876e61d4d4a1d5e1d2689377685fcaec783378d) \\n 8/21/21 — 5.684 ETH / $18,079 [(view tx)](https://etherscan.io/tx/0x43822f7d15b7e78ac7ae5fbff7442541fcf03551fc3324460517777a6045e008)\n\nFunds sent directly to [care.org](https://my.care.org/site/Donation2?df_id=30197&mfc_pref=T&30197.donation=form1&_ga=2.18597753.1765542984.1629153044-549514178.1629153044).\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=184](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=184)\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186)\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186)\n\nDonate fiat [here](https://my.care.org/site/Donation2?df_id=30197&mfc_pref=T&30197.donation=form1&_ga=2.18597753.1765542984.1629153044-549514178.1629153044).", "timestamp": 1629653670, "digest": "sb9NU7Wzpg-53wqNFPM-MCwWQ7y48RJlri3cr3Nt0Do", "contributor": "0xc8f8e2F59Dd95fF67c3d39109ecA2e2A017D4c8a"}
{"id": "wp8Nknslutbf2kXT2oRahkwZHtXrnEQMnZkGN8DWK_Y", "title": "Care Package", "body": "*\"The need in Afghanistan is overwhelming and growing fast as conflict escalates and drives more than 393,000 newly displaced people from their homes. They need food, water, shelter, and protection. Please give now to rush emergency aid to displaced Afghan families and your gift will matched to go twice as far to help.\"*\n\nEach edition of this NFT covers one family’s emergency needs for a month. (scroll down to buy 10 at a time)\n\nFunds are routed directly to \\[care.org\\]\n\nFunds Sent: \n\n8/18/21 — 33.824 ETH / $103,992 [(view tx)](https://etherscan.io/tx/0x633a96794e090e2977e9c95063d16051bcdbbbadea02e703fbb62f263971b322)\n\n8/19/21 — 5.04 ETH / $16,238 [(view tx)](https://etherscan.io/tx/0x292796e375a25bf44a8ed40a7876e61d4d4a1d5e1d2689377685fcaec783378d)\n\n8/21/21 — 5.684 ETH / $18,079 [(view tx)](https://etherscan.io/tx/0x43822f7d15b7e78ac7ae5fbff7442541fcf03551fc3324460517777a6045e008)\n\nFunds sent directly to [care.org](https://my.care.org/site/Donation2?df_id=30197&mfc_pref=T&30197.donation=form1&_ga=2.18597753.1765542984.1629153044-549514178.1629153044).\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=184](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=184)\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186)\n\n[edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186](edition://0xaf89C5E115Ab3437fC965224D317d09faa66ee3E?editionId=186)\n\nDonate fiat [here](https://my.care.org/site/Donation2?df_id=30197&mfc_pref=T&30197.donation=form1&_ga=2.18597753.1765542984.1629153044-549514178.1629153044).", "timestamp": 1629654571, "digest": "sb9NU7Wzpg-53wqNFPM-MCwWQ7y48RJlri3cr3Nt0Do", "contributor": "0xc8f8e2F59Dd95fF67c3d39109ecA2e2A017D4c8a"}
{"id": "pmNDhhrOa_Tzb6RWN3mQpBalXnpa7sJE5Vqb2KI3KLk", "title": "Round 2 Summary: We got a Punk!!!", "body": "**Round 2 is complete and PaperclipDAO has obtained a Cryptopunk and some Meebit to boot! We acquired [CryptoPunk #3744](https://opensea.io/assets/0xb7f7f6c52f2e2fdb1963eab30438024864c313f6/3744) (long known as “the Paperclip”) and a portion of a [Dissected Meebit](https://opensea.io/assets/0xc36442b4a4522e871399cd717abdd847ab11fe88/113567) from Calvin Liu who gets a [1/5 SE Earthen Concept Art Card](https://opensea.io/assets/0x76be3b62873462d2142405439777e971754e8e77/42) and 400 $CLIP in return. [Calvin](https://twitter.com/cjliu49), welcome to PaperclipDAO! 🖇**\n\n![\"The Paperclip\" Punk #3744 and a Dissected Meebit #10761 ](https://images.mirror-media.xyz/publication-images/aa1edbee-857e-4146-a800-8e8c4d69c9dd.png?height=1146&width=1899)\n\nWe weighed several factors in making a final decision on which trade to accept. We care about more than just the economic value of a trade; we value the culture, story, and journey of the NFT as well. In Round 1, we picked the future. But the past always has a way of coming back around… And the 2 (ok 1.02) historic LarvaLabs NFTs we acquired bridge the past to the future beautifully.\n\nThis decision wasn’t easy; we received incredible offers and, as in Round 1, were sad to part with all the memorable NFTs that we couldn’t accept. Check out the full [Round 2 offer thread](https://discord.gg/ePU79bhBjs) to see all the amazing offers.\n\nWe only have time to highlight a few of our favorites. In no particular order, here is our homage to the ones that got away in Round 2:\n\n* Several offers paid homage to @paperclipDAO. To immortalize the Parallel Trade from @bitcoinPalmer, urukai ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/876169347423428659)) created 2 Proof of Beauty $HASH pieces: One that represents the tx from Palmer to Planck ($HASH sent to Palmer in tx - 0x9d685696e1401a38eee957665f57a720212b409b373c8fe275608309c44da9cb) and a second that represents the tx from Planck to the PaperclipDAO address. What an awesome gesture! We were also offered a [$CLIP Kitty](https://opensea.io/assets/0x06012c8cf97bead5deae237070f9587f8e7a266d/906847) by AnimalTPFR ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/875903985746346004)).\n* We received several offers for [Blitmaps](https://www.blitmap.com), a really cool community started by [Dom Hofmann](https://twitter.com/dhof). Blitmaps and their metadata are stored and processed entirely on-chain. There are 100 originals, created by 17 artists, which the community mixed to make 1,600 siblings — unique pieces that combine the composition of one original with the palette of another. We were offered a sibling: [Pale Tower Warm Vibe ](https://opensea.io/assets/0x8d04a8c79ceb0889bdd12acdf3fa9d207ed3ff63/966)(from [avilés](https://discord.com/channels/874453658425757729/874453942279495680/877022849204879450)) and an original (!!!) [#35 - Candy](https://opensea.io/assets/0x8d04a8c79ceb0889bdd12acdf3fa9d207ed3ff63/35) from itzler ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/877663806602510387)). Naturally, this was a finalist.\n\n![Candy Blitmap #35](https://images.mirror-media.xyz/publication-images/0f8355ea-2f01-45f6-9e91-11aa660fa7ec.png?height=320&width=320)\n\n* Granolah offered [Ghxst 006](https://opensea.io/assets/0x495f947276749ce646f68ac8c248420045cb7b5e/80494307024529346018053650490912529916739680814770830097664395480945836163073) ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/877027607273697360)), one of the original 6! [Ghxsts](https://twitter.com/Ghxsts/status/1388764180950355971?s=20) is an NFT project designed and created by NFT artist, [GxngYxng](https://twitter.com/GxngYxngNFT). Each item in the collection is hand-drawn and released one at a time in random order. There are currently 101 Ghxsts, 42 Immxrtals, 50 Demxns, 12 Zxdiacs, 25 Alixns, and 9 Sirxns. Granolah later upped his offer to include 3 cool cats as well.\n* [OhhShiny](https://twitter.com/AxieKing) offered us [Bored Ape 636](https://opensea.io/assets/0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d/636), a zombie and dmt (and also our first BAYC offer!) The [Bored Ape Yacht Club](https://twitter.com/BoredApeYC) NFTs double as a PFP and a Yacht Club membership (aka a swamp club for Apes). We received a few other offers for PFPs: [Sisyphus](https://twitter.com/0xSisyphus) offered a crowned [Pudgy Penguin](https://opensea.io/assets/0xbd3531da5cf5857e7cfaa92426877b022e612cf8/2251) and state ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/877032545064222740)) offered a bundle of a [classy pengu](https://opensea.io/assets/0xbd3531da5cf5857e7cfaa92426877b022e612cf8/2120) & [classy meeb](https://opensea.io/assets/0x7bd29408f11d2bfc23c34f18275bbf23bb716bc7/4875).\n* We already mentioned our beloved CryptoPunk #3744 but we also received an incredible offer from [Andy8052](https://twitter.com/andy8052) for [80,000 $DEAD tokens](https://fractional.art/vaults/0x0C7060BF06a78AAAAB3fac76941318A52a3F4613) (currently valued at $200,000) which represent 6.66% of [Zombie Punk #2066](https://www.larvalabs.com/cryptopunks/details/2066), ([link to discord](https://discord.com/channels/874453658425757729/874453942279495680/877652589326508052)).\n* Mr. bulldops offered the [0xmons Dza](https://opensea.io/assets/0x0427743df720801825a5c82e0582b1e915e0f750/170) ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/877697951575732224)), a Lovecraftian “pixel monster” generated by a neural net. Though we find it hard to believe that the artist behind this amazing project is the same 0xmons who created our beloved sudoswap, we have been assured this is actually the case.\n* We were also offered a [FuckinTroll](https://opensea.io/assets/0x240eb6b465f61dfc965053791f963cd0f0e4fdb0/1933), a [Shiny Nyan Cat](https://opensea.io/assets/0xb32979486938aa9694bfc898f35dbed459f44424/7) ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/877687465664733214)), several [Treeverse plots](https://twitter.com/TheTreeverse), and a few [RTFKT](https://twitter.com/RTFKTstudios) NFTs. FunkySapienX offered a completed Budweiser [Tom Sachs Rocket](https://opensea.io/assets/0x6b00de202e3cd03c523ca05d8b47231dbdd9142b/45) ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/877698997807108098)) as part of his proposed trade.\n* We were offered several other [ParallelNFT](https://twitter.com/ParallelNFT) cards, which were tempting as always.\n* A very fun offer was [a pair of very clever ENS domains](https://discordapp.com/channels/874453658425757729/874453942279495680/877698413716734003), twenti.eth and thirti.eth.\n* We were offered a [Gods Unchained Genesis Card](https://opensea.io/assets/0x0e3a2a1f2146d86a604adc220b4967a898d7fe07/97630568), the [1st NFT Digital album KINGS OF LEON x YELLOWHEART NFTS](https://opensea.io/assets/0x6d8cc2d52de2ac0b1c6d8bc964ba66c91bb756e1/3), a [membership-token](https://opensea.io/assets/0xef3c951e22c65f6256746f4e227e19a5bcbf393c/119) to the Elektra DAO, a collaboration of musicians using the blockchain to create a community, and Chapter 3 and Chapter 4 of the [Elektra Story](https://discord.com/channels/874453658425757729/874453942279495680/875843371359428649): a story in the form of NFTs.\n* Cliff Vandercave made an offer for a POAP token of a Rare Pepe created by Matt Furie himself, limited to about 500 participants who attended the event in the Metaverse. Cliff proposed that this swap would represent a unique intersection of PaperclipDAO, Parallel, Matt Furie, POAP, Metaverse DAO, and Cryptovoxels, forming a beautiful bond of NFT history ([discord link](https://discord.com/channels/874453658425757729/874453942279495680/877624309340864573)).\n\nEach of these offers adds to the storied history of the PaperclipDAO and we couldn’t be more excited to add our second trade to the history books. In Round 2, we looked to the past, what does the future hold for Round 3? 🖇🖇", "timestamp": 1629660738, "digest": "ojMnU86zC3q5QU3IMlDrNmoRoTesitlcSv9rEDSiWOE", "contributor": "0xdD2DB8E99E49a6aAae2BBa9A50c2FDbaadB60FE5"}
{"id": "0cJwlqxjndiJF4Yk02cQyZdefi-_zavSijTvsXvox18", "title": "Algorithmic Insurance", "body": "The idea of Decentralized Insurance has been one of the classic examples for ideal use-cases of blockchain applications. Insurance providers may have incentives to withhold claims for as long as possible, and users have little transparency into the process — leading to an overall poor user experience. The idea of putting these insurance processes on-chain is an appealing one: insurance claims can be programatically & immediately paid out if some conditions are met, and everyone has full transparency on the state of the entire system at all times. For example, someone could buy travel insurance for their flight, and a smart contract could automatically payout if Google Flights reports that the flight was delayed.\n\nMany attempts at decentralized insurance have been tried over the years, but anything that touches the real world has not gotten much traction at all. Firstly, there are many concerns around getting *trustworthy* data on-chain — if a contract can be maliciously fed data that says a hurricane happened when it didn't, that on-chain weather insurance market is as good as useless. Secondly, the current set of Ethereum/DeFi users are not generally looking for vehicle/household/travel insurance on-chain — they are looking for something more *crypto-native*.\n\n# Smart Contract Insurance\n\nThe immense growth of DeFi in the last two years has single-handedly revived market demand for decentralized insurance products. Many individuals or funds have significant amounts of capital in these DeFi protocols, and *actually* want protection against smart contract exploits. Hence, a new wave of decentralized insurance protocols have sprung up in the last year, mostly focused on smart contract insurance.\n\nMost of these projects tend to innovate along two main axes — Payout Mechanism and Pricing:\n\nProjects like [Nexus Mutual](https://nexusmutual.io/) rely on a set of humans to determine if a smart contract exploit actually happened or not, and hence if payouts should happen. Others like [Cozy Finance](https://www.cozy.finance/), [Ante Finance](https://www.ante.finance/), and [Risk Harbor](https://www.riskharbor.com/) rely purely on specific on-chain conditions to be met to determine if a payout should happen.\n\nOn the pricing axis, some projects use Bonding Curves based on utilization rates to price insurance — e.g some formula that prices the cover based on the demand & supply for that insurance market. Others rely on a team of human experts to determine the \"fair\" value of coverage for a protocol based on information such as track record of the protocol not getting hacked, audits, and so on.\n\nThis post will focus on the Payout Mechanism of decentralized insurance protocols.\n\n# Algorithmic Payouts\n\nIn traditional finance, there would need to be all sorts of trusted intermediaries to verify if some event happened or not before an insurance company can pay out a claim. However, because smart contract exploits are purely on-chain events, we should be able to write programs on-chain that determine if the other on-chain events happened or not — zero trust assumptions required.\n\n[https://twitter.com/0xSisyphus/status/1415294150333841408](https://twitter.com/0xSisyphus/status/1415294150333841408)\n\nAs builders and investors in this space, we should always be on the lookout for things that blockchains uniquely enable which cannot be done in traditional finance — algorithmic insurance could be one of them.\n\n# How do they work?\n\nThe easiest way to think about how these systems work is that they are a prediction market on whether or not a certain condition within a smart contract is met. For example, if you lend USDC on Compound, your USDC balance should monotonically increase over time as interest is accrued — if you are suddenly unable to claim less USDC than you originally deposited, one could infer that a smart contract exploit has occurred. Since we can check this on-chain reliably, we can then create a prediction market as to whether or not `getPricePerFullShare() > 1` returns true or false.\n\nBecause Compound has never been exploited, we can imagine that most people would prefer to take the \"true\" side of the market — skewing the odds of the prediction market significantly. This creates attractive odds for the \"false\" side of the market — if by some chance Compound does get exploited they will get a huge payout, otherwise they will lose a small amount of capital. You can see how this starts to look like an insurance market, where the \"false\" bettors are paying a premium to the \"true\" bettors for a chance of a big payout if the prediction market resolves in their favor.\n\nFor more complicated DeFi protocols, we may want to define more complex payout conditions  than simply checking if a deposit token can claim sufficient underlying asset. For example, we could combine multiple conditions together, such as `totalAssets() > x && getPricePerFullShare() > 1`  or even define a condition that checks the contract state over some time period, such as `totalAssets` must not decrease by more than 50% within a 10-minute window or the condition will return true. The beauty of programmable finance is that we can create arbitrarily complex payout conditions because we know for sure that the on-chain checks will execute deterministically. Furthermore, since all these payout conditions are defined on-chain, we can *always* know when a market will pay-out, no matter how complex the conditions get — it is impossible to obfuscate the risk through legal \"terms and conditions\".\n\nOne way to bootstrap these markets is for the core team themselves to create these insurance markets and take the short side — both as a way to bootstrap liquidity for buyers of cover as well as indicating confidence in their own code. [Ante](https://www.ante.finance/) is an example of a project that focuses on this and tries to get adoption through developers writing insurance markets for their own projects. We could imagine a world where projects who do not write insurance for their own protocol are as risky as projects that have unaudited smart contracts. \n\n# Potential Pitfalls\n\nWhen users buy smart contract insurance, they simply want to get covered if a protocol gets exploited. However, it can be potentially difficult to define through code *what* an exploit actually is. For example, a classic \"rug pull\" where the creator of a liquidity pool removes all the liquidity could lead to a user holding a bag of tokens that are worth $0. Is this an exploit, or is this expected behavior for providing liquidity in an AMM? \n\nOne \"solution\" to this problem is by simply letting the free market determine which markets are the canonical insurance markets for various protocols. For example, we may be able to create two different insurance markets for Compound: the first pays out if cTokens cannot claim >1 underlying, the second pays out if cTokens cannot claim >0.5 underlying. In my opinion, it is likely that the market will naturally converge around one of the two as the \"canonical\" insurance market for Compound, maybe through Schelling points like the Compound team advocating for one or the other — letting most of the liquidity stay within one market. It could be much less \"obvious\" which market should be the canonical market for a protocol if the complexity of it is high, but this free market approach assumes that the market will sort itself out.\n\nIn this model, it matters *a lot* which specific insurance market you are a buyer or seller of. It is possible that a protocol was obviously (to the human eye) maliciously exploited, but the payout condition that your market resolves on does not get triggered. Conversely, it is possible that insurance sellers may be \"unfairly\" forced to pay out a claim if an edge case in the protocol causes the payout condition to trigger even in the absence of a malicious exploit.\n\nHence, accurate communication about what the payout conditions entail are extremely important so that insurance buyers and sellers know exactly what they are signing up for. This is mostly a UX challenge, and translating the payout conditions from code into words is a non-trivial task. Teams may be tempted to obfuscate the risks of insurance selling as \"risk-free yield\" to attract TVL, or present the products to buyers as all-encompassing smart contract insurance to boost their volumes.\n\n# Beyond Insurance\n\nAlthough the market for smart contract insurance is huge, this type of protocol can also be used to create markets for exotic swaps aimed towards speculators. Changing the payout condition from things that \"should never happen\" to \"should *rarely* happen\" makes it much more interesting for speculators who are looking for asymmetric opportunities.\n\nComposability also allows developers to combine these markets in arbitrary ways, for example rehypothecating the collateral used to sell insurance in one market as collateral in a different market, letting one pool of capital sell insurance on multiple (hopefully uncorrelated) things at the same time.\n\nThese tokens from both sides of the insurance market should also be composable with other DeFi protocols. Some ideas include: \n\n* *Protected-cToken,* which is a token that packages the insurance token + cToken into one\n* *Perpetual insurance buying,* where a contract can constantly extract yield from a yield-bearing asset to pay for insurance \n* *Levered insurance selling,* where users can borrow against their insurance-selling positions to sell more insurance and generate more yield \n\nAlgorithmic Insurance opens up the design space significantly for interesting financial products on-chain because it is purely self-contained and does not rely on human decision making to payout. This objectivity helps developers reason about exactly what exposure the insurance market provides, and can build other apps on top of these primitives without worrying about human decision making or trust — which is what DeFi should be all about. ", "timestamp": 1629673143, "digest": "72Oo_gd-e_XxyX7q8R88wQeoTXh69YZ9nXJYIwmMEDU", "contributor": "0xe16c0E1Bf3B75F67E83C9e47B9c0Eb8Bf1B99CCd"}
{"id": "_0YPWTkaCOnGmvC1-w4evS07kDSzcB-lJjp5CrmRuks", "title": "Algorithmic Insurance", "body": "The idea of Decentralized Insurance has been one of the classic examples for ideal use-cases of blockchain applications. Insurance providers may have incentives to withhold claims for as long as possible, and users have little transparency into the process — leading to an overall poor user experience. The idea of putting these insurance processes on-chain is an appealing one: insurance claims can be programatically & immediately paid out if some conditions are met, and everyone has full transparency on the state of the entire system at all times. For example, someone could buy travel insurance for their flight, and a smart contract could automatically payout if Google Flights reports that the flight was delayed.\n\nMany attempts at decentralized insurance have been tried over the years, but anything that touches the real world has not gotten much traction at all. Firstly, there are many concerns around getting *trustworthy* data on-chain — if a contract can be maliciously fed data that says a hurricane happened when it didn't, that on-chain weather insurance market is as good as useless. Secondly, the current set of Ethereum/DeFi users are not generally looking for vehicle/household/travel insurance on-chain — they are looking for something more *crypto-native*.\n\n# Smart Contract Insurance\n\nThe immense growth of DeFi in the last two years has single-handedly revived market demand for decentralized insurance products. Many individuals or funds have significant amounts of capital in these DeFi protocols, and *actually* want protection against smart contract exploits. Hence, a new wave of decentralized insurance protocols have sprung up in the last year, mostly focused on smart contract insurance.\n\nMost of these projects tend to innovate along two main axes — Payout Mechanism and Pricing:\n\nProjects like [Nexus Mutual](https://nexusmutual.io/) rely on a set of humans to determine if a smart contract exploit actually happened or not, and hence if payouts should happen. Others like [Cozy Finance](https://www.cozy.finance/), [Ante Finance](https://www.ante.finance/), and [Risk Harbor](https://www.riskharbor.com/) rely purely on specific on-chain conditions to be met to determine if a payout should happen.\n\nOn the pricing axis, some projects use Bonding Curves based on utilization rates to price insurance — e.g some formula that prices the cover based on the demand & supply for that insurance market. Others rely on a team of human experts to determine the \"fair\" value of coverage for a protocol based on information such as track record of the protocol not getting hacked, audits, and so on.\n\nThis post will focus on the Payout Mechanism of decentralized insurance protocols.\n\n# Algorithmic Payouts\n\nIn traditional finance, there would need to be all sorts of trusted intermediaries to verify if some event happened or not before an insurance company can pay out a claim. However, because smart contract exploits are purely on-chain events, we should be able to write programs on-chain that determine if the other on-chain events happened or not — zero trust assumptions required.\n\n[https://twitter.com/0xSisyphus/status/1415294150333841408](https://twitter.com/0xSisyphus/status/1415294150333841408)\n\nAs builders and investors in this space, we should always be on the lookout for things that blockchains uniquely enable which cannot be done in traditional finance — algorithmic insurance could be one of them.\n\n# How do they work?\n\nThe easiest way to think about how these systems work is that they are a prediction market on whether or not a certain condition within a smart contract is met. For example, if you lend USDC on Compound, your USDC balance should monotonically increase over time as interest is accrued — if you are suddenly unable to claim less USDC than you originally deposited, one could infer that a smart contract exploit has occurred. Since we can check this on-chain reliably, we can then create a prediction market as to whether or not `getPricePerFullShare() > 1` returns true or false.\n\nBecause Compound has never been exploited, we can imagine that most people would prefer to take the \"true\" side of the market — skewing the odds of the prediction market significantly. This creates attractive odds for the \"false\" side of the market — if by some chance Compound does get exploited they will get a huge payout, otherwise they will lose a small amount of capital. You can see how this starts to look like an insurance market, where the \"false\" bettors are paying a premium to the \"true\" bettors for a chance of a big payout if the prediction market resolves in their favor.\n\nFor more complicated DeFi protocols, we may want to define more complex payout conditions  than simply checking if a deposit token can claim sufficient underlying asset. For example, we could combine multiple conditions together, such as `totalAssets() > x && getPricePerFullShare() > 1`  or even define a condition that checks the contract state over some time period, such as `totalAssets` must not decrease by more than 50% within a 10-minute window or the condition will return true. The beauty of programmable finance is that we can create arbitrarily complex payout conditions because we know for sure that the on-chain checks will execute deterministically. Furthermore, since all these payout conditions are defined on-chain, we can *always* know when a market will pay-out, no matter how complex the conditions get — it is impossible to obfuscate the risk through legal \"terms and conditions\".\n\nOne way to bootstrap these markets is for the core team themselves to create these insurance markets and take the short side — both as a way to bootstrap liquidity for buyers of cover as well as indicating confidence in their own code. [Ante](https://www.ante.finance/) is an example of a project that focuses on this and tries to get adoption through developers writing insurance markets for their own projects. We could imagine a world where projects who do not write insurance for their own protocol are as risky as projects that have unaudited smart contracts.\n\n# Potential Pitfalls\n\nWhen users buy smart contract insurance, they simply want to get covered if a protocol gets exploited. However, it can be potentially difficult to define through code *what* an exploit actually is. For example, a classic \"rug pull\" where the creator of a liquidity pool removes all the liquidity could lead to a user holding a bag of tokens that are worth $0. Is this an exploit, or is this expected behavior for providing liquidity in an AMM?\n\nOne \"solution\" to this problem is by simply letting the free market determine which markets are the canonical insurance markets for various protocols. For example, we may be able to create two different insurance markets for Compound: the first pays out if cTokens cannot claim >1 underlying, the second pays out if cTokens cannot claim >0.5 underlying. In my opinion, it is likely that the market will naturally converge around one of the two as the \"canonical\" insurance market for Compound, maybe through Schelling points like the Compound team advocating for one or the other — letting most of the liquidity stay within one market. It could be much less \"obvious\" which market should be the canonical market for a protocol if the complexity of it is high, but this free market approach assumes that the market will sort itself out.\n\nIn this model, it matters *a lot* which specific insurance market you are a buyer or seller of. It is possible that a protocol was obviously (to the human eye) maliciously exploited, but the payout condition that your market resolves on does not get triggered. Conversely, it is possible that insurance sellers may be \"unfairly\" forced to pay out a claim if an edge case in the protocol causes the payout condition to trigger even in the absence of a malicious exploit.\n\nHence, accurate communication about what the payout conditions entail are extremely important so that insurance buyers and sellers know exactly what they are signing up for. This is mostly a UX challenge, and translating the payout conditions from code into words is a non-trivial task. Teams may be tempted to obfuscate the risks of insurance selling as \"risk-free yield\" to attract TVL, or present the products to buyers as all-encompassing smart contract insurance to boost their volumes.\n\n# Beyond Insurance\n\nAlthough the market for smart contract insurance is huge, this type of protocol can also be used to create markets for exotic swaps aimed towards speculators. Changing the payout condition from things that \"should never happen\" to \"should *rarely* happen\" makes it much more interesting for speculators who are looking for asymmetric opportunities.\n\nComposability also allows developers to combine these markets in arbitrary ways, for example rehypothecating the collateral used to sell insurance in one market as collateral in a different market, letting one pool of capital sell insurance on multiple (hopefully uncorrelated) things at the same time.\n\nThese tokens from both sides of the insurance market should also be composable with other DeFi protocols. Some ideas include:\n\n* *Protected-cToken,* which is a token that packages the insurance token + cToken into one\n* *Perpetual insurance buying,* where a contract can constantly extract yield from a yield-bearing asset to pay for insurance\n* *Levered insurance selling,* where users can borrow against their insurance-selling positions to sell more insurance and generate more yield\n\nAlgorithmic Insurance opens up the design space significantly for interesting financial products on-chain because it is purely self-contained and does not rely on human decision making to payout. This objectivity helps developers reason about exactly what exposure the insurance market provides, and can build other apps on top of these primitives without worrying about human decision making or trust — which is what DeFi should be all about.\n\n*Thanks to [Miyuki](https://twitter.com/miyuki_crypto) and [Emily](https://twitter.com/AutomataEmily) for reading this beforehand and giving their feedback* ", "timestamp": 1629673396, "digest": "72Oo_gd-e_XxyX7q8R88wQeoTXh69YZ9nXJYIwmMEDU", "contributor": "0xe16c0E1Bf3B75F67E83C9e47B9c0Eb8Bf1B99CCd"}
{"id": "kVP6DOWbqu-c45TYGr8H4N7HzUfyayNQwYFThKDvamo", "title": "Tutorial: How to Purchase an NFT on OpenSea", "body": "Today, we’re showing you **how to purchase a non-fungible token (NFT) on [Opensea](https://opensea.io/)** - by writing a post on Mirror and listing it as an NFT itself. \n\n## What you will need for this Quest:\n\n* ETH to purchase the NFT and cover gas fees \n* MetaMask wallet (or another web3 wallet)\n\n## What is OpenSea?\n\n![OpenSea's homepage](https://images.mirror-media.xyz/publication-images/b2a1ee39-d38f-4ec1-84a4-30a5d15d0f0c.png?height=616&width=1432)\n\nOpenSea is a peer-to-peer marketplace for NFTs – showcasing everything from digital collectibles and art to virtual land and utility tokens. \n\nThink of OpenSea as the eBay of NFTs, where you can bid on an NFT auction, mint and sell your own NFT, or explore the wild west of non-fungibles. OpenSea is where most NFT collectors go to discover almost any project.\n\n## How to Purchase an NFT on Opensea \n\n### Step 1: Choose your NFT on OpenSea.\n\nGo to [OpenSea](https://opensea.io/) and choose which NFT you want to buy.\n\nFor the purposes of this tutorial, simply **choose an NFT on OpenSea that you love** and would be happy holding on to for the long term. \n\n***Note****: you may need to pay additional gas fees if it’s your first time purchasing on OpenSea.*\n\n### Step 2: Click “Buy Now” or “Place Bid”\n\nSome NFTs are on auction while others can be bought right away. \n\nFor this tutorial, we will buy an NFT that is ready for immediate purchase. \n\nClick “Buy Now” and agree to OpenSea’s Terms of Service.\n\n![](https://images.mirror-media.xyz/publication-images/ead79986-eada-4887-8430-66a6d6f41ae9.png?height=145&width=586)\n\n### Step 3: Agree to OpenSea’s Terms of Service and Checkout.\n\nOnce you check the box, and click “Checkout\", you will be prompted to pay ETH gas fees for your NFT. Set the amount of gas fees and confirm your NFT purchase.\n\n![](https://images.mirror-media.xyz/publication-images/d1c7722e-9cf6-4475-b3d5-cddfef80d34e.png?height=146&width=502)\n\n### Step 4: Wait for confirmation.\n\nNow you wait! Depending how much gas you used, it could take up to 30 minutes or longer for the payment to confirm.\n\nOnce confirmed, your NFT will be in your wallet and displayed on your OpenSea profile. \n\n### That’s all you need! \n\nCongratulations on buying an NFT on OpenSea. You can now show your NFT off to all of your friends.\n\nPlease let me know if this tutorial was helpful (I could have gone into more detail but wanted to keep this short and sweet) or if you have any questions whatsoever in the RabbitHole Discord. \n\nSee you on the other side of the RabbitHole!", "timestamp": 1629679918, "digest": "6UF5AgtcoSBgHvJR_-ZvAyBEJiiCEINP1SEXsRpe2NM", "contributor": "0xF69EFCc2dECE59Fbc75B23aE75f51DD7Be3bb1E1"}
{"id": "VIQmKRZwu4Rx5Owy18U16hJwjSp9rmcbM8iRpgCEYvI", "title": "Tutorial: How to Purchase an NFT on OpenSea", "body": "Today, we’re showing you **how to purchase a non-fungible token (NFT) on [Opensea](https://opensea.io/)** - by writing a post on Mirror and listing it as an NFT itself. \n\n## What you will need for this Quest:\n\n* ETH to purchase the NFT and cover gas fees \n* MetaMask wallet (or another web3 wallet)\n\n## What is OpenSea?\n\n![OpenSea's homepage](https://images.mirror-media.xyz/publication-images/b2a1ee39-d38f-4ec1-84a4-30a5d15d0f0c.png?height=616&width=1432)\n\nOpenSea is a peer-to-peer marketplace for NFTs – showcasing everything from digital collectibles and art to virtual land and utility tokens. \n\nThink of OpenSea as the eBay of NFTs, where you can bid on an NFT auction, mint and sell your own NFT, or explore the wild west of non-fungibles. OpenSea is where most NFT collectors go to discover almost any project.\n\n## How to Purchase an NFT on Opensea \n\n### Step 1: Choose your NFT on OpenSea.\n\nGo to [OpenSea](https://opensea.io/) and choose which NFT you want to buy.\n\nFor the purposes of this tutorial, simply **choose an NFT on OpenSea that you love** and would be happy holding on to for the long term. \n\n***Note****: you may need to pay additional gas fees if it’s your first time purchasing on OpenSea.*\n\n### Step 2: Click “Buy Now” or “Place Bid”\n\nSome NFTs are on auction while others can be bought right away. \n\nFor this tutorial, we will buy an NFT that is ready for immediate purchase. \n\nClick “Buy Now” and agree to OpenSea’s Terms of Service.\n\n![](https://images.mirror-media.xyz/publication-images/ead79986-eada-4887-8430-66a6d6f41ae9.png?height=145&width=586)\n\n### Step 3: Agree to OpenSea’s Terms of Service and Checkout.\n\nOnce you check the box, and click “Checkout\", you will be prompted to pay ETH gas fees for your NFT. Set the amount of gas fees and confirm your NFT purchase.\n\n![](https://images.mirror-media.xyz/publication-images/d1c7722e-9cf6-4475-b3d5-cddfef80d34e.png?height=146&width=502)\n\n### Step 4: Wait for confirmation.\n\nNow you wait! Depending how much gas you used, it could take up to 30 minutes or longer for the payment to confirm.\n\nOnce confirmed, your NFT will be in your wallet and displayed on your OpenSea profile. \n\n### That’s all you need! \n\nCongratulations on buying an NFT on OpenSea. You can now show your NFT off to all of your friends.\n\nPlease let me know if this tutorial was helpful (I could have gone into more detail but wanted to keep this short and sweet) or if you have any questions whatsoever in the RabbitHole Discord. \n\nSee you on the other side of the RabbitHole!", "timestamp": 1629679980, "digest": "6UF5AgtcoSBgHvJR_-ZvAyBEJiiCEINP1SEXsRpe2NM", "contributor": "0xF69EFCc2dECE59Fbc75B23aE75f51DD7Be3bb1E1"}
{"id": "3aCP0hOTUJte4lbnxe7gcqc2pi6Vf_AW-2NskP-VZKs", "title": "UX Research to Improve NFT Auction Experience, using Mempool Data", "body": "User experience (UX) describes how people feel when they interact with a system or service and encompasses several factors including usability, design, marketing, accessibility, performance, comfort, and utility. Don Norman once said,\n\n> “Everything has a personality; everything sends an emotional signal. Even where this was not the intention of the designer, the people who view the website infer personalities and experience emotions. Bad websites have horrible personalities and instill horrid emotional states in their users, usually unwittingly. We need to design things — products, websites, services — to convey whatever personality and emotions are desired.”\n\nEthereum's personality is someone who is extremely inscrutable and easily misunderstood. To make matters worse, most users don't even think about it as interacting with Ethereum when they're using your interface or a wallet. If you're ever in the live chat of an Artblock's auction, you'll notice as soon as an auction ends there are at least a dozen people complaining that it is Metamask's fault that they didn't get a mint. I think in the last year the UX of many dapps on Ethereum has improved greatly in both product interaction and explainability of transactions. For the most part, dapps don't just leave you with a loading spinner after you sign the transaction anymore.\n\nEven with the design of dapps is generally improving, I'm not sure how deep UX research has gone yet. When I see data analytics or research on various protocols, users are mostly treated as homogenous. This is somewhat shifting with some of the analysis I've seen of Uniswap V3 liquidity providers and Rabbithole Questers, but even those are still heavily focused on just confirmed transactions on-chain. From my own experience, most of the emotion evoked and behavioral quirks happen when I'm submitting, waiting, speeding up, or canceling my transaction. For some applications, the user might leave and go do something else after submitting a transaction. But for a product like Artblock's auctions, they're going to stay around until the confirmation has happened, likely checking anything they can for updates and with compounding anxiety.\n\nI think we can do a much better job of understanding user behaviors and frictions by starting to leverage the mempool more. The [mempool](https://compassmining.io/education/what-is-a-mempool/) is where unconfirmed transactions are temporarily stored by a node. This means if you submit, speed up, or cancel your transaction then those actions will show up in the mempool first. It's important to note that the data from the mempool is not stored in the node, so you can't query historical data the same way you could on confirmed transactions. From here, you can see that they submitted a few transactions, sped up the transactions quite a few times but nowhere near the gas price needed, and ultimately saw confirmations 20 blocks later. I believe this is a pretty good proxy for user experience and likely emotions they were feeling throughout the whole process. If we understand how different groups of users behave in this cycle, we can figure out how to supplement their decision-making or ease their anxiety. To my knowledge, pretty much only the Ethereum Foundation, All Core Devs, and some wallet teams leverage the mempool data for UX reasons.\n\n**UX Research Thesis:** By looking at a user's behavior through auctions over time and also their wallet history, we can start to give behavioral identities to different groups of users. From here, we can identify the main issues to try and alleviate. We'll do this by taking a month of Artblocks Auctions data using [Blocknative](https://www.blocknative.com/), and layering in the history of these addresses using [Dune queries](https://dune.xyz/).\n\nThis article will be more technical than some of my previous ones since I believe the work can and should be generalized fairly easily.     *I want to emphasize that my background is not in UX research, I'm purely experimenting with what I think crypto-native UX research could look like.*\n\n## Data Sourcing and Preprocessing All Auction Data\n\n*If you have no interest in the technical bits, skip to the next section on Feature Engineering*\n\n### Blocknative and Mempool Data Streaming\n\nUsing Blocknative's Mempool explorer, you can filter for transactions submitted to specific contracts or from specific wallets. In my case, I wanted to listen to the whitelisted minter contracts for Artblock's NFT contract. You can find the stream that I used [here](https://tinyurl.com/yftus9h2), and save it down if you want to use the exact same setup.\n\nYou can find the whitelisted minter addresses with the following query [in their subgraph](https://thegraph.com/legacy-explorer/subgraph/artblocks/art-blocks):\n\n```python\n{\n  contracts(first: 2) {\n    id\n    mintWhitelisted\n  }\n}\n```\n\nThere were three steps to get to a subscription filter for all purchases:\n\n1. Add the new address with the \"create new subscription\" button\n2. Add the ABIs by clicking the \"ABI\" button next to the address. In my case I just needed the \"purchase\" function.\n\n```python\n{\n    \"inputs\": [\n      {\n        \"internalType\": \"uint256\",\n        \"name\": \"_projectId\",\n        \"type\": \"uint256\"\n      }\n    ],\n    \"name\": \"purchase\",\n    \"outputs\": [\n      {\n        \"internalType\": \"uint256\",\n        \"name\": \"_tokenId\",\n        \"type\": \"uint256\"\n      }\n    ],\n    \"stateMutability\": \"payable\",\n    \"type\": \"function\"\n  }\n```\n\n1. Add filters for the `methodName` matches `purchase` (make sure you don't do a global filter)\n\nIn the end, your setup should look like this:\n\n![](https://images.mirror-media.xyz/publication-images/ad1f283a-7ce1-4899-9fa8-bdfda6a716e1.png?height=577&width=844)\n\nTo store this data down, I created a ngrok/express endpoint to store in an SQLite database, run locally. I've created a [GitHub template with steps to replicate this setup](https://github.com/andrewhong5297/blocknative_stream_to_sql). Probably the most important point to remember here is that you need to include the POST endpoint as part of the ngrok URL when adding it as a webhook in the Blocknative account page.\n\n### Key Preprocessing Functions\n\n**Multiple Transaction Hashes**\n\nWhen you speed up or cancel a transaction, the original transaction hash is replaced with the new transaction. This means if you want to track a users' transaction through its full lifecycle, you need to reconcile the new transaction hashes with the original one. Assuming you speed up a transaction five times, you'll have six hashes total (the original hash + five new hashes). I reconciled this by getting a dictionary mapping of `tx_hash` to the new `replaceHash`, and then replaced recursively.\n\n```python\nreplaceHashKeys = dict(zip(auctions[\"replaceHash\"],auctions[\"tx_hash\"])) #assign tx_hash based on replacements, just to keep consistency. \nreplaceHashKeys.pop(\"none\") #remove none key\n\ndef recursive_tx_search(key):\n    if key in replaceHashKeys:\n        return recursive_tx_search(replaceHashKeys[key])\n    else:\n        return key\n\nauctions[\"tx_hash\"] = auctions[\"tx_hash\"].apply(lambda x: recursive_tx_search(x))\n```\n\n**Blocknumber Issues**\n\nDropped transactions have a `blocknumber` of 0, so to deal with this I sorted my dataframe by `timestamp` in ascending order, and then did a backward fill so the 0 would be replaced by the correct `blocknumber` it was dropped in. This is important fix for feature engineering.\n\n```python\nauctions = auctions.sort_values(by=\"timestamp\",ascending=True)\nauctions[\"blocknumber\"] = auctions[\"blocknumber\"].replace(to_replace=0, method='bfill') #deal with dropped txs that show as blocknumber 0\n```\n\n**Dealing with Mints Outside of Main Auction Period**\n\nFor most projects, the artist will mint a few pieces before the auction opens up to the public. Some projects don't sell out right away, so you will get mints still occurring a few days after the auction has opened. My analysis is focused on the key auction period, mostly the first 30 minutes. To get rid of the two mint cases above, I removed outliers based on `blocknumber`.\n\n```python\nto_remove_indicies = []\nfor project in list(set(auctions[\"projectId\"])):\n    auction_spec = auctions[auctions[\"projectId\"]==project]\n    all_times = pd.Series(list(set(auction_spec.blocknumber)))\n    to_remove_blocktimes = all_times[(np.abs(stats.zscore(all_times)) > 2.5)]\n    if len(to_remove_blocktimes)==0:\n        break\n    to_remove_indicies.extend(auction_spec.index[auction_spec['blocknumber'].isin(to_remove_blocktimes)].tolist())\nauctions.drop(index=to_remove_indicies, inplace=True)\n```\n\n**Adding on Dutch Auction Prices**\n\nFor all projects in the dataset besides project 118, a Dutch auction price format was used. I took the mint price data using a [dune query](https://dune.xyz/queries/113834), and then merged it onto the dataset. I had to use a forward and backward fill for the blocks that had mempool actions but no confirmations during the auction.\n\n```python\nauction_prices = pd.read_csv(r'artblock_auctions_analytics/datasets/dune_auction_prices.csv', index_col=0)\nauctions = pd.merge(auctions,auction_prices, how=\"left\", left_on=[\"projectId\",\"blocknumber\"],right_on=[\"projectId\",\"blocknumber\"])\nauctions.sort_values(by=[\"projectId\",\"blocknumber\"], ascending=True, inplace=True)\nauctions[\"price_eth\"].fillna(method=\"ffill\", inplace=True)\nauctions[\"price_eth\"].fillna(method=\"bfill\", inplace=True)\n```\n\n## Feature Engineering For Each Auction\n\n*If you have no interest in the technical bits, just read the parts in bold and skip the rest.*\n\nIn data science, a feature is a variable that is calculated from the larger dataset to be used as an input in some sort of model or algorithm. All features are calculated in the     `preprocess_auction` function and are calculated *per auction rather than combining all the auctions into a feature set.*\n\n**The first set of features are the totals for transaction states**, and was a simple `pivot_table` function:\n\n* `number_submitted` : total number of transactions submitted\n* `cancel` : count of transactions that ended in canceled\n* `failed`: count of transactions that ended in failed\n* `dropped`: count of transactions that ended in dropped\n* `confirmed`: count of transactions that ended in confirmed\n\nI mentioned earlier some data was not captured for auctions due to various issues, these transactions were dropped from the dataset.\n\n**The next set of features included their gas behavior.**         The key concept here was capturing how far away their transaction gas was from the average confirmed gas per block (shifted by 1 block). **Then we can create features for the average, median, and standard deviation of gas price distance over the whole auction.** There are a bunch of transposes and index resets to get the `blocknumber` columns in the right order, but the important function is `fill_pending_values_gas` which forward fills the gas price between actions captured. This means that if I put in a transaction at `blocknumber` 1000 with a gas of 0.05 ETH and my next action wasn't until `blocknumber` 1005 where I sped up to 0.1 ETH gas, then this function will fill in the blocks between 1000-1005 with 0.05 ETH.\n\n```python\ndef fill_pending_values_gas(x):\n    first = x.first_valid_index()\n    last = x.last_valid_index()\n    x.loc[first:last] = x.loc[first:last].fillna(method=\"ffill\")\n    return x\n```\n\n**The third set of features were calculating the total number and frequency of actions taken in the auction.**     Here we start with a pivot of total actions (speed ups) per block, with some special calculations for getting the first instance of pending for each transaction:\n\n```python\nget_first_pending = df[df[\"status\"]==\"pending\"] #first submitted \nget_first_pending = get_first_pending.drop_duplicates(subset=[\"tx_hash\",\"status\"], keep=\"first\")\nauctions_time_data = pd.concat([get_first_pending,df[df[\"status\"]==\"speedup\"]], axis=0)\ntime_action = auctions_time_data.pivot_table(index=[\"sender\",\"tx_hash\"], columns=\"blocknumber\",values=\"status\",aggfunc=\"count\") \\\n                    .reindex(set(df[\"blocknumber\"]), axis=1, fill_value=np.nan)\n```\n\nFrom here we get to `average_action_delay` in three steps:\n\n1. we take the number of actions per block (yes some people sped up transactions multiple times in the same block)\n2. we drop the blocks with no actions, and then take the difference between the blocknumbers that remain. We add a 0 for each additional action taken per block.\n3. Taking the mean across the differences and added zeroes gives us the `average_action_delay`\n\n```python\ndef get_actions_diff(row):\n    row = row.dropna().reset_index()\n    actions_diff_nominal =list(row[\"blocknumber\"].diff(1).fillna(0))\n \n    #take the blocks with muliple actions and subtract one, then sum up. \n    zeros_to_add = sum([ actions - 1 if actions > 1 else 0 for actions in row[row.columns[1]]])\n    actions_diff_nominal.extend(list(np.zeros(int(zeros_to_add))))\n    actions_diff = np.mean(actions_diff_nominal)\n    if (actions_diff==0) and (zeros_to_add==0):\n        return 2000 #meaning they never took another action\n    else:\n        return actions_diff\n```\n\n`total_actions` is much simpler, as it is just the sum of actions across the pivot.\n\n```python\ntime_action[\"total_actions\"] = time_action.iloc[:,:-1].sum(axis=1)\n```\n\n**The last time-dependent feature is** `block_entry`, which is an important one due to the introduction of Dutch auctions. Essentially this tracks what block the transaction was submitted on since the start.\n\n```python\nget_first_pending[\"block_entry\"] =   get_first_pending[\"blocknumber\"] - get_first_pending[\"blocknumber\"].min()\n\nentry_pivot = get_first_pending.pivot_table(index=\"sender\",values=\"block_entry\",aggfunc=\"min\")\n```\n\n`price_eth` is added as a feature as well, which is tied to the `block_entry` point.\n\n**The last set of features were based on a Dune query, specifically the days since the first transaction, total gas used in transactions, and the total number of transactions.**     To get the address array in the right format I used the following line of code after reading in the SQL data:\n\n```python\nall_users = list(set(auctions[\"sender\"].apply(lambda x: x.replace('0x','\\\\x'))))\nall_users_string = \"('\" + \"'),('\".join(all_users) + \"')\"\n```\n\nThe dune query for this is fairly simple. I pasted the addresses string under `VALUES`, and made some CTEs to get the features I wanted. In the final `SELECT` I tried to add each address's ens as well. You can find the query here: <https://dune.xyz/queries/96523>\n\nLastly, we just merge in the per wallet data on days active, total gas used, and the total number of transactions.\n\n```python\nauctions_all_df = pd.merge(auctions_all_df,wh,on=\"sender\",how=\"left\")\nauctions_all_df.set_index([\"sender\",\"ens\"],inplace=True)\n```\n\n*With all this done, we're finally ready to run some fun unsupervised learning algorithms and try and validate our hypothesis on user groups.*\n\n## Clustering and Visualizing User Groups\n\nBefore I started this project, I had expected to see the following user groups pop out of the data:\n\n* **Set and Forget:** There should be two groups here, those who set a transaction with really high gas and an average/low gas and then don't touch it for the rest of the auction.\n* **Speed Up:** There should be two groups here as well, those who are speeding up often and updating the transaction directly as a factor of gas prices and those who are speeding up often but with basically no change in gas price.\n\nI was very interested in validating these groups, seeing how large each group was, and seeing if any users moved between groups over the course of many auctions. The easiest way to do this was to use unsupervised machine learning to identify clusters of user groups based on the variability across all features. Essentially this is like looking at the distribution of income in a state and then splitting it into sub-distributions for different income concentrations, geographic coordinates, and age. Note that this isn't binning, where the distribution is split into equal ranges - it is instead calculated based on the density of observations within the whole range. The approach we will take is called \"unsupervised\" because our dataset doesn't have any existing labels, rather than something like a regression where there is a value being predicted that can be verified as right or wrong.\n\nThe algorithm I decided to use is called [k-means](https://stanford.edu/\\~cpiech/cs221/handouts/kmeans.html), where k stands for the number of clusters you are expecting to identify. Each cluster has a \"centroid\", which is like the center of a circle. There are various methods for figuring out how many clusters are optimal, and the two I used were elbow points and silhouette scores. Those are both fancy ways of asking,\n\n> \"does each additional cluster help increase the density of the cluster (calculated as the average distance of points in a cluster from the centroid) and maintain adequate separation between clusters (no overlap between two clusters)?\"\n\nI found that 3 clusters were optimal in terms of most inertia improvement while keeping a high silhouette score (around 0.55).\n\n![6 auctions were used for this analysis](https://images.mirror-media.xyz/publication-images/7bef376a-b96c-42de-87b6-991db4503ef9.png?height=264&width=393)\n\nWith clusters chosen, we want to be able to visualize and verify their existence. There are over 15 variables, so we need to reduce the number of dimensions in order to plot it. Dimension reduction typically relies on either PCA or t-SNE algorithms, in our case I went with t-SNE. Don't worry too much about understanding this part, these algorithms essentially capture the variance across all features to give us X and Y components that maximize the spread of points from each other.\n\n**Let's look at project 118, LeWitt Generator Generator, from August 4th:**\n\n![](https://images.mirror-media.xyz/publication-images/4f7dad88-47b6-4dde-9a52-f0e4258cb829.png?height=605&width=615)\n\n![These are the sub-distributions for each variable by cluster, calculated using a KDE. The colors match those in the above clusters.](https://images.mirror-media.xyz/publication-images/5bafdebf-7b9b-43b3-8ed0-7e4ac923725f.png?height=856&width=1123)\n\nAfter taking a look at the sub-distributions for each variable and some data examples, I was able to categorize the clusters. The Orange cluster is the group that is speeding up the most while also submitting slightly lower gas transactions on average. Blue and Green clusters exhibit similar behavior to each other, but addresses in Blue typically have less history than addresses in the Green cluster.\n\nLooking at the overall picture, it seems the original hypothesis on \"speed-up\" and \"set high and set low\" producing two groups each was wrong. Instead, we have a single \"speed-up\" group (Orange) and a single \"set-and-forget\" group (Blue and Green are behaviorally the same). I think the new (Blue) versus old wallets (Green) in the \"set and forget\" group probably carries a lot of overlap in actual users, where users just created new wallets to bid on more mints. Based on their impatience and lower-than-average gas prices, the \"speed-up\" group reads to me as either less experienced or just more greedy than other users. This group also seems to be fairly anxious throughout the whole auction. *What did surprise me is that the speed-up group makes up a smaller proportion of total bidders, as I had expected that group to make up 60-70% of bidders instead of 30%.*\n\n**Now, where this user behavior study gets really interesting is when comparing project 118 (the set price of 0.75 ETH) to project 140 (Dutch auction with the price decreasing from 1.559 to .159 ETH).**\n\n**Here's the clustering for project 140, Good Vibrations from August 21st:**\n\n![](https://images.mirror-media.xyz/publication-images/ae090b2e-141c-4dd0-a0cb-06f4ed45dd13.png?height=605&width=615)\n\n![](https://images.mirror-media.xyz/publication-images/7294dd21-59e5-48fd-b7a1-7babbda77040.png?height=856&width=1130)\n\nWe can see that now most of the clustering variability comes from `block_entry`, `price_eth`, and all of the `gas_behavior` features. This is a big departure from the main variables for project 118. In 118, the set price meant that people entered the auction in a fairly uniform distribution (quantity left didn't seem to matter), and the \"speed-up\" group makes actions up fairly endlessly - probably very anxious.\n\nIn project 140, we don't see the same difference in actions within `average_action_delay` or `total_actions` , instead, we see probably that same \"speed-up\" group entering at a very late stage block and setting far below-average gas prices as seen in `average_gas_behavior`. The Green cluster probably represents users with more experience than the Orange cluster, but who are still transitioning between Orange and Blue in terms of their behavior. If I was to try and map this to the clusters in 118, I believe the \"speed-up\" group has now become the \"greedy\" group (Orange) that is entering late and bidding a low gas. The \"set-and-forget\" group maps pretty well to the \"early-grab\" group (Green and Blue), since they all exhibit pretty good patience and an adequate safety net on gas bids.\n\n**I call the Orange group \"greedy\" not just because of their behavior, but also because of their rate of failed transactions.**\n\nFor project 118, fail rates are of the \"speed-up\" versus \"set-and-forget\" groups are within 10-15%.\n\n![percent_lost takes (cancel + dropped + failed) / number_submitted](https://images.mirror-media.xyz/publication-images/306752cb-87c2-4494-8915-86102c214d29.png?height=77&width=724)\n\nFor project 140, the fail rate of the \"greedy\" cluster is around 69% versus the \"early-grab\" group at around 5-15%.\n\n![](https://images.mirror-media.xyz/publication-images/55ceaf8d-6756-4807-beb9-fcfbbe4e549e.png?height=84&width=727)\n\n**Overall, my read of this is that the group's bad habits and emotions were amplified - it feels to me like we made a tradeoff in anxiety → greed. This may have made the auction less stressful, but ultimately led to more users being upset (due to failed mints).**\n\n*I'm sure there's a more granular analysis that can be done to segment the auctions further based on factory/curated/playground or by artists themselves too. This will only get more interesting and complex as the community continues to grow, and emotions play a larger factor in both a single auction and on if they return for future auctions.*\n\n*This study of multiple auctions helped us validate our assumptions, understand the proportions of user groups, and see how users' good or bad behaviors shift over time (and other parameters). Now we need to plug it into the rest of the product cycle process.*\n\n## Where We Go From Here:\n\nThe reason that I chose just Artblocks auctions for this and not a mix of platforms is because I wanted to look for a place where the variability in terms of interface and project type is mostly controlled. This should have given us fairly consistent users and behavior types.\n\nThis is just the start of a UX research cycle, so ideally we could continue in the following steps:\n\n1. Use an unsupervised machine learning algorithm to identify the user groups (clusters) and see how many people are making \"mistakes\" when entering an auction. *This is the step we covered today.*\n2. Create a new user interface, such as a [histogram view on the bidding screen](https://twitter.com/andrewhong5297/status/1423020670825504768), or showing historical data on when most people usually enter/crowd the auction and at what prices. Anything to give both current and historical context to the user, especially those from the speed cluster.\n3. With each auction, run the mempool/wallet data through the created algorithm to see if the user groups have shifted and if specific users have \"learned\" to participate in the auction differently (i.e. did they move between user groups). *I think that the most value can be found in this step if done well. Using ENS or other identifiers to help supplement this grouping would be exponentially helpful too*\n4. Based on the results, continue to iterate on the user interface and design. You could also run more informed A/B testing since you could even figure out what screens to show by making an educated guess based on the users' last cluster (or use label propagation for new users).\n\n**The Dutch auction-style change is also an example of step #2, and we were able to see a clear shift in user behaviors.** While typically this sort of A/B testing is focused on increasing engagement or conversions, here we are optimizing for the user's ability to learn and improve instead. This may become even more robust if this was iterated in a multiplatform context, so that we can study how someone is learning at an ecosystem level (maybe even supplement with Rabbithole data and user profiles). Since my Artblocks user research is all based on publicly sourced data, it can be replicated and supplemented by any other auction/sale platform. **Crypto could be the first industry that would have a synchronized and transparent set of user groups and UX research, to be applied in products and academia.** Nansen wallet labeling is already a step towards this, but it's different when teams from different products build this up from various facets and approaches.\n\nWhat I would eventually envision is using data to build on the following user personas (with subgroups/levels within them):\n\n* I want to buy a Fidenza, so I can either buy one through a private sale, bid on one in an auction myself, bid on one in a prtyDAO bid auction, or buy a fraction of one with [fractional.art](https://fractional.art/)\n* I like Fidenzas in general, so I'll just buy the NFTX Fidenza index token or an NFT basket of Artblocks Curated on [fractional.art](https://fractional.art/)\n* I'm a collector already, so I want to swap or bid on a Fidenza using a curated set of NFTs and ERC20s I already hold (using genie-xyz swap).\n* I like the rush of acquiring through initial mint versus secondary market, and heavily participate in auctions like Artblocks live mints.\n\nI hope you found this project interesting and/or helpful, I had a ton of fun with it. Thanks to the folks at Blocknative for setting me up, and the community at Artblocks for answering my many auction questions. As always, feel free to reach out with any questions or ideas you may have!\n\nYou can find the GitHub repo [with all the data and script here](https://github.com/andrewhong5297/artblock_auctions_analysis). The script may be a bit hard to read since I’m still refactoring and cleaning it up. The script and some of the analysis here may get updated as I analyze the last few auctions of August for new patterns.", "timestamp": 1629683368, "digest": "l_-4fQ08cpxUZpn9V9S5R27wfKvNgdnrXlZAWZWvdlg", "contributor": "0x2Ae8c972fB2E6c00ddED8986E2dc672ED190DA06"}
{"id": "8QN_hen-KHTyajVYG8NuNzsZcEXNhnbMlzeEa-ANkg8", "title": "selah: Ꮧ ᏕᏖᏗᏒ ᎥᏕ ᏰᎧᏒᏁ", "body": "This week marks the release of *Star Power*, Marley’s genre-bending EP and first official release. Selah is dropping her music video SLICK on Glass marking the first video auction ever on Glass.\n\n> ***“Music is in Selah Marley’s blood. As the daughter of eight-time Grammy winner Lauryn Hill and the granddaughter of reggae icon Bob Marley, she descends from two of the most influential artists ever.“***\n>\n> ***— Vogue (Janelle Okwodu, 2021).***\n\nA deeply personal undertaking, Selah raps about growing up in the shadow of fame, asserting ownership over her body in a society that prioritizes the male gaze, and the power of reclaiming your identity. Selah is coming to her own.\n\nSLICK is Selah’s second NFT drop for her STAR POWER EP. The first collection was titled 24HRS.\n\n# 24HRS\n\nVisually depicting the theme of her song, 24HRS and her EP, STAR POWER as a whole. Collection 1 consisted of 1 live auction NFT and 2 open edition NFTs that granted buyers access to a variety of redeemable rewards ranging from a vacation in Puerto Morales, Mexico to exclusive STAR POWER content (unreleased visuals, etc).\n\n![DAYSTAR](https://images.mirror-media.xyz/publication-images/77404c6c-5051-492d-868e-6bd0d83750ca.gif?height=320&width=320)\n\n> The 24HRS Daystar open editions were available for purchase with a credit card and priced at 12 dollars. The goal was to **lower the barrier to entry** for day-one fans and **offer future perks to day-one NFT holders**.\n\nThe SLICK collection is her first time displaying her work to audiences outside of her core fans. The video visually reflects the multifaceted nature of Selah's Star Power. It is the first time she is expanding outside of her core cult-like following and giving new early supporters a closer peak into her life story.\n\n# THE SLICK EXPERIENCE\n\nSLICK is a 3-part experience, in which Selah will release a collection of art through different mediums over the course of a week.\n\n### Music Video\n\nSLICK will launch on **Monday 8/23 at 1:30PM EST** with the music video—a visual depiction of Selah’s coming-of-age reflected by a few of her siblings who recount their memories with Selah, as well as their own adolescent experiences. Featured on the track & in the video is Selah’s brother, Josh (YGMARLEY) who gives his own perspective as a maturing adolescent in this unique position. The SLICK music video will be **auctioned on Glass as a 1/1 video NFT**.\n\n![SLICK Music Video](https://images.mirror-media.xyz/publication-images/2e993440-9e4d-4df5-8b5c-a99fa37f9e46.png?height=720&width=1439)\n\n### Video Game\n\nThe SLICK video game will follow, releasing **Wednesday (8/25)**. The SLICK video game is an interactive GameBoy game (available on web, iOS/Google, & GameBoy ROM), in which the player will get to experience & play as Selah as she tries to be SLICK with her father to rebel against his rules. It’s an interactive walkthrough of how the song SLICK came into creation—a bit of a Pinocchio story.\n\n![SLICK Gameplay](https://images.mirror-media.xyz/publication-images/4b0a0841-2b0a-4579-8605-5741be567448.gif?height=271&width=300)\n\n# NFT Electronics Pack\n\nFinally, the week will close with the release of the SLICK NFT electronics pack—a limited collection of 3D motion graphics tied with physical and digital perks.\n\n### Cassette Tape\n\nThe cassette tape plays the evolution of SLICK as it plays the different variations Selah & her brother made over time.  The virtual cassette tape will be met with a physical cassette tape collection with all of the SLICK versions.\n\n![SLICK Cassette Tape Still](https://images.mirror-media.xyz/publication-images/7726a756-f266-49d7-ad6c-cf66a68ee32f.jpeg?height=1410&width=1406)\n\n### GameBoy\n\nThe GameBoy plays a visual walkthrough of the SLICK video game. The virtual GameBoy will be met with a physical GameBoy cartridge & a custom STAR POWER Gameboy.\n\n![SLICK GameBoy Still](https://images.mirror-media.xyz/publication-images/e2433902-0523-4a36-8ab1-25f6530d5f9f.jpeg?height=1394&width=1394)\n\n### iMac G3\n\nThe iMac G3 plays behind-the-scenes clips of the SLICK music video. The virtual G3 will be met with a physical DVD of the SLICK music video, behind-the-scenes, and an extended cut.\n\n![SLICK G3 Still](https://images.mirror-media.xyz/publication-images/5831c704-8a60-4e59-9af5-a26843651242.jpeg?height=750&width=750)\n\n# The Glass Drop\n\n**The winner of the SLICK music video auction will get the 1/1 SLICK music video NFT, and one edition from each item in the SLICK Electronics Pack (1 cassette tape, 1 GameBoy, and 1 Mac G3) along with the physical items.**\n\nAuction Opens: Monday 8/23 at 1:30PM EST\n\n# Links\n\n**<http://starpower.digital/>**\n\n**<https://selah.ffm.to/starpower>**\n\n**<https://www.vogue.com/article/selah-marley-star-power-ep-interview>**\n\n**<https://www.instagram.com/selah>**\n\n**<https://www.bitski.com/starpower>**", "timestamp": 1629693624, "digest": "Y-vmz2ihDf_AB3dEXhk8XEymx_ygbfN25g2szX5XHeQ", "contributor": "0xDb6c1a8aF1883262aaD221A25816468ef693D4A2"}
{"id": "8zmjCQ2EW0rPg6LMRPKDh9F9PClmYGSqJFO4sHxmEPw", "title": "selah: Ꮧ ᏕᏖᏗᏒ ᎥᏕ ᏰᎧᏒᏁ", "body": "This week marks the release of *Star Power*, Marley’s genre-bending EP and first official release. Selah is dropping her music video SLICK on Glass marking the first video auction ever on Glass.\n\n> ***“Music is in Selah Marley’s blood. As the daughter of eight-time Grammy winner Lauryn Hill and the granddaughter of reggae icon Bob Marley, she descends from two of the most influential artists ever.“***\n>\n> ***— Vogue (Janelle Okwodu, 2021).***\n\nA deeply personal undertaking, Selah raps about growing up in the shadow of fame, asserting ownership over her body in a society that prioritizes the male gaze, and the power of reclaiming your identity. Selah is coming to her own.\n\nSLICK is Selah’s second NFT drop for her STAR POWER EP. The first collection was titled 24HRS.\n\n# 24HRS\n\nVisually depicting the theme of her song, 24HRS and her EP, STAR POWER as a whole. Collection 1 consisted of 1 live auction NFT and 2 open edition NFTs that granted buyers access to a variety of redeemable rewards ranging from a vacation in Puerto Morales, Mexico to exclusive STAR POWER content (unreleased visuals, etc).\n\n![DAYSTAR](https://images.mirror-media.xyz/publication-images/77404c6c-5051-492d-868e-6bd0d83750ca.gif?height=320&width=320)\n\n> The 24HRS Daystar open editions were available for purchase with a credit card and priced at 12 dollars. The goal was to **lower the barrier to entry** for day-one fans and **offer future perks to day-one NFT holders**.\n\nThe SLICK collection is her first time displaying her work to audiences outside of her core fans. The video visually reflects the multifaceted nature of Selah's Star Power. It is the first time she is expanding outside of her core cult-like following and giving new early supporters a closer peak into her life story.\n\n# THE SLICK EXPERIENCE\n\nSLICK is a 3-part experience, in which Selah will release a collection of art through different mediums over the course of a week.\n\n### Music Video\n\nSLICK will launch on **Monday 8/23 at 1:30PM EST** with the music video—a visual depiction of Selah’s coming-of-age reflected by a few of her siblings who recount their memories with Selah, as well as their own adolescent experiences. Featured on the track & in the video is Selah’s brother, Josh (YGMARLEY) who gives his own perspective as a maturing adolescent in this unique position. The SLICK music video will be **auctioned on Glass as a 1/1 video NFT**.\n\n![SLICK Music Video](https://images.mirror-media.xyz/publication-images/2e993440-9e4d-4df5-8b5c-a99fa37f9e46.png?height=720&width=1439)\n\n### Video Game\n\nThe SLICK video game will follow, releasing **Wednesday (8/25)**. The SLICK video game is an interactive GameBoy game (available on web, iOS/Google, & GameBoy ROM), in which the player will get to experience & play as Selah as she tries to be SLICK with her father to rebel against his rules. It’s an interactive walkthrough of how the song SLICK came into creation—a bit of a Pinocchio story.\n\n![SLICK Gameplay](https://images.mirror-media.xyz/publication-images/4b0a0841-2b0a-4579-8605-5741be567448.gif?height=271&width=300)\n\n# NFT Electronics Pack\n\nFinally, the week will close with the release of the SLICK NFT electronics pack—a limited collection of 3D motion graphics tied with physical and digital perks.\n\n### Cassette Tape\n\nThe cassette tape plays the evolution of SLICK as it plays the different variations Selah & her brother made over time.  The virtual cassette tape will be met with a physical cassette tape collection with all of the SLICK versions.\n\n![SLICK Cassette Tape Still](https://images.mirror-media.xyz/publication-images/7726a756-f266-49d7-ad6c-cf66a68ee32f.jpeg?height=1410&width=1406)\n\n### GameBoy\n\nThe GameBoy plays a visual walkthrough of the SLICK video game. The virtual GameBoy will be met with a physical GameBoy cartridge & a custom STAR POWER Gameboy.\n\n![SLICK GameBoy Still](https://images.mirror-media.xyz/publication-images/e2433902-0523-4a36-8ab1-25f6530d5f9f.jpeg?height=1394&width=1394)\n\n### iMac G3\n\nThe iMac G3 plays behind-the-scenes clips of the SLICK music video. The virtual G3 will be met with a physical DVD of the SLICK music video, behind-the-scenes, and an extended cut.\n\n![SLICK G3 Still](https://images.mirror-media.xyz/publication-images/5831c704-8a60-4e59-9af5-a26843651242.jpeg?height=750&width=750)\n\n# The Glass Drop\n\n**The winner of the SLICK music video auction will get the 1/1 SLICK music video NFT, and one edition from each item in the SLICK Electronics Pack (1 cassette tape, 1 GameBoy, and 1 Mac G3) along with the physical items.**\n\nAuction Opens: Monday 8/23 at 1:30PM EST\n\n# Links\n\n**<http://starpower.digital/>**\n\n**<https://selah.ffm.to/starpower>**\n\n**<https://www.vogue.com/article/selah-marley-star-power-ep-interview>**\n\n**<https://www.instagram.com/selah>**\n\n**<https://www.bitski.com/starpower>**", "timestamp": 1629693620, "digest": "Y-vmz2ihDf_AB3dEXhk8XEymx_ygbfN25g2szX5XHeQ", "contributor": "0xDb6c1a8aF1883262aaD221A25816468ef693D4A2"}
{"id": "uc7j1EMgL6f03zIYIRLdcyqWZ46quDEORj7fY1ju-do", "title": "selah: Ꮧ ᏕTᏗᏒ IᏕ ᗷOᏒᏁ", "body": "This week marks the release of *Star Power*, Marley’s genre-bending EP and first official release. Selah is dropping her music video SLICK on Glass marking the first video auction ever on Glass.\n\n> ***“Music is in Selah Marley’s blood. As the daughter of eight-time Grammy winner Lauryn Hill and the granddaughter of reggae icon Bob Marley, she descends from two of the most influential artists ever.“***\n>\n> ***— Vogue (Janelle Okwodu, 2021).***\n\nA deeply personal undertaking, Selah raps about growing up in the shadow of fame, asserting ownership over her body in a society that prioritizes the male gaze, and the power of reclaiming your identity. Selah is coming to her own.\n\nSLICK is Selah’s second NFT drop for her STAR POWER EP. The first collection was titled 24HRS.\n\n# 24HRS\n\nVisually depicting the theme of her song, 24HRS and her EP, STAR POWER as a whole. Collection 1 consisted of 1 live auction NFT and 2 open edition NFTs that granted buyers access to a variety of redeemable rewards ranging from a vacation in Puerto Morales, Mexico to exclusive STAR POWER content (unreleased visuals, etc).\n\n![DAYSTAR](https://images.mirror-media.xyz/publication-images/77404c6c-5051-492d-868e-6bd0d83750ca.gif?height=320&width=320)\n\n> The 24HRS Daystar open editions were available for purchase with a credit card and priced at 12 dollars. The goal was to **lower the barrier to entry** for day-one fans and **offer future perks to day-one NFT holders**.\n\nThe SLICK collection is her first time displaying her work to audiences outside of her core fans. The video visually reflects the multifaceted nature of Selah's Star Power. It is the first time she is expanding outside of her core cult-like following and giving new early supporters a closer peak into her life story.\n\n# THE SLICK EXPERIENCE\n\nSLICK is a 3-part experience, in which Selah will release a collection of art through different mediums over the course of a week.\n\n### Music Video\n\nSLICK will launch on **Monday 8/23 at 1:30PM EST** with the music video—a visual depiction of Selah’s coming-of-age reflected by a few of her siblings who recount their memories with Selah, as well as their own adolescent experiences. Featured on the track & in the video is Selah’s brother, Josh (YGMARLEY) who gives his own perspective as a maturing adolescent in this unique position. The SLICK music video will be **auctioned on Glass as a 1/1 video NFT**.\n\n![SLICK Music Video](https://images.mirror-media.xyz/publication-images/2e993440-9e4d-4df5-8b5c-a99fa37f9e46.png?height=720&width=1439)\n\n### Video Game\n\nThe SLICK video game will follow, releasing **Wednesday (8/25)**. The SLICK video game is an interactive GameBoy game (available on web, iOS/Google, & GameBoy ROM), in which the player will get to experience & play as Selah as she tries to be SLICK with her father to rebel against his rules. It’s an interactive walkthrough of how the song SLICK came into creation—a bit of a Pinocchio story.\n\n![SLICK Gameplay](https://images.mirror-media.xyz/publication-images/4b0a0841-2b0a-4579-8605-5741be567448.gif?height=271&width=300)\n\n# NFT Electronics Pack\n\nFinally, the week will close with the release of the SLICK NFT electronics pack—a limited collection of 3D motion graphics tied with physical and digital perks.\n\n### Cassette Tape\n\nThe cassette tape plays the evolution of SLICK as it plays the different variations Selah & her brother made over time.  The virtual cassette tape will be met with a physical cassette tape collection with all of the SLICK versions.\n\n![SLICK Cassette Tape Still](https://images.mirror-media.xyz/publication-images/7726a756-f266-49d7-ad6c-cf66a68ee32f.jpeg?height=1410&width=1406)\n\n### GameBoy\n\nThe GameBoy plays a visual walkthrough of the SLICK video game. The virtual GameBoy will be met with a physical GameBoy cartridge & a custom STAR POWER Gameboy.\n\n![SLICK GameBoy Still](https://images.mirror-media.xyz/publication-images/e2433902-0523-4a36-8ab1-25f6530d5f9f.jpeg?height=1394&width=1394)\n\n### iMac G3\n\nThe iMac G3 plays behind-the-scenes clips of the SLICK music video. The virtual G3 will be met with a physical DVD of the SLICK music video, behind-the-scenes, and an extended cut.\n\n![SLICK G3 Still](https://images.mirror-media.xyz/publication-images/5831c704-8a60-4e59-9af5-a26843651242.jpeg?height=750&width=750)\n\n# The Glass Drop\n\n**The winner of the SLICK music video auction will get the 1/1 SLICK music video NFT, and one edition from each item in the SLICK Electronics Pack (1 cassette tape, 1 GameBoy, and 1 Mac G3) along with the physical items.**\n\nAuction Opens: Monday 8/23 at 1:30PM EST\n\n# Links\n\n**<http://starpower.digital/>**\n\n**<https://selah.ffm.to/starpower>**\n\n**<https://www.vogue.com/article/selah-marley-star-power-ep-interview>**\n\n**<https://www.instagram.com/selah>**\n\n**<https://www.bitski.com/starpower>**", "timestamp": 1629710589, "digest": "Y-vmz2ihDf_AB3dEXhk8XEymx_ygbfN25g2szX5XHeQ", "contributor": "0xDb6c1a8aF1883262aaD221A25816468ef693D4A2"}
{"id": "B7Z3moGd8f7dOK7fiN3nXT6ZW66K59eynEYorl87Ms8", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711591, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "kD4PfjP40S7-X0nWd1i3WryZcQk9eQROd2WsbuD-VQY", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711500, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "Ht9m-Gm_8Ugzy1zn__dFQb8a7FUXfkNj2YAuFJraeUM", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711629, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "6jLQONap-0RWz7x0_PiHLoI5PMAE9Oh-pqgdQUGHeRo", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711699, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "fX32nq350Lqprp5r_9Pe0dBXInJEEx1PTtKKfd0r5wM", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711666, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "I5_ASP0cKgC-Dpf1-Q0u8pfTymcWlvYTben-dMwWAC0", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711674, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "6wJlGLiptEF2nQjXaScmrat7_0VLWP4zXxrMZNPYkSg", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711784, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "jn1DFurHq3YvOCquC_pFIn72XRwYvuwSf-iL-Uv7Q6k", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711749, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "pcUJe6kCsl5AM5BCI7ua54CHz0lsCmJJNoSVKMdZqAA", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”. \n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea. \n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection. \n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility. \n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional. \n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list. \n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose. \n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet. \n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it. \n* Upload the file you’d like to NFT. \n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more. \n  * The maximum file size supported is 40 MB. \n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is. \n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle. \n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply. \n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies. \n* Click on Create. \n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing. \n\nI created two NFTs with this article. One on Ethereum and the other on Polygon. \n\nIf you like this post, share this on Twitter with your wallet address and tag [@bvajresh](https://www.twitter.com/bvajresh) and [@rabbithole_gg](https://twitter.com/rabbithole_gg) and I will send you the NFT on Polygon for free. Supply is limited so act fast!", "timestamp": 1629711756, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "ky_6Nj6PqRbcqryphRDk3FMleId1LFiSvhpjO1ey4hg", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”.\n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea.\n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection.\n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility.\n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional.\n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list.\n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose.\n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet.\n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it.\n* Upload the file you’d like to NFT.\n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more.\n  * The maximum file size supported is 40 MB.\n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is.\n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle.\n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply.\n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies.\n* Click on Create.\n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing.\n\nCongratulations! You have now minted an NFT on OpenSea!", "timestamp": 1629712008, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "ag1GYqQTIZ70OHPzDGRyji2t8Elp9NG2O6UjXg1J4H8", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”.\n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea.\n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection.\n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility.\n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional.\n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list.\n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose.\n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet.\n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it.\n* Upload the file you’d like to NFT.\n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more.\n  * The maximum file size supported is 40 MB.\n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is.\n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle.\n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply.\n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies.\n* Click on Create.\n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing.\n\nCongratulations! You have now minted an NFT on OpenSea!", "timestamp": 1629713981, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "1S7vrvbAPJEmNvFWSWDhpf11nHv5B9MHfjzqy-i1y30", "title": "Test", "body": "dfasdfsdf\n\n![](https://images.mirror-media.xyz/publication-images/9182d430-faa4-4855-aa5e-8b3dad3fee8c.gif?height=1700&width=1700&size=large)\n\nasdffs", "timestamp": 1629727684, "digest": "CpMi4rdX4x1qABlUGB4dQP8KPAWW307tslpwXGLToog", "contributor": "0x5FE6C3F8d12D5Ad1480F6DC01D8c7864Aa58C523"}
{"id": "XnxJbvlFFeoGeIgvNYClrIhFqE9dkx9S6p70mcgmgIs", "title": "VC Investor Relations", "body": "I realized a long time ago that the VC’s customer is the founder/CEO/portfolio company and that our investors (called LPs in VC speak) are our “shareholders”. That was a very defining moment for me and has clarified what matters the most in a VC firm.\n\nThat said, we take investor relations very seriously at USV and always have.\n\nThis is our model:\n\n1/ We are loyal to our LPs and offer them the opportunity to invest with us fund after fund after fund unless something has materially altered the relationship. That is very rare but has happened.\n\n2/ We regularly provide our LPs with a lot of information on our portfolio. We send financial reports including detailed schedules of investments quarterly and we provide detailed one-page writeups on each and every portfolio company twice a year.\n\n3/ We do two “quarterly calls” a year, one in the spring to review Q4 and Q1 and one in the summer to review Q2. These are now Zoom meetings. We are approaching our summer call which is what prompted me to write about this today.\n\n4/ We do one annual meeting in the fall after Q3 results are out. These used to be in-person meetings in our office featuring several (3-5) presentations from a representative mix of portfolio CEOs. We like to have a wide variety of companies present (by stage, performance, etc) and absolutely do not do a “greatest hits” experience at these meetings. We did our annual meeting over Zoom last year and may continue to do that going forward as it makes it much easier for the portfolio CEOs to present and easier for our LPs to attend. If we do that, I will miss the in-person interaction we have at our annual meeting but also believe making things easier for everyone is very important.\n\n5/ We don’t do splashy meetings at fancy places with our LPs. We believe in substance over form when it comes to investor relations and we believe that our LPs do as well.\n\nThe Gotham Gal and I are investors in dozens of VC funds/firms and there are many ways that VCs do this. Some provide little to no information and let the returns speak for themselves. That can work too. But I believe frequency, regularity, and transparency are the key factors to focus on with investors. It has worked well for us.\n", "timestamp": 1629727694, "digest": "BQQb1unzfR_4PunmcPtOMOekl7QQu9B08EI1SXFdS5s", "contributor": "0xB8332710AB60A1B2e8FE6D775d83524C9E0cE136"}
{"id": "CZ1C9zcVcm3T0pYbzkl0OXSLmbyMW2jDpVsgeRxVpX0", "title": "The Weekly Hop #2: Experimenting Towards Decentralization", "body": "***The Weekly Hop is a newsletter from the leaders behind RabbitHole, a platform onboarding the next million users into crypto while training and connecting them to work opportunities in DAOs.***\n\nRabbitHole is on a mission to be the University of the Metaverse. This mission includes onboarding new people into crypto, teaching them how to be crypto-native, credentialing crypto users for their on-chain tasks, and matching users to protocols to find the best opportunities to participate in and contribute to crypto networks.\n\nBut the truth is, this is not a mission that should be controlled by any single individual or entity. The Pathfinder program is the first step in decentralizing RabbitHole by recruiting the best community members and empowering them to look after the design, creation, and implementation of RabbitHole Quests. \n\nEventually, the Pathfinder program will become the RabbitHole DAO. In today’s edition of the weekly hop, we will talk about the strategy behind decentralizing through the Pathfinder program and the experiments we are launching to progress down the path of decentralization. \n\n### Decentralizing through atomic units of work\n\nThe Pathfinder program is a strategic way for RabbitHole to decentralize. We broke down the creation of our core offering (Quests) into the following atomic units of work:\n\n* **Content creation** (Pioneers, currently live)\n  * Writing tutorials for the Tasks\n* **NFT design & minting** (Artisan, September launch)\n  * Creating the NFT reward for completing a Quest\n* **Quest proposals** (Stewards, October launch)\n  * Seeking funding and creating Tasks\n* **Subgraph development** (Navigators, November launch)\n  * Finding on-chain data to verify Task completion\n\nWhereas many DAOs start by involving people in nebulous, somewhat peripheral tasks (e.g., community moderation), the Pathfinders are driving RabbitHole’s core business through discrete bounties.\n\nBy distributing the responsibilities for Quest creation, RabbitHole is moving towards a healthy transformation into a DAO and allows Pathfinders to make meaningful contributions with little involvement from the core team. \n\n### Experimenting along the way\n\nAfter \\~4 weeks of running the Pathfinder program, we identified significant areas for improvement. This experimental approach is built into the Pathfinder culture. For example, instead of “paying” Pioneers for bounties, we purchase NFTs that they mint on Mirror.\n\nStarting with a small targeted group of focused community members has enabled us to learn and iterate quickly while pushing the envelope on what is imaginable as we grow into a DAO. As we progress, the Pathfinders will be driving more and more of these improvements themselves.\n\n***The First Experiment - The Pioneer Budget***\n\nUp until now, the RabbitHole core team has funded bounties which determined which tutorials the Pioneers created, but moving forward we will try a different approach.\n\nStarting in September, we are going to be experimenting with a flat budget model. Instead of creating a list of bounties and paying on a per bounty basis, we are going to provide the Pioneers with a budget that they manage themselves. The core team will request specific tutorials for upcoming Quests, but beyond that, the budget will be used at the discretion of the Pioneer Treasury Workstream. \n\nWe’re giving the Pioneers a more open directive to explore web3.\n\nThis helps achieve a few things:\n\n* Pioneers can take more ownership over the work they are doing.\n* The number of bounties completed will theoretically increase because they are no longer determined by RabbitHole, but by the needs of the community and the skills and insights of the Pioneers\n* It provides a good experiment for a future DAO by introducing more novel reward mechanisms, similar to [Coordinape](https://coordinape.com/), where community members who work with one another determine compensation in a decentralized way.\n\n### Another prolific week\n\nWe had another week with great output from the Pioneers, who may be leading the most active Mirror publication at the moment. The group created and published the following Tutorials in the last week:\n\n* [LP on Balancer](https://rabbithole.mirror.xyz/_MqYw-kl3-KlGnxOtgXNPr9Be8MD1Bsn4zGotMvxQQQ) (Curly)\n* [Swap on Balancer](https://rabbithole.mirror.xyz/fwDeEvyMeTzuqQ8Kz1jv34tgbsBEKqfP3ZehkjWYUIg) (Curly + Chase)\n* [LP on Uniswap Optimism](https://rabbithole.mirror.xyz/s6iIQ4gRcsD0VygJiHgH8srRf-_ld1RZdNDe3Ts8XCU) (Alex)\n* [Set-up Polygon network](https://rabbithole.mirror.xyz/Hm0zb1vkvx0yoj9EfvqCt119vsMdcD0Cp9Sk25hyNic) (Vajresh)\n* [Mint alETH on Alchemix](https://rabbithole.mirror.xyz/1DawsxSdv3GXVzcLhAd_GxV22EF4FZettQOtSIQhMR8) (Zach)\n* [Set-up Optimism network](https://rabbithole.mirror.xyz/u4qNFL7G5lmhEI7txECyFcsbfQi4rHGHPGdnBNI73pE) (Anay)\n* [How to Purchase on OpenSea](https://rabbithole.mirror.xyz/6UF5AgtcoSBgHvJR_-ZvAyBEJiiCEINP1SEXsRpe2NM) (Jonah)\n\n### Do you want to help create the University of the Metaverse?\n\nIf you want to help bring RabbitHole’s vision to reality and be a part of the future DAO, please reach out to @[chaserchapman](https://twitter.com/chaserchapman) and @[flynnjamm](https://twitter.com/Flynnjamm). \n\nSee you next week for the Weekly Hop!\n\n[split://0x8c7D35aC356883E60e4bb55Aa6adA748b0d34309?network=mainnet](split://0x8c7D35aC356883E60e4bb55Aa6adA748b0d34309?network=mainnet)\n\n*Tips will be split amongst Pioneers that contributed to this post (45%) and myself (55%). Any tips I personally receive will be used to purchase future Pioneer Editions, so all tips are* directly funding crypto user onboarding and education!", "timestamp": 1629737855, "digest": "2aWOCnFgLTRePfJn4C2P8RgmYh_I0VbPcColjQTtcJ8", "contributor": "0xaAba999423e939a67373E422dFabE088d5bc31A5"}
{"id": "D-EtransOR4T6RENIRMWDPb5fgPf0HJz4XTQbxyDosM", "title": "Sparking the Dirt NFT Market", "body": "A few months ago, we sold NFT editions to raise around $30,000 to fund the newsletter Dirt, the first newsletter run on NFTs. We then used that money for a successful two-month-long season of content, paying writers and editors a great rate for great newsletters. During that time, the subscribers grew almost 50% and Dirt was shared by influential media figures and publications. Plus, we have over 100 holders for our 131 original NFTs. \n\nToday is the first step beyond that initial launch. We’ve now created the Dirty S1 Holo, a combination of our sold-out Pea Green and Pearl Pink NFT editions. The Holo is an edition of 25, but there are two ways to get it: If you have two or more of our pre-existing NFTs, you’ll get it airdropped for free. Or, you can buy it for 0.3 ETH here on Mirror. Holo owners will also get an extra 50 $DIRT-S1 tokens.\n\nIf you have two Dirt NFTs already, we got you; if you want to buy Dirt NFTs check OpenSea here. When you get two, email / tweet us at newsletterdirt@gmail.com or @dirtyxyz. The first 25 holders will get the Holo. \n\nBuy Dirty S1 Holo:\n\n\\[EMBED\\]\n\nPost-Holo Plans \n\nYou can read more about the Holo launch in our newsletter, or continue below for the business-y details. \n\nWhy do you want the Holo? Well, it’s cool, obviously. It looks rare, as they say — our rarest Dirt NFT besides the 1/1 Rainbow Wave. But it plays into our longer experiment to run a media company using NFTs. By launching Holo, we’re stimulating the secondary market in Dirt NFTs, letting early holders cash out (if they want) and bringing in new users. Like all good NFT projects, we just launched a Discord; we want an active community of buying and selling NFTs. \n\n ", "timestamp": 1629754311, "digest": "TQ9QIlyx2kEhcehnmRzRTmS3XmaiCqKF-7vKipWM21Q", "contributor": "0x25cbCFA16e43e58517f80D583087Ea89241293E6"}
{"id": "HjvFj-tOR6lflLHIB2U60fB0thoAEQGasMKy36qPcqU", "title": "UX Research to Improve NFT Auction Experience, using Mempool Data", "body": "User experience (UX) describes how people feel when they interact with a system or service and encompasses several factors including usability, design, marketing, accessibility, performance, comfort, and utility. Don Norman once said,\n\n> “Everything has a personality; everything sends an emotional signal. Even where this was not the intention of the designer, the people who view the website infer personalities and experience emotions. Bad websites have horrible personalities and instill horrid emotional states in their users, usually unwittingly. We need to design things — products, websites, services — to convey whatever personality and emotions are desired.”\n\nEthereum's personality is someone who is extremely inscrutable and easily misunderstood. To make matters worse, most users don't even think about it as interacting with Ethereum when they're using your interface or a wallet. If you're ever in the live chat of an Artblock's auction, you'll notice as soon as an auction ends there are at least a dozen people complaining that it is Metamask's fault that they didn't get a mint. I think in the last year the UX of many dapps on Ethereum has improved greatly in both product interaction and explainability of transactions. For the most part, dapps don't just leave you with a loading spinner after you sign the transaction anymore.\n\nEven with the design of dapps is generally improving, I'm not sure how deep UX research has gone yet. When I see data analytics or research on various protocols, users are mostly treated as homogenous. This is somewhat shifting with some of the analysis I've seen of Uniswap V3 liquidity providers and Rabbithole Questers, but even those are still heavily focused on just confirmed transactions on-chain. From my own experience, most of the emotion evoked and behavioral quirks happen when I'm submitting, waiting, speeding up, or canceling my transaction. For some applications, the user might leave and go do something else after submitting a transaction. But for a product like Artblock's auctions, they're going to stay around until the confirmation has happened, likely checking anything they can for updates and with compounding anxiety.\n\nI think we can do a much better job of understanding user behaviors and frictions by starting to leverage the mempool more. The [mempool](https://compassmining.io/education/what-is-a-mempool/) is where unconfirmed transactions are temporarily stored by a node. This means if you submit, speed up, or cancel your transaction then those actions will show up in the mempool first. It's important to note that the data from the mempool is not stored in the node, so you can't query historical data the same way you could on confirmed transactions. For example, you could observe that someone submitted a few transactions, sped up the transactions quite a few times at the same gas price, and saw confirmations 20 blocks later. I believe studying their behavior throughout the transaction lifecycle is a pretty good proxy for user experience and likely emotions they were feeling throughout the whole process. If we understand how different groups of users behave in this cycle, we can figure out how to supplement their decision-making or ease their anxiety. To my knowledge, pretty much only the Ethereum Foundation, All Core Devs, and some wallet teams leverage the mempool data for UX reasons.\n\n**UX Research Thesis:** By looking at a user's behavior through auctions over time and also their wallet history, we can start to give behavioral identities to different groups of users. From here, we can identify the main issues to try and alleviate. We'll do this by taking a month of Artblocks Auctions data using [Blocknative](https://www.blocknative.com/), and layering in the history of these addresses using [Dune queries](https://dune.xyz/).\n\nThis article will be more technical than some of my previous ones since I believe the work can and should be generalized fairly easily. *I want to emphasize that my background is not in UX research, I'm purely experimenting with what I think crypto-native UX research could look like.*\n\n## Data Sourcing and Preprocessing All Auction Data\n\n*If you have no interest in the technical bits, skip to the next section on Feature Engineering*\n\n### Blocknative and Mempool Data Streaming\n\nUsing Blocknative's Mempool explorer, you can filter for transactions submitted to specific contracts or from specific wallets. In my case, I wanted to listen to the whitelisted minter contracts for Artblock's NFT contract. You can find the stream that I used [here](https://tinyurl.com/yftus9h2), and save it down if you want to use the exact same setup.\n\nYou can find the whitelisted minter addresses with the following query [in their subgraph](https://thegraph.com/legacy-explorer/subgraph/artblocks/art-blocks):\n\n```python\n{\n  contracts(first: 2) {\n    id\n    mintWhitelisted\n  }\n}\n```\n\nThere were three steps to get to a subscription filter for all purchases:\n\n1. Add the new address with the \"create new subscription\" button\n2. Add the ABIs by clicking the \"ABI\" button next to the address. In my case I just needed the \"purchase\" function.\n\n```python\n{\n    \"inputs\": [\n      {\n        \"internalType\": \"uint256\",\n        \"name\": \"_projectId\",\n        \"type\": \"uint256\"\n      }\n    ],\n    \"name\": \"purchase\",\n    \"outputs\": [\n      {\n        \"internalType\": \"uint256\",\n        \"name\": \"_tokenId\",\n        \"type\": \"uint256\"\n      }\n    ],\n    \"stateMutability\": \"payable\",\n    \"type\": \"function\"\n  }\n```\n\n1. Add filters for the `methodName` matches `purchase` (make sure you don't do a global filter)\n\nIn the end, your setup should look like this:\n\n![](https://images.mirror-media.xyz/publication-images/ad1f283a-7ce1-4899-9fa8-bdfda6a716e1.png?height=577&width=844)\n\nTo store this data down, I created a ngrok/express endpoint to store in an SQLite database, run locally. I've created a [GitHub template with steps to replicate this setup](https://github.com/andrewhong5297/blocknative_stream_to_sql). Probably the most important point to remember here is that you need to include the POST endpoint as part of the ngrok URL when adding it as a webhook in the Blocknative account page.\n\n### Key Preprocessing Functions\n\n**Multiple Transaction Hashes**\n\nWhen you speed up or cancel a transaction, the original transaction hash is replaced with the new transaction. This means if you want to track a users' transaction through its full lifecycle, you need to reconcile the new transaction hashes with the original one. Assuming you speed up a transaction five times, you'll have six hashes total (the original hash + five new hashes). I reconciled this by getting a dictionary mapping of `tx_hash` to the new `replaceHash`, and then replaced recursively.\n\n```python\nreplaceHashKeys = dict(zip(auctions[\"replaceHash\"],auctions[\"tx_hash\"])) #assign tx_hash based on replacements, just to keep consistency. \nreplaceHashKeys.pop(\"none\") #remove none key\n\ndef recursive_tx_search(key):\n    if key in replaceHashKeys:\n        return recursive_tx_search(replaceHashKeys[key])\n    else:\n        return key\n\nauctions[\"tx_hash\"] = auctions[\"tx_hash\"].apply(lambda x: recursive_tx_search(x))\n```\n\n**Blocknumber Issues**\n\nDropped transactions have a `blocknumber` of 0, so to deal with this I sorted my dataframe by `timestamp` in ascending order, and then did a backward fill so the 0 would be replaced by the correct `blocknumber` it was dropped in. This is important fix for feature engineering.\n\n```python\nauctions = auctions.sort_values(by=\"timestamp\",ascending=True)\nauctions[\"blocknumber\"] = auctions[\"blocknumber\"].replace(to_replace=0, method='bfill') #deal with dropped txs that show as blocknumber 0\n```\n\n**Dealing with Mints Outside of Main Auction Period**\n\nFor most projects, the artist will mint a few pieces before the auction opens up to the public. Some projects don't sell out right away, so you will get mints still occurring a few days after the auction has opened. My analysis is focused on the key auction period, mostly the first 30 minutes. To get rid of the two mint cases above, I removed outliers based on `blocknumber`.\n\n```python\nto_remove_indicies = []\nfor project in list(set(auctions[\"projectId\"])):\n    auction_spec = auctions[auctions[\"projectId\"]==project]\n    all_times = pd.Series(list(set(auction_spec.blocknumber)))\n    to_remove_blocktimes = all_times[(np.abs(stats.zscore(all_times)) > 2.5)]\n    if len(to_remove_blocktimes)==0:\n        break\n    to_remove_indicies.extend(auction_spec.index[auction_spec['blocknumber'].isin(to_remove_blocktimes)].tolist())\nauctions.drop(index=to_remove_indicies, inplace=True)\n```\n\n**Adding on Dutch Auction Prices**\n\nFor all projects in the dataset besides project 118, a Dutch auction price format was used. I took the mint price data using a [dune query](https://dune.xyz/queries/113834), and then merged it onto the dataset. I had to use a forward and backward fill for the blocks that had mempool actions but no confirmations during the auction.\n\n```python\nauction_prices = pd.read_csv(r'artblock_auctions_analytics/datasets/dune_auction_prices.csv', index_col=0)\nauctions = pd.merge(auctions,auction_prices, how=\"left\", left_on=[\"projectId\",\"blocknumber\"],right_on=[\"projectId\",\"blocknumber\"])\nauctions.sort_values(by=[\"projectId\",\"blocknumber\"], ascending=True, inplace=True)\nauctions[\"price_eth\"].fillna(method=\"ffill\", inplace=True)\nauctions[\"price_eth\"].fillna(method=\"bfill\", inplace=True)\n```\n\n## Feature Engineering For Each Auction\n\n*If you have no interest in the technical bits, just read the parts in bold and skip the rest.*\n\nIn data science, a feature is a variable that is calculated from the larger dataset to be used as an input in some sort of model or algorithm. All features are calculated in the     `preprocess_auction` function and are calculated *per auction rather than combining all the auctions into a feature set.*\n\n**The first set of features are the totals for transaction states**, and was a simple `pivot_table` function:\n\n* `number_submitted` : total number of transactions submitted\n* `cancel` : count of transactions that ended in canceled\n* `failed`: count of transactions that ended in failed\n* `dropped`: count of transactions that ended in dropped\n* `confirmed`: count of transactions that ended in confirmed\n\nI mentioned earlier some data was not captured for auctions due to various issues, these transactions were dropped from the dataset.\n\n**The next set of features included their gas behavior.**         The key concept here was capturing how far away their transaction gas was from the average confirmed gas per block (shifted by 1 block). **Then we can create features for the average, median, and standard deviation of gas price distance over the whole auction.** There are a bunch of transposes and index resets to get the `blocknumber` columns in the right order, but the important function is `fill_pending_values_gas` which forward fills the gas price between actions captured. This means that if I put in a transaction at `blocknumber` 1000 with a gas of 0.05 ETH and my next action wasn't until `blocknumber` 1005 where I sped up to 0.1 ETH gas, then this function will fill in the blocks between 1000-1005 with 0.05 ETH.\n\n```python\ndef fill_pending_values_gas(x):\n    first = x.first_valid_index()\n    last = x.last_valid_index()\n    x.loc[first:last] = x.loc[first:last].fillna(method=\"ffill\")\n    return x\n```\n\n**The third set of features were calculating the total number and frequency of actions taken in the auction.**     Here we start with a pivot of total actions (speed ups) per block, with some special calculations for getting the first instance of pending for each transaction:\n\n```python\nget_first_pending = df[df[\"status\"]==\"pending\"] #first submitted \nget_first_pending = get_first_pending.drop_duplicates(subset=[\"tx_hash\",\"status\"], keep=\"first\")\nauctions_time_data = pd.concat([get_first_pending,df[df[\"status\"]==\"speedup\"]], axis=0)\ntime_action = auctions_time_data.pivot_table(index=[\"sender\",\"tx_hash\"], columns=\"blocknumber\",values=\"status\",aggfunc=\"count\") \\\n                    .reindex(set(df[\"blocknumber\"]), axis=1, fill_value=np.nan)\n```\n\nFrom here we get to `average_action_delay` in three steps:\n\n1. we take the number of actions per block (yes some people sped up transactions multiple times in the same block)\n2. we drop the blocks with no actions, and then take the difference between the blocknumbers that remain. We add a 0 for each additional action taken per block.\n3. Taking the mean across the differences and added zeroes gives us the `average_action_delay`\n\n```python\ndef get_actions_diff(row):\n    row = row.dropna().reset_index()\n    actions_diff_nominal =list(row[\"blocknumber\"].diff(1).fillna(0))\n \n    #take the blocks with muliple actions and subtract one, then sum up. \n    zeros_to_add = sum([ actions - 1 if actions > 1 else 0 for actions in row[row.columns[1]]])\n    actions_diff_nominal.extend(list(np.zeros(int(zeros_to_add))))\n    actions_diff = np.mean(actions_diff_nominal)\n    if (actions_diff==0) and (zeros_to_add==0):\n        return 2000 #meaning they never took another action\n    else:\n        return actions_diff\n```\n\n`total_actions` is much simpler, as it is just the sum of actions across the pivot.\n\n```python\ntime_action[\"total_actions\"] = time_action.iloc[:,:-1].sum(axis=1)\n```\n\n**The last time-dependent feature is** `block_entry`, which is an important one due to the introduction of Dutch auctions. Essentially this tracks what block the transaction was submitted on since the start.\n\n```python\nget_first_pending[\"block_entry\"] =   get_first_pending[\"blocknumber\"] - get_first_pending[\"blocknumber\"].min()\n\nentry_pivot = get_first_pending.pivot_table(index=\"sender\",values=\"block_entry\",aggfunc=\"min\")\n```\n\n`price_eth` is added as a feature as well, which is tied to the `block_entry` point.\n\n**The last set of features were based on a Dune query, specifically the days since the first transaction, total gas used in transactions, and the total number of transactions.**     To get the address array in the right format I used the following line of code after reading in the SQL data:\n\n```python\nall_users = list(set(auctions[\"sender\"].apply(lambda x: x.replace('0x','\\\\x'))))\nall_users_string = \"('\" + \"'),('\".join(all_users) + \"')\"\n```\n\nThe dune query for this is fairly simple. I pasted the addresses string under `VALUES`, and made some CTEs to get the features I wanted. In the final `SELECT` I tried to add each address's ens as well. You can find the query here: <https://dune.xyz/queries/96523>\n\nLastly, we just merge in the per wallet data on days active, total gas used, and the total number of transactions.\n\n```python\nauctions_all_df = pd.merge(auctions_all_df,wh,on=\"sender\",how=\"left\")\nauctions_all_df.set_index([\"sender\",\"ens\"],inplace=True)\n```\n\n*With all this done, we're finally ready to run some fun unsupervised learning algorithms and try and validate our hypothesis on user groups.*\n\n## Clustering and Visualizing User Groups\n\nBefore I started this project, I had expected to see the following user groups pop out of the data:\n\n* **Set and Forget:** There should be two groups here, those who set a transaction with really high gas and an average/low gas and then don't touch it for the rest of the auction.\n* **Speed Up:** There should be two groups here as well, those who are speeding up often and updating the transaction directly as a factor of gas prices and those who are speeding up often but with basically no change in gas price.\n\nI was very interested in validating these groups, seeing how large each group was, and seeing if any users moved between groups over the course of many auctions. The easiest way to do this was to use unsupervised machine learning to identify clusters of user groups based on the variability across all features. Essentially this is like looking at the distribution of income in a state and then splitting it into sub-distributions for different income concentrations, geographic coordinates, and age. Note that this isn't binning, where the distribution is split into equal ranges - it is instead calculated based on the density of observations within the whole range. The approach we will take is called \"unsupervised\" because our dataset doesn't have any existing labels, rather than something like a regression where there is a value being predicted that can be verified as right or wrong.\n\nThe algorithm I decided to use is called [k-means](https://stanford.edu/\\~cpiech/cs221/handouts/kmeans.html), where k stands for the number of clusters you are expecting to identify. Each cluster has a \"centroid\", which is like the center of a circle. There are various methods for figuring out how many clusters are optimal, and the two I used were elbow points and silhouette scores. Those are both fancy ways of asking,\n\n> \"does each additional cluster help increase the density of the cluster (calculated as the average distance of points in a cluster from the centroid) and maintain adequate separation between clusters (no overlap between two clusters)?\"\n\nI found that 3 clusters were optimal in terms of most inertia improvement while keeping a high silhouette score (around 0.55).\n\n![6 auctions were used for this analysis](https://images.mirror-media.xyz/publication-images/7bef376a-b96c-42de-87b6-991db4503ef9.png?height=264&width=393)\n\nWith clusters chosen, we want to be able to visualize and verify their existence. There are over 15 variables, so we need to reduce the number of dimensions in order to plot it. Dimension reduction typically relies on either PCA or t-SNE algorithms, in our case I went with t-SNE. Don't worry too much about understanding this part, these algorithms essentially capture the variance across all features to give us X and Y components that maximize the spread of points from each other.\n\n**Let's look at project 118, LeWitt Generator Generator, from August 4th:**\n\n![](https://images.mirror-media.xyz/publication-images/4f7dad88-47b6-4dde-9a52-f0e4258cb829.png?height=605&width=615)\n\n![These are the sub-distributions for each variable by cluster, calculated using a KDE. The colors match those in the above clusters.](https://images.mirror-media.xyz/publication-images/5bafdebf-7b9b-43b3-8ed0-7e4ac923725f.png?height=856&width=1123)\n\nAfter taking a look at the sub-distributions for each variable and some data examples, I was able to categorize the clusters. The Orange cluster is the group that is speeding up the most while also submitting slightly lower gas transactions on average. Blue and Green clusters exhibit similar behavior to each other, but addresses in Blue typically have less history than addresses in the Green cluster.\n\nLooking at the overall picture, it seems the original hypothesis on \"speed-up\" and \"set high and set low\" producing two groups each was wrong. Instead, we have a single \"speed-up\" group (Orange) and a single \"set-and-forget\" group (Blue and Green are behaviorally the same). I think the new (Blue) versus old wallets (Green) in the \"set and forget\" group probably carries a lot of overlap in actual users, where users just created new wallets to bid on more mints. Based on their impatience in `average_action_delay` and lower-than-average `average_gas_behavior`, the \"speed-up\" group reads to me as less experienced and more anxious than other users. *What did surprise me is that the speed-up group makes up a smaller proportion (30%) of total bidders, as I had expected that group to make up 60-70% of bidders instead.*\n\n**Now, where this user behavior study gets really interesting is when comparing project 118 (the set price of 0.75 ETH) to project 140 (Dutch auction with the price decreasing from 1.559 to .159 ETH).**\n\n**Here's the clustering for project 140, Good Vibrations from August 21st:**\n\n![](https://images.mirror-media.xyz/publication-images/ae090b2e-141c-4dd0-a0cb-06f4ed45dd13.png?height=605&width=615)\n\n![](https://images.mirror-media.xyz/publication-images/7294dd21-59e5-48fd-b7a1-7babbda77040.png?height=856&width=1130)\n\nWe can see that now most of the clustering variability comes from `block_entry`, `price_eth`, and all of the `gas_behavior` features. This is a big departure from the main variables for project 118. In 118, the set price meant that people entered the auction in a fairly uniform distribution (quantity left didn't seem to matter), and the \"speed-up\" group submitted actions up fairly endlessly.\n\nIn project 140, we don't see the same difference in actions within `average_action_delay` or `total_actions` , instead, we see a new group (Orange) entering at a very late stage block and setting far below-average gas prices as seen in `average_gas_behavior`. If I was to try and map this to the clusters in 118, I believe the \"speed-up\" group has now become the \"greedy\" group (Orange) that is entering late and bidding a low gas. The Green cluster probably represents users with more experience than the Orange cluster, but who are still transitioning between Orange and Blue in terms of their behavior. The \"set-and-forget\" group maps pretty well to the \"early-grab\" group (Green and Blue) since they all exhibit pretty good patience and an adequate safety net on gas bids.\n\n**I call the Orange group \"greedy\" not just because of their behavior, but also because of their rate of failed transactions.**\n\nFor project 118, fail rates are of the \"speed-up\" versus \"set-and-forget\" groups are within 10-15%.\n\n![percent_lost takes (cancel + dropped + failed) / number_submitted](https://images.mirror-media.xyz/publication-images/306752cb-87c2-4494-8915-86102c214d29.png?height=77&width=724)\n\nFor project 140, the fail rate of the \"greedy\" cluster is around 69% versus the \"early-grab\" group at around 5-15%.\n\n![](https://images.mirror-media.xyz/publication-images/55ceaf8d-6756-4807-beb9-fcfbbe4e549e.png?height=84&width=727)\n\n**Overall, my read of this is that the group's bad habits and emotions were amplified - it feels to me like we made a tradeoff in anxiety → greed. This may have made the auction less stressful, but ultimately led to more users being upset (due to failed mints).**\n\n*I'm sure there's a more granular analysis that can be done to segment the auctions further based on factory/curated/playground or by artists themselves too. This will only get more interesting and complex as the community continues to grow, and emotions play a larger factor in both a single auction and on if they return for future auctions.*\n\n*This study of multiple auctions helped us validate our assumptions, understand the proportions of user groups, and see how users' good or bad behaviors shift over time (and other parameters). Now we need to plug it into the rest of the product cycle process.*\n\n## Where We Go From Here:\n\nThe reason that I chose just Artblocks auctions for this and not a mix of platforms is because I wanted to look for a place where the variability in terms of interface and project type is mostly controlled. This should have given us fairly consistent users and behavior types.\n\nThis is just the start of a UX research cycle, so ideally we could continue in the following steps:\n\n1. Use an unsupervised machine learning algorithm to identify the user groups (clusters) and see how many people are making \"mistakes\" when entering an auction. *This is the step we covered today.*\n2. Create a new user interface, such as a [histogram view on the bidding screen](https://twitter.com/andrewhong5297/status/1423020670825504768), or showing historical data on when most people usually enter/crowd the auction and at what prices. Anything to give both current and historical context to the user, especially those from the speed cluster.\n3. With each auction, run the mempool/wallet data through the created algorithm to see if the user groups have shifted and if specific users have \"learned\" to participate in the auction differently (i.e. did they move between user groups). *I think that the most value can be found in this step if done well. Using ENS or other identifiers to help supplement this grouping would be exponentially helpful too*\n4. Based on the results, continue to iterate on the user interface and design. You could also run more informed A/B testing since you could even figure out what screens to show by making an educated guess based on the users' last cluster (or use label propagation for new users).\n\n**The Dutch auction-style change is also an example of step #2, and we were able to see a clear shift in user behaviors.** While typically this sort of research and testing is focused on increasing engagement or conversions, here we are optimizing for the user's ability to learn and improve instead. This may become even more robust if this was iterated in a multiplatform context, so that we can study how someone is learning at an ecosystem level (maybe even supplement with Rabbithole data and user profiles). Since my Artblocks user research is all based on publicly sourced data, it can be replicated and supplemented by any other auction/sale platform. **Crypto could be the first industry that would have a synchronized and transparent set of user groups and UX research, to be applied in products and academia.** Nansen wallet labeling is already a step towards this, but it's different when teams from different products build this up from various facets and approaches.\n\nWhat I would eventually envision is using data to build on the following user personas (with subgroups/levels within them):\n\n* I want to buy a Fidenza, so I can either buy one through a private sale, bid on one in an auction myself, bid on one in a prtyDAO bid auction, or buy a fraction of one with [fractional.art](https://fractional.art/)\n* I like Fidenzas in general, so I'll just buy the NFTX Fidenza index token or an NFT basket of Artblocks Curated on [fractional.art](https://fractional.art/)\n* I'm a collector already, so I want to swap or bid on a Fidenza using a curated set of NFTs and ERC20s I already hold (using genie-xyz swap).\n* I like the rush of acquiring through initial mint versus secondary market, and heavily participate in auctions like Artblocks live mints.\n\nI hope you found this project interesting and/or helpful, I had a ton of fun with it. Thanks to the folks at Blocknative for setting me up, and the community at Artblocks for answering my many auction questions. As always, feel free to reach out with any questions or ideas you may have!\n\nYou can find the GitHub repo [with all the data and script here](https://github.com/andrewhong5297/artblock_auctions_analysis). The script may be a bit hard to read since I’m still refactoring and cleaning it up. The script and some of the analysis here may get updated as I analyze the last few auctions of August for new patterns.", "timestamp": 1629754335, "digest": "l_-4fQ08cpxUZpn9V9S5R27wfKvNgdnrXlZAWZWvdlg", "contributor": "0x2Ae8c972fB2E6c00ddED8986E2dc672ED190DA06"}
{"id": "NjvYBGlvSqFhuTlGTBZ0BpY4jsq19hLVTd69bBZyzng", "title": "Finding The Adjacent Possible", "body": "With my eyes closed and mind wide open I listened as our lecturer unraveled a journey of innovation, economic forces, and human evolution.\n\nHe summarized thousands of years of complex dynamics with a single, simple equation - **The Theory of Adjacent Possible**, stating that dynamical systems making incremental changes over time explode into an exponential realm of possibilities, and within, a profound order that governs evolution.\n\nA spark of curiosity ignited within me on that cold January day and so did a burning question: **how can I apply the principles of complex systems to understand dynamics of collective decisions, self-organization, and the adjacent possible within teams, organizations, and society in today’s world?**\n\nThat lecturer was Stuart Kauffman, theoretical biologist and expert in self-organization of complex systems, and I was at the [New England Complex Systems Institute (NECSI)](http://www.necsi.edu) for the 2019 Winter School at MIT to learn about complex systems science, research, and computer modeling. I was so moved by the ideas he put forth that I reached out to Stu to learn more and try to understand my place within this mesh of interests, ideas, and intuition I was developing. He encouraged me to pursue the PhD I was interested in, and invited me for coffee at his home in Santa Fe to talk and share ideas. His succinct advice for me, the intellectual wanderer: \"**take your curiosity seriously.**\"\n\n## Where I go forth\n\nThis was the genesis of my own personal adjacent possible. A path of curiosity unwinding forward, without knowing the direction, but being guided by wonder and awe for how we work together, and what we can accomplish when we are free and without bounds.\n\nThat path has led me to the Cambrian explosion of Web3, with an adjacent possible of DAOs, new communities, and tools to express ourselves churning open as we build. The systems are being created, and I am developing the methods to understand and shape them. Mirror will be my space to create, share, and make connections within this sea of evolution.\n\nI’ll be writing about DAOs, PhD research, governance and control of complex systems, emergence, and self-organization. I’ll also tap into the eros with my trove of [Processing](http://www.processing.org) art I’ve silently been generating and storing on my hard drive for years (like the banner of this post). **The funds I generate through this Mirror publication will go toward building a community of complex systems research, art, and curiosity.**\n\nSo thank you Mirror, and the community that got me here. **I can’t wait to go forth with you**. Special shoutouts to [Creator Cabins](https://creators.mirror.xyz/), [@JonathanHillis](https://twitter.com/JonathanHillis), [@SpaceXponential](https://twitter.com/SpaceXponential), [@rafathebuilder](https://twitter.com/rafathebuilder), [@jongold](http://www.twitter.com/jongold) for inspiring me, and helping me find my way. Find me on Twitter [@c_lemp](http://www.twitter.com/c_lemp) to share ideas and start a conversation.", "timestamp": 1629756981, "digest": "9nAXRheAFAKQKYdsa2baEIcpLslUyTJGqVXfIZe90DE", "contributor": "0x97b9958faceC9ACB7ADb2Bb72a70172CB5a0Ea7C"}
{"id": "DxI5AZtFGw4UuyFin8J7vexA5Sv4hqPo7kZAJNJMqK4", "title": "Finding The Adjacent Possible", "body": "With my eyes closed and mind wide open I listened as our lecturer unraveled a journey of innovation, economic forces, and human evolution.\n\nHe summarized thousands of years of complex dynamics with a single, simple equation - **The Theory of Adjacent Possible**, stating that dynamical systems making incremental changes over time explode into an exponential realm of possibilities, and within, a profound order that governs evolution.\n\nA spark of curiosity ignited within me on that cold January day and so did a burning question: **how can I apply the principles of complex systems to understand dynamics of collective decisions, self-organization, and the adjacent possible within teams, organizations, and society in today’s world?**\n\nThat lecturer was Stuart Kauffman, theoretical biologist and expert in self-organization of complex systems, and I was at the [New England Complex Systems Institute (NECSI)](http://www.necsi.edu) for the 2019 Winter School at MIT to learn about complex systems science, research, and computer modeling. I was so moved by the ideas he put forth that I reached out to Stu to learn more and try to understand my place within this mesh of interests, ideas, and intuition I was developing. He encouraged me to pursue the PhD I was interested in, and invited me for coffee at his home in Santa Fe to talk and share ideas. His succinct advice for me, the intellectual wanderer: \"**take your curiosity seriously.**\"\n\n## Where I go forth\n\nThis was the genesis of my own personal adjacent possible. A path of curiosity unwinding forward, without knowing the direction, but being guided by wonder and awe for how we work together, and what we can accomplish when we are free and without bounds.\n\nThat path has led me to the Cambrian explosion of Web3, with an adjacent possible of DAOs, new communities, and tools to express ourselves churning open as we build. The systems are being created, and I am developing the methods to understand and shape them. Mirror will be my space to create, share, and make connections within this sea of evolution.\n\nI’ll be writing about DAOs, PhD research, governance and control of complex systems, emergence, and self-organization. I’ll also tap into the eros with my trove of [Processing](http://www.processing.org) art I’ve silently been generating and storing on my hard drive for years (like the banner of this post). **The funds I generate through this Mirror publication will go toward building a community of complex systems research, art, and curiosity.**\n\nSo thank you Mirror, and the community that got me here. **I can’t wait to go forth with you**. Special shoutouts to [Creator Cabins](https://creators.mirror.xyz/), [@JonathanHillis](https://twitter.com/JonathanHillis), [@SpaceXponential](https://twitter.com/SpaceXponential), [@rafathebuilder](https://twitter.com/rafathebuilder), [@jongold](http://www.twitter.com/jongold) for inspiring me, and helping me find my way. Find me on Twitter [@c_lemp](http://www.twitter.com/c_lemp) to share ideas and start a conversation.", "timestamp": 1629757043, "digest": "9nAXRheAFAKQKYdsa2baEIcpLslUyTJGqVXfIZe90DE", "contributor": "0x97b9958faceC9ACB7ADb2Bb72a70172CB5a0Ea7C"}
{"id": "KaXwm9qwy7d9pnovW9_-tdzF_DVb03QMWAn6tvAS258", "title": "Finding The Adjacent Possible", "body": "With my eyes closed and mind wide open I listened as our lecturer unraveled a journey of innovation, economic forces, and human evolution.\n\nHe summarized thousands of years of complex dynamics with a single, simple equation - **The Theory of Adjacent Possible**, stating that dynamical systems making incremental changes over time explode into an exponential realm of possibilities, and within, a profound order that governs evolution.\n\nA spark of curiosity ignited within me on that cold January day and so did a burning question: **how can I apply the principles of complex systems to understand dynamics of collective decisions, self-organization, and the adjacent possible within teams, organizations, and society in today’s world?**\n\nThat lecturer was Stuart Kauffman, theoretical biologist and expert in self-organization of complex systems, and I was at the [New England Complex Systems Institute (NECSI)](http://www.necsi.edu) for the 2019 Winter School at MIT to learn about complex systems science, research, and computer modeling. I was so moved by the ideas he put forth that I reached out to Stu to learn more and try to understand my place within this mesh of interests, ideas, and intuition I was developing. He encouraged me to pursue the PhD I was interested in, and invited me for coffee at his home in Santa Fe to talk and share ideas. His succinct advice for me, the intellectual wanderer: \"**take your curiosity seriously.**\"\n\n## Where I go forth\n\nThis was the genesis of my own personal adjacent possible. A path of curiosity unwinding forward, without knowing the direction, but being guided by wonder and awe for how we work together, and what we can accomplish when we are free and without bounds.\n\nThat path has led me to the Cambrian explosion of Web3, with an adjacent possible of DAOs, new communities, and tools to express ourselves churning open as we build. The systems are being created, and I am developing the methods to understand and shape them. Mirror will be my space to create, share, and make connections within this sea of evolution.\n\nI’ll be writing about DAOs, PhD research, governance and control of complex systems, emergence, and self-organization. I’ll also tap into the eros with my trove of [Processing](http://www.processing.org) art I’ve silently been generating and storing on my hard drive for years (like the banner of this post). **The funds I generate through this Mirror publication will go toward building a community of complex systems research, art, and curiosity.**\n\nSo thank you Mirror, and the community that got me here. **I can’t wait to go forth with you**. Special shoutouts to [Creator Cabins](https://creators.mirror.xyz/), [@JonathanHillis](https://twitter.com/JonathanHillis), [@SpaceXponential](https://twitter.com/SpaceXponential), [@rafathebuilder](https://twitter.com/rafathebuilder), [@jongold](http://www.twitter.com/jongold) for inspiring me, and helping me find my way. Find me on Twitter [@c_lemp](http://www.twitter.com/c_lemp) to share ideas and start a conversation.", "timestamp": 1629757066, "digest": "9nAXRheAFAKQKYdsa2baEIcpLslUyTJGqVXfIZe90DE", "contributor": "0x97b9958faceC9ACB7ADb2Bb72a70172CB5a0Ea7C"}
{"id": "lKchYizdl-rS_OqhT6-d3VvbNcIqpYOpoFSRUimUUrg", "title": "Tutorial: How to Mint an NFT on OpenSea", "body": "In this tutorial, you’ll learn how to mint your own NFT on OpenSea.\n\n## What you’ll need\n\n1. Metamask/Rainbow wallet (or another web3 wallet)\n2. ETH for gas fees\n\n## What is OpenSea?\n\n![OpenSea Home Page](https://images.mirror-media.xyz/publication-images/bf8b44a1-6777-4be1-9755-3489d3e45df0.png?height=1538&width=2880)\n\nOpenSea is a peer-to-peer marketplace for trading NFTs. NFTs can be anything from works of art to virtual land, in-game items, and other utility tokens. By collecting NFTs from multiple blockchains, OpenSea acts as a one-stop shop where most collectors go to discover and purchase NFTs.\n\n## How to Mint an NFT on OpenSea\n\n### Step 1: Connect your wallet\n\n![](https://images.mirror-media.xyz/publication-images/49540ca8-4f4a-4f86-8577-cc9f7a24819d.png?height=1046&width=878)\n\n* Click on the wallet icon at the top right and connect your wallet. (If you’re using a Rainbow Wallet, you can connect it using the WalletConnect option)\n\n### Step 2: Create a Collection\n\nA Collection is like a folder that allows you to group NFTs that are part of the same series. Each collection has a unique link that displays only the NFTs that are part of that collection. If you are minting a lot of NFTs for different reasons, creating a collection is a good way to organize them and also makes it easy for collectors to access. For example, here is the Rabbithole Quest NFT collection on OpenSea\n\n<https://opensea.io/collection/rabbitholegg>\n\n* To create a collection, go to <https://opensea.io/collections> and click on “Create a Collection”.\n* You’ll need to create a few assets in order to create your collection\n  * Logo Image. Recommended Size: 350 \\* 350 (Required)\n  * Feature Image. Recommended Size: 600 \\* 400 (Optional)\n    * This image will be used when featuring your collection on the homepage or other pages on OpenSea.\n  * Banner Image. Recommended Size: 1400 \\* 400 (Optional)\n    * This image appears at the top of the collection page.\n\n![](https://images.mirror-media.xyz/publication-images/b641e75b-c410-417e-b72c-09f870d8e950.png?height=952&width=822)\n\n* Add the Name of your Collection\n* Customize the URL of your collection.\n* Add a description. The limit is 1000 characters and you can use Markdown!\n* Select the Category that your collection represents\n  * You can choose from Art, Music, Trading Cards, Collectibles, Sports, and Utility.\n* Add External Links\n  * You can link to your website, Discord server, Twitter, Instagram, Medium, and Telegram. All links are Optional.\n* Add the Royalty\n  * This allows you to define the percentage of secondary sales that you’d like to receive. You can set a royalty of up to 10%\n  * Once you enter a value, you can add a payout wallet address. Royalties are paid out monthly to the address that you list.\n* Choose the default blockchain\n  * NFTs that are added to this collection will be minted on the blockchain you choose.\n  * You can choose Ethereum or Polygon. The transaction and gas fees on Polygon are very cheap compared to the Ethereum Mainnet.\n  * Choose the blockchain that is most relevant to your application.\n* NSFW Toggle\n  * If your collection will contain items that are NSFW, click on the toggle. This will protect users that have SafeSearch enabled while browsing OpenSea\n\n### Step 3: Create a new item\n\n* Go to <https://opensea.io/asset/create>. This page allows you to add information about your NFT and mint it.\n* Upload the file you’d like to NFT.\n  * OpenSea supports a range of file types like JPG, PNG, GIF, MP4, WEBM, MP3, and more.\n  * The maximum file size supported is 40 MB.\n* Add the name of the item, an external link, and a description.\n  * Markdown is supported in the description.\n* Select the Collection that you’d like to add the NFT to.\n* If you’re creating an NFT series with multiple NFTs that have different traits, you can use the Properties, Levels, and Stats options to include them. This will allow users to calculate how rare a particular trait is.\n* Add Unlockable Content\n  * This content can only be unlocked by the owner of the NFT. You can add a link to a private discord server, link to the high-quality file, etc\n* NSFW Toggle.\n  * Select this if your item is NSFW\n* Blockchain\n  * You can choose either Ethereum or Polygon.\n  * Minting on Ethereum is expensive due to the transaction and gas fees.\n  * Minting on Polygon is very cheap.\n* Supply.\n  * If you choose to mint on Ethereum, you can only choose to mint 1 NFT. Minting on Polygon allows you to mint multiple copies.\n* Click on Create.\n\nYour NFT is now live! You can now choose to list it for sale. Listing it is free. Add the price of the NFT and sign a gasless transaction with your wallet to confirm the listing.\n\nCongratulations! You have now minted an NFT on OpenSea!", "timestamp": 1629761986, "digest": "gNLrq_4rj3LmaJ7FVOLM92IH_BHw9ZtpiTtSa6PNyuQ", "contributor": "0x7721F140C2968D5C639A9B40a1e6CA48a9b7c41D"}
{"id": "cVjFiU1GzfWrHy-Bl8mj7Y2z6IMg_V1BSQL7E9T2_tI", "title": "test opensea", "body": "[ethereum://0xabEFBc9fD2F806065b4f3C237d4b59D9A97Bcac7/100](ethereum://0xabEFBc9fD2F806065b4f3C237d4b59D9A97Bcac7/100)\n\n", "timestamp": 1629764527, "digest": "n9OuHIUS72Z42p4kjeqs-GR3fyUiPtMyTmgiE5SUpTI", "contributor": "0x5FE6C3F8d12D5Ad1480F6DC01D8c7864Aa58C523"}
{"id": "vXwYsoZdfIwmQdDHitEPMWCa_IiMhcxiquolUCXkwxs", "title": "NounsDAO", "body": "[NounsDAO](https://nouns.wtf/) is an NFT project that combines pseudorandom image generation with auctioning and community governance.\n\nEach NFT in the project is called a *Noun*, and only one Noun is minted each day - potentially forever. This novel combination of scarcity and infinity may just create sustainable value for its community.\n\nAfter each Noun is auctioned, the revenue from the sale goes into a treasury. NounDAO has adapted governance protocols from DeFi to give the owners of Nouns the ability to create and vote on proposals for the use of treasury funds and protocol upgrades.\n\nAt time of writing, there is 2,074.0122 ETH (\\~$6.88m) in the treasury, and the project has only been running for 15 days. The first treasury proposal has been to donate a total of 30 ETH (\\~$100k) to various charities.\n\nThere are four mechanisms worth highlighting in the NounsDAO protocol, each of which contains some innovation: **auctioning**, **trait generation**, **rendering**, and **governance**.\n\n![Examples of three nouns, rendered in SVG - each Noun has a unique background, body, head, and accessory.](https://images.mirror-media.xyz/publication-images/12e8e7a4-18f0-4503-902b-e6fa1cf0efbc.png?height=506&width=1012)\n\n## **Auctioning a Noun**\n\nThe lifecycle of a Noun begins with the creation of an auction on NounDAO's auction house; the auction house mints the Noun. The NounDAO auction house is a simplified version of Zora/Mirror's auction house that only accepts bids in ETH.\n\nOnly one auction is run at a time, and each auction runs for a minimum of one day. Whenever an auction is closed, a new one is immediately created. This means that there is always exactly one Noun on sale via auction at any time.\n\nEach auction has a reserve price, but it is currently set to a negligible 1 wei. New bids must increment the current bid by at least 2%, otherwise they revert. Nouns are typically being sold for between 100 and 150 ETH.\n\nIf a bid is made within the last 5 minutes, the auction is extended by 5 minutes, and when the auction is over, the noun is transferred from the auction contract to the highest bidder.\n\n![](https://images.mirror-media.xyz/publication-images/d8d4696e-9d73-47a8-9b71-634468cffbbd.png?height=1102&width=2120)\n\n## Noun **Generation**\n\nWhen a Noun is created, it is given an integer ID that can be used by anyone to view its traits.\n\nA Noun is composed of 5 traits: *background*, *body*, *accessory*, *head*, *glasses.*\n\nWhen a Noun is created, the protocol generates a random integer to represent each of these traits, distinguishing it from other Nouns. The theoretical maximum value of each trait is 2^48 - 1, but in practice it is limited by the number of images that have been uploaded to the protocol for that trait.\n\nTraits are publicly viewable for each Noun, and are stored on the core contract: `NounsToken`, which also holds user balances. The actual image rendering of these traits is decoupled, encoded on a contract called the `descriptor`, which can be swapped out through a governance proposal (unless governance has chosen to make this locked).\n\n![The traits for each Noun is publicly viewable on the core NounsToken contract.](https://images.mirror-media.xyz/publication-images/c9e74e17-f1ad-4b9e-9954-3318edf64bf0.png?height=710&width=1120)\n\n## Rendering a Noun\n\nThe integer used for each of the Noun's traits is a reference to an image stored in a contract called `NounsDescriptor`. Images are stored as string of bytes in a lossless data compression format called RLE, and are ultimately renderable by the contract as SVG.\n\nWhen the UI calls the tokenURI method, the contract returns the SVG derived from these compressed RLE images. This is innovative and worth considering for any new blockchain art project.\n\n![The tokenURI method returns all data required to render a high-fidelity image, without needing to reference anything off-chain.](https://images.mirror-media.xyz/publication-images/758876c3-612d-4a4b-b8fc-79d65b3fc189.png?height=702&width=1104)\n\nBecause Noun rendering is decoupled from the core Noun contract itself, this functionality can be improved and swapped if a new compression or rendering technique is proposed.\n\n## **Governance**\n\nNounsDAO is community-governed through a modification of Compound's Governor Bravo contract. It is modified to use ERC721 balances instead of ERC20 balances for voting, but also contains some other subtle differences.\n\nThe first governance proposal has been to \"Donate 5 ETH each to 6 Charities\", which looks like it will pass with unanimous consent.\n\n![](https://images.mirror-media.xyz/publication-images/3f5cb7c1-91fc-42d4-bb3c-55ca0fd7dc52.png?height=876&width=1440)\n\nThere are two other subtle differences that NounsDAO made to Governor Bravo:\n\n* During token transfer, delegation is moved via a hook, rather than in the transfer method itself.\n* The function that returns an account’s delegates will return the account itself if that account hasn't delegated. This is probably an improvement.\n\n## Technical Evaluation\n\nThe NounDAO code is cleanly written, using well-known open-source libraries and modern Solidity. Methods are expressed simply, but also contain a few well-executed innovations.\n\nUpgradability is achieved through decoupling functionality into separate contracts and making the pointers to those contracts governable, rather than through proxy-delegation methods often used. This keeps things simple while still allowing peripheral parts of the architecture - like Noun rendering - to be updated.\n\nThe rendering method itself is very innovative and worth understanding. NounsDAO has figured out a way to render somewhat sophisticated images entirely on-chain via SVGs, which improves on most other NFT projects including CryptoPunks.\n\nI've read every line of code in this project, and did not found any bugs. This doesn't mean there aren't any, and it might still be worthwhile to have a formal audit. Overall, I was impressed by the smart contracts created by this DAO, and by the innovation of the product as a whole.", "timestamp": 1629769011, "digest": "BqF6OA0uX2msUo4tp_iy9ZH1Dwz-_BWPOEJEoX46Yhg", "contributor": "0xCC65fA278B917042822538c44ba10AD646824026"}
{"id": "xhiqzhPFGKy1mA6WHR1I1cuYYKlGYubD5bKRWx28JUQ", "title": "NounsDAO", "body": "[NounsDAO](https://nouns.wtf/) is an NFT project that combines a novel form of pseudorandom image rendering with auctioning and community governance.\n\nEach NFT in the project is called a *Noun*, and one Noun is minted every day, indefinitely. The DAO seeks to achieve sustainable value through this combination of scarcity and longevity.\n\nAll Nouns are auctioned, and sales revenue goes into a treasury. NounDAO has adapted governance protocols from DeFi to allow owners of Nouns to create and vote on proposals over treasury funds and protocol upgrades.\n\nAt time of writing, there is 2,074.0122 ETH (\\~$6.88m) in the treasury, and the project has been running for 15 days. The first treasury proposal has been to donate a total of 30 ETH (\\~$100k) to various charities.\n\nThere are four mechanisms worth highlighting in the NounsDAO protocol, each of which contains some innovation: **auctioning**, **trait generation**, **rendering**, and **governance**.\n\n![Examples of three nouns, rendered in SVG - each Noun has a unique background, body, head, and accessory.](https://images.mirror-media.xyz/publication-images/12e8e7a4-18f0-4503-902b-e6fa1cf0efbc.png?height=506&width=1012)\n\n## **Auctioning a Noun**\n\nThe lifecycle of a Noun begins with the creation of an auction on NounDAO's auction house; the auction house mints the Noun. The NounDAO auction house is a simplified version of Zora/Mirror's auction house that only accepts bids in ETH.\n\nOnly one auction is run at a time, and each auction runs for a minimum of one day. Whenever an auction is closed, a new one is immediately created. This means that there is always exactly one Noun on sale via auction at any time.\n\nEach auction has a reserve price, but it is currently set to a negligible 1 wei. New bids must increment the current bid by at least 2%, otherwise they revert. Nouns are typically being sold for between 100 and 150 ETH.\n\nIf a bid is made within the last 5 minutes, the auction is extended by 5 minutes, and when the auction is over, the noun is transferred from the auction contract to the highest bidder.\n\n![](https://images.mirror-media.xyz/publication-images/d8d4696e-9d73-47a8-9b71-634468cffbbd.png?height=1102&width=2120)\n\n## Noun **Generation**\n\nWhen a Noun is created, it is given an integer ID that can be used by anyone to view its traits.\n\nA Noun is composed of 5 traits: *background*, *body*, *accessory*, *head*, *glasses.*\n\nWhen a Noun is created, the protocol generates a random integer to represent each of these traits, distinguishing it from other Nouns. The theoretical maximum value of each trait is 2^48 - 1, but in practice it is limited by the number of images that have been uploaded to the protocol for that trait.\n\nTraits are publicly viewable for each Noun, and are stored on the core contract: `NounsToken`, which also holds user balances. The actual image rendering of these traits is decoupled, encoded on a contract called the `descriptor`, which can be swapped out through a governance proposal (unless governance has chosen to make this locked).\n\n![The traits for each Noun is publicly viewable on the core NounsToken contract.](https://images.mirror-media.xyz/publication-images/c9e74e17-f1ad-4b9e-9954-3318edf64bf0.png?height=710&width=1120)\n\n## Rendering a Noun\n\nThe integer used for each of the Noun's traits is a reference to an image stored in a contract called `NounsDescriptor`. Images are stored as string of bytes in a lossless data compression format called RLE, and are ultimately renderable by the contract as SVG.\n\nWhen the UI calls the tokenURI method, the contract returns the SVG derived from these compressed RLE images. This is innovative and worth considering for any new blockchain art project.\n\n![The tokenURI method returns all data required to render a high-fidelity image, without needing to reference anything off-chain.](https://images.mirror-media.xyz/publication-images/758876c3-612d-4a4b-b8fc-79d65b3fc189.png?height=702&width=1104)\n\nBecause Noun rendering is decoupled from the core Noun contract itself, this functionality can be improved and swapped if a new compression or rendering technique is proposed.\n\n## **Governance**\n\nNounsDAO is community-governed through a modification of Compound's Governor Bravo contract. It is modified to use ERC721 balances instead of ERC20 balances for voting, but also contains some other subtle differences.\n\nThe first governance proposal has been to \"Donate 5 ETH each to 6 Charities\", which looks like it will pass with unanimous consent.\n\n![](https://images.mirror-media.xyz/publication-images/3f5cb7c1-91fc-42d4-bb3c-55ca0fd7dc52.png?height=876&width=1440)\n\nThere are two other subtle differences that NounsDAO made to Governor Bravo:\n\n* During token transfer, delegation is moved via a hook, rather than in the transfer method itself.\n* The function that returns an account’s delegates will return the account itself if that account hasn't delegated. This is probably an improvement.\n\n## Technical Evaluation\n\nThe NounDAO code is cleanly written, using well-known open-source libraries and modern Solidity. Methods are expressed simply, but also contain a few well-executed innovations.\n\nUpgradability is achieved through decoupling functionality into separate contracts and making the pointers to those contracts governable, rather than through proxy-delegation methods often used. This keeps things simple while still allowing peripheral parts of the architecture - like Noun rendering - to be updated.\n\nThe rendering method itself is very innovative and worth understanding. NounsDAO has figured out a way to render somewhat sophisticated images entirely on-chain via SVGs, which improves on most other NFT projects including CryptoPunks.\n\nI've read every line of code in this project, and did not found any bugs. This doesn't mean there aren't any, and it might still be worthwhile to have a formal audit. Overall, I was impressed by the smart contracts created by this DAO, and by the innovation of the product as a whole.", "timestamp": 1629770310, "digest": "BqF6OA0uX2msUo4tp_iy9ZH1Dwz-_BWPOEJEoX46Yhg", "contributor": "0xCC65fA278B917042822538c44ba10AD646824026"}
{"id": "StrX1_NKLwJ4q0LTxUoqS1CJ_eBOUTImi4M84bpFFf8", "title": "NounsDAO", "body": "[NounsDAO](https://nouns.wtf/) is an NFT project that combines a novel form of pseudorandom image generation with auctioning and community governance.\n\nEach NFT in the project is called a *Noun*, and only one Noun is minted each day, indefinitely. The DAO seeks to achieve sustainable value through this combination of scarcity and longevity.\n\nEach Noun is auctioned, and sales revenue goes into a treasury. NounDAO has adapted governance protocols from DeFi to allow owners of Nouns to create and vote on proposals over treasury funds and protocol upgrades.\n\nAt time of writing, there is 2,074.0122 ETH (\\~$6.88m) in the treasury, and the project has been running for 15 days. The first treasury proposal has been to donate a total of 30 ETH (\\~$100k) to various charities.\n\nThere are four mechanisms worth highlighting in the NounsDAO protocol, each of which contains some innovation: **auctioning**, **trait generation**, **rendering**, and **governance**.\n\n![Examples of three nouns, rendered in SVG - each Noun has a unique background, body, head, and accessory.](https://images.mirror-media.xyz/publication-images/12e8e7a4-18f0-4503-902b-e6fa1cf0efbc.png?height=506&width=1012)\n\n## **Auctioning a Noun**\n\nThe lifecycle of a Noun begins with the creation of an auction on NounDAO's auction house; the auction house mints the Noun. The NounDAO auction house is a simplified version of Zora/Mirror's auction house that only accepts bids in ETH.\n\nOnly one auction is run at a time, and each auction runs for a minimum of one day. Whenever an auction is closed, a new one is immediately created. This means that there is always exactly one Noun on sale via auction at any time.\n\nEach auction has a reserve price, but it is currently set to a negligible 1 wei. New bids must increment the current bid by at least 2%, otherwise they revert. Nouns are typically being sold for between 100 and 150 ETH.\n\nIf a bid is made within the last 5 minutes, the auction is extended by 5 minutes, and when the auction is over, the noun is transferred from the auction contract to the highest bidder.\n\n![](https://images.mirror-media.xyz/publication-images/d8d4696e-9d73-47a8-9b71-634468cffbbd.png?height=1102&width=2120)\n\n## Noun **Generation**\n\nWhen a Noun is created, it is given an integer ID that can be used by anyone to view its traits.\n\nA Noun is composed of 5 traits: *background*, *body*, *accessory*, *head*, *glasses.*\n\nWhen a Noun is created, the protocol generates a random integer to represent each of these traits, distinguishing it from other Nouns. The theoretical maximum value of each trait is 2^48 - 1, but in practice it is limited by the number of images that have been uploaded to the protocol for that trait.\n\nTraits are publicly viewable for each Noun, and are stored on the core contract: `NounsToken`, which also holds user balances. The actual image rendering of these traits is decoupled, encoded on a contract called the `descriptor`, which can be swapped out through a governance proposal (unless governance has chosen to make this locked).\n\n![The traits for each Noun is publicly viewable on the core NounsToken contract.](https://images.mirror-media.xyz/publication-images/c9e74e17-f1ad-4b9e-9954-3318edf64bf0.png?height=710&width=1120)\n\n## Rendering a Noun\n\nThe integer used for each of the Noun's traits is a reference to an image stored in a contract called `NounsDescriptor`. Images are stored as string of bytes in a lossless data compression format called RLE, and are ultimately renderable by the contract as SVG.\n\nWhen the UI calls the tokenURI method, the contract returns the SVG derived from these compressed RLE images. This is innovative and worth considering for any new blockchain art project.\n\n![The tokenURI method returns all data required to render a high-fidelity image, without needing to reference anything off-chain.](https://images.mirror-media.xyz/publication-images/758876c3-612d-4a4b-b8fc-79d65b3fc189.png?height=702&width=1104)\n\nBecause Noun rendering is decoupled from the core Noun contract itself, this functionality can be improved and swapped if a new compression or rendering technique is proposed.\n\n## **Governance**\n\nNounsDAO is community-governed through a modification of Compound's Governor Bravo contract. It is modified to use ERC721 balances instead of ERC20 balances for voting, but also contains some other subtle differences.\n\nThe first governance proposal has been to \"Donate 5 ETH each to 6 Charities\", which looks like it will pass with unanimous consent.\n\n![](https://images.mirror-media.xyz/publication-images/3f5cb7c1-91fc-42d4-bb3c-55ca0fd7dc52.png?height=876&width=1440)\n\nThere are two other subtle differences that NounsDAO made to Governor Bravo:\n\n* During token transfer, delegation is moved via a hook, rather than in the transfer method itself.\n* The function that returns an account’s delegates will return the account itself if that account hasn't delegated. This is probably an improvement.\n\n## Technical Evaluation\n\nThe NounDAO code is cleanly written, using well-known open-source libraries and modern Solidity. Methods are expressed simply, but also contain a few well-executed innovations.\n\nUpgradability is achieved through decoupling functionality into separate contracts and making the pointers to those contracts governable, rather than through proxy-delegation methods often used. This keeps things simple while still allowing peripheral parts of the architecture - like Noun rendering - to be updated.\n\nThe rendering method itself is very innovative and worth understanding. NounsDAO has figured out a way to render somewhat sophisticated images entirely on-chain via SVGs, which improves on most other NFT projects including CryptoPunks.\n\nI've read every line of code in this project, and did not found any bugs. This doesn't mean there aren't any, and it might still be worthwhile to have a formal audit. Overall, I was impressed by the smart contracts created by this DAO, and by the innovation of the product as a whole.", "timestamp": 1629770112, "digest": "BqF6OA0uX2msUo4tp_iy9ZH1Dwz-_BWPOEJEoX46Yhg", "contributor": "0xCC65fA278B917042822538c44ba10AD646824026"}
